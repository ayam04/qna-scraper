[
    {
        "question": "Watch this video on Hadoop Training for Beginners:",
        "answer": "Basic Interview Questions",
        "reference": "intellipaat.com"
    },
    {
        "question": "1. Differentiate between Pig and Hive.",
        "answer": "Criteria Apache Pig Apache Hive\nNature Procedural data flow language Declarative SQL-like language\nApplication Used for programming Used for report creation\nUsed by Researchers and programmers Mainly Data Analysts\nOperates on The client-side of a cluster The server-side of a cluster\nAccessing raw data Not as fast as HiveQL Faster with in-built features\nSchema or data type Always defined in the script itself Stored in the local database\nEase of learning Takes little extra time and effort to master Easy to learn from database experts",
        "reference": "intellipaat.com"
    },
    {
        "question": "2. How to skip header rows from a table in Hive?",
        "answer": "Imagine that header records in a table are as follows:\nSystem=\u2026\nVersion=\u2026\nSub-version=\u2026\nSuppose, we do not want to include the above three lines of headers in our Hive query. To skip the header lines from our table in Hive, we will set a table property.\nCREATE EXTERNAL TABLE employee (\nname STRING,\njob STRING,\ndob STRING,\nid INT,\nsalary INT)\nROW FORMAT DELIMITED FIELDS TERMINATED BY \u2018 \u2018 STORED AS TEXTFILE\nLOCATION \u2018/user/data\u2019\nTBLPROPERTIES(\"skip.header.line.count\"=\"2\u201d);",
        "reference": "intellipaat.com"
    },
    {
        "question": "3. What is a Hive variable? What do we use it for?",
        "answer": "Hive variables are basically created in the Hive environment that is referenced by Hive scripting languages. They allow to pass some values to a Hive query when the query starts executing. They use the source command.\nHave a look at this blog from intellipaat for better understanding of Hive environments \u2013 What is Apache Hive: Tutorial for Hive in Hadoop",
        "reference": "intellipaat.com"
    },
    {
        "question": "4. Explain the process to access subdirectories recursively in Hive queries.",
        "answer": "By using the below commands, we can access subdirectories recursively in Hive:\nhive> Set mapred.input.dir.recursive=true;\nhive> Set hive.mapred.supports.subdirectories=true;\nHive tables can be pointed to the higher level directory, and this is suitable for the directory structure like:\n/data/country/state/city/\nGet 100% Hike!\nMaster Most in Demand Skills Now !\nBy providing your contact details, you agree to our Terms of Use & Privacy Policy",
        "reference": "intellipaat.com"
    },
    {
        "question": "5. Can we change the settings within a Hive session? If yes, how?",
        "answer": "Yes, we can change the settings within a Hive session using the SET command. It helps change the Hive job settings for an exact query. For example, the following command shows that buckets are occupied according to the table definition:\nhive> SET hive.enforce.bucketing=true;\nWe can see the current value of any property by using SET with the property name. SET will list all the properties with their values set by Hive.\nhive> SET hive.enforce.bucketing;\n\nhive.enforce.bucketing=true\nThis list will not include the defaults of Hadoop. So, we should use the below code:\nSET -v\nIt will list all the properties including the Hadoop defaults in the system.",
        "reference": "intellipaat.com"
    },
    {
        "question": "6. Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?",
        "answer": "Yes, we can add the nodes by following the below steps:\nStep 1: Take a new system; create a new username and password\nStep 2: Install SSH and with the master node setup SSH connections\nStep 3: Add ssh public_rsa id key to the authorized keys file\nStep 4: Add the new DataNode hostname, IP address, and other details in /etc/hosts slaves file:\n192.168.1.102 slave3.in slave3\nStep 5: Start the DataNode on a new node\nStep 6: Login to the new node like suhadoop or:\nssh -X hadoop@192.168.1.103\nStep 7: Start HDFS of the newly added slave node by using the following command:\n./bin/hadoop-daemon.sh start data node\nStep 8: Check the output of the jps command on the new node\nGo through this Hadoop Training in London to get a clear understanding of Hadoop!",
        "reference": "intellipaat.com"
    },
    {
        "question": "7. Explain the concatenation function in Hive with an example.",
        "answer": "The concatenate function will join the input strings. We can specify\n\u2018n\u2019 number of strings separated by a comma.\nExample:\nCONCAT ('Intellipaat','-','is','-','a','-','eLearning',\u2019-\u2019,\u2019provider\u2019);\nOutput:\nIntellipaat-is-a-eLearning-provider\nEvery time, we set the limits of the strings by \u2018-\u2018. If it is common for every string, then Hive provides another command:\nCONCAT_WS\nIn this case, we have to specify the set limits of the operator first as follows:\nCONCAT_WS ('-',\u2019Intellipaat\u2019,\u2019is\u2019,\u2019a\u2019,\u2019eLearning\u2019,\u2018provider\u2019);\nOutput:\nIntellipaat-is-a-eLearning-provider\n\n\nIntermediate Interview Questions",
        "reference": "intellipaat.com"
    },
    {
        "question": "8. Explain the Trim and Reverse functions in Hive with examples.",
        "answer": "The trim function will delete the spaces associated with a string.\nExample:\nTRIM(\u2018 INTELLIPAAT \u2018);\nOutput:\nINTELLIPAAT\nTo remove the leading space:\nLTRIM(\u2018 INTELLIPAAT\u2019);\nTo remove the trailing space:\nRTRIM(\u2018INTELLIPAAT \u2018);\nIn the reverse function, characters are reversed in the string.\nExample:\nREVERSE(\u2018INTELLIPAAT\u2019);\nOutput:\nTAAPILLETNI",
        "reference": "intellipaat.com"
    },
    {
        "question": "9. How to change the column data type in Hive? Explain RLIKE in Hive.",
        "answer": "We can change the column data type by using ALTER and CHANGE as follows:\nALTER TABLE table_name CHANGE column_namecolumn_namenew_datatype;\nFor example, if we want to change the data type of the salary column from integer to bigint in the employee table, we can use the following:\nALTER TABLE employee CHANGE salary salary BIGINT;\nRLIKE: Its full form is Right-Like and it is a special function in Hive. It helps examine two substrings, i.e., if the substring of A matches with B, then it evaluates to true.\nExample:\n\u2018Intellipaat\u2019 RLIKE \u2018tell\u2019 \uf0e0 True\n\u2018Intellipaat\u2019 RLIKE \u2018^I.*\u2019 \uf0e0 True (this is a regular expression)",
        "reference": "intellipaat.com"
    },
    {
        "question": "10. What are the components used in Hive Query Processor?",
        "answer": "Following are the components of a Hive Query Processor:\nParse and Semantic Analysis (ql/parse)\nMetadata Layer (ql/metadata)\nType Interfaces (ql/typeinfo)\nSessions (ql/session)\nMap/Reduce Execution Engine (ql/exec)\nPlan Components (ql/plan)\nHive Function Framework (ql/udf)\nTools (ql/tools)\nOptimizer (ql/optimizer)",
        "reference": "intellipaat.com"
    },
    {
        "question": "11. What are Buckets in Hive?",
        "answer": "Buckets in Hive are used in segregating Hive table data into multiple files or directories. They are used for efficient querying.",
        "reference": "intellipaat.com"
    },
    {
        "question": "12. What kind of data warehouse application is suitable for Hive? What are the types of tables in Hive?",
        "answer": "Hive is not considered a full database. The design rules and regulations of Hadoop and HDFS have put restrictions on what Hive can do. However, Hive is most suitable for data warehouse applications because it:\nAnalyzes relatively static data\nHas less responsive time\nDoes not make rapid changes in data\nAlthough Hive doesn\u2019t provide fundamental features required for Online Transaction Processing (OLTP), it is suitable for data warehouse applications in large datasets. There are two types of tables in Hive:\nManaged tables\nExternal tables\nGet a better understanding of Hive by going through this Hadoop Tutorial now!",
        "reference": "intellipaat.com"
    },
    {
        "question": "13. What is the definition of Hive? What is the present version of Hive? Explain ACID transactions in Hive.",
        "answer": "Hive is an open-source data warehouse system. We can use Hive for analyzing and querying large datasets. It\u2019s similar to SQL. The present version of Hive is 0.13.1. Hive supports ACID (Atomicity, Consistency, Isolation, and Durability) transactions. ACID transactions are provided at row levels. Following are the options Hive uses to support ACID transactions:\nInsert\nDelete\nUpdate",
        "reference": "intellipaat.com"
    },
    {
        "question": "14. What is the maximum size of a string data type supported by Hive? Explain how Hive supports binary formats.",
        "answer": "The maximum size of a string data type supported by Hive is 2 GB. Hive supports the text file format by default, and it also supports the binary format sequence files, ORC files, Avro data files, and Parquet files.\nSequence file: It is a splittable, compressible, and row-oriented file with a general binary format.\nORC file: Optimized row columnar (ORC) format file is a record-columnar and column-oriented storage file. It divides the table in row split. Each split stores the value of the first row in the first column and follows subsequently.\nAvro data file: It is the same as a sequence file that is splittable, compressible, and row-oriented but without the support of schema evolution and multilingual binding.\nParquet file: In Parquet format, along with storing rows of data adjacent to one another, we can also store column values adjacent to each other such that both horizontally and vertically datasets are partitioned.\nLearn more about Hadoop from this Hadoop Training in New York to get ahead in your career!",
        "reference": "intellipaat.com"
    },
    {
        "question": "15. What is the precedence order of Hive configuration?",
        "answer": "We are using a precedence hierarchy for setting properties:\nThe SET command in Hive\nThe command-line \u2013hiveconf option\nHive-site.XML\nHive-default.xml\nHadoop-site.xml\nHadoop-default.xml",
        "reference": "intellipaat.com"
    },
    {
        "question": "16. If you run a select * query in Hive, why doesn't it run MapReduce?",
        "answer": "The hive.fetch.task.conversion property of Hive lowers the latency of MapReduce overhead, and in effect when executing queries such as SELECT, FILTER, LIMIT, etc. it skips the MapReduce function.\nIf you have any doubts or queries related to Hive, get them clarified from Hadoop experts on our Hive Community!\n\n\nAdvanced Interview Questions",
        "reference": "intellipaat.com"
    },
    {
        "question": "17. How can we improve the performance with ORC format tables in Hive?",
        "answer": "We can store Hive data in a highly efficient manner in an Optimized Row Columnar (ORC) file format. It can simplify many Hive file format limitations. We can improve the performance by using ORC files while reading, writing, and processing data.\nSet hive.compute.query.using.stats-true;\nSet hive.stats.dbclass-fs;\nCREATE TABLE orc_table (\nidint,\nname string)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY \u2018:\u2019\nLINES TERMINATED BY \u2018n\u2019\nSTORES AS ORC;\nNeed a reason to learn Apache Hadoop and Hive? Well, go through this blog post to find out Why Hadoop is the new black?",
        "reference": "intellipaat.com"
    },
    {
        "question": "18. Explain the functionality of ObjectInspector.",
        "answer": "ObjectInspector helps analyze the internal structure of a row object and the individual structure of columns in Hive. It also provides a uniform way to access complex objects that can be stored in multiple formats in the memory.\nAn instance of Java class\nA standard Java object\nA lazily initialized object\nObjectInspector tells the structure of the object and also the ways to access the internal fields inside the object.",
        "reference": "intellipaat.com"
    },
    {
        "question": "19. Whenever we run a Hive query, a new metastore_db is created. Why?",
        "answer": "A local metastore is created when we run Hive in an embedded mode. Before creating, it checks whether the metastore exists or not, and this metastore property is defined in the configuration file, hive-site.xml. The property is:\njavax.jdo.option.ConnectionURL\nwith the default value:\njdbc:derby:;databaseName=metastore_db;create=true\nTherefore, we have to change the behavior of the location to an absolute path so that from that location the metastore can be used.\nInterested in learning Hadoop? Check out the Hadoop Training in Sydney!",
        "reference": "intellipaat.com"
    },
    {
        "question": "20. Differentiate between Hive and HBase.",
        "answer": "Hive HBase\nEnables most SQL queries Does not allow SQL queries\nOperations do not run in real time Operations run in real time\nA data warehouse framework A NoSQL database\nRuns on top of MapReduce Runs on top of HDFS\nCompare Hive and HBase: Learn the differences and choose the best for your use case in our Hive vs Hbase blog now!",
        "reference": "intellipaat.com"
    },
    {
        "question": "21. How can we access the subdirectories recursively?",
        "answer": "By using the below commands, we can access subdirectories recursively in Hive:\nhive> Set mapred.input.dir.recursive=true;\nhive> Set hive.mapred.supports.subdirectories=true;\nHive tables can be pointed to the higher level directory, and this is suitable for the directory structure:\n/data/country/state/city/",
        "reference": "intellipaat.com"
    },
    {
        "question": "22. What are the uses of Hive Explode?",
        "answer": "Hadoop Developers consider an array as their input and convert it into a separate table row. To convert complicated data types into desired table formats, Hive uses Explode.\nLearn end-to-end Hadoop concepts through the Hadoop Course in Hyderabad to take your career to a whole new level!",
        "reference": "intellipaat.com"
    },
    {
        "question": "23. What is the available mechanism for connecting applications when we run Hive as a server?",
        "answer": "Thrift Client: Using Thrift, we can call Hive commands from various programming languages, such as C++, PHP, Java, Python, and Ruby.\nJDBC Driver: JDBC Driver enables accessing data with JDBC support, by translating calls from an application into SQL and passing the SQL queries to the Hive engine.\nODBC Driver: It implements the ODBC API standard for the Hive DBMS, enabling ODBC-compliant applications to interact seamlessly with Hive.",
        "reference": "intellipaat.com"
    },
    {
        "question": "24. How do we write our own custom SerDe?",
        "answer": "Mostly, end-users prefer writing a Deserializer instead of using SerDe as they want to read their own data format instead of writing to it, e.g., RegexDeserializer deserializes data with the help of the configuration parameter \u2018regex\u2019 and with a list of column names.\nIf our SerDe supports DDL (i.e., SerDe with parameterized columns and column types), we will probably implement a protocol based on DynamicSerDe, instead of writing a SerDe. This is because the framework passes DDL to SerDe through the \u2018Thrift DDL\u2019 format and it\u2019s totally unnecessary to write a \u201cThrift DDL\u201d parser.\nAre you interested in learning Hadoop from experts? Enroll in our Hadoop Course in Bangalore now!",
        "reference": "intellipaat.com"
    },
    {
        "question": "25. Mention various date types supported by Hive.",
        "answer": "The timestamp data type stores date in the java.sql.timestamp format.\nThree collection data types in Hive are:\nArrays\nMaps\nStructs",
        "reference": "intellipaat.com"
    }
]