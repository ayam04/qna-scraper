[
    {
        "question": "1. What do you understand by Natural Language Processing?",
        "answer": "Natural Language Processing is a field of computer science that deals with communication between computer systems and humans. It is a technique used in Artificial Intelligence and Machine Learning. It is used to create automated software that helps understand human-spoken languages to extract useful information from the data. Techniques in NLP allow computer systems to process and interpret data in the form of natural languages.",
        "reference": "intellipaat.com"
    },
    {
        "question": "2. List any two real-life applications of Natural Language Processing.",
        "answer": "Two real-life applications of Natural Language Processing are as follows:\nGoogle Translate: Google Translate is one of the famous applications of Natural Language Processing. It helps convert written or spoken sentences into any language. Also, we can find the correct pronunciation and meaning of a word by using Google Translate. It uses advanced techniques of Natural Language Processing to achieve success in translating sentences into various languages.\n\nChatbots: To provide a better customer support service, companies have started using chatbots for 24/7 service. AI Chatbots help resolve the basic queries of customers. If a chatbot is not able to resolve any query, then it forwards it to the support team, while still engaging the customer. It helps make customers feel that the customer support team is quickly attending to them. With the help of chatbots, companies have become capable of building cordial relations with customers. It is only possible with the help of Natural Language Processing.",
        "reference": "intellipaat.com"
    },
    {
        "question": "3. What are stop words?",
        "answer": "Stop words are said to be useless data for a search engine. Words such as articles, prepositions, etc. are considered stop words. There are stop words such as was, were, is, am, the, a, an, how, why, and many more. In Natural Language Processing, we eliminate the stop words to understand and analyze the meaning of a sentence. The removal of stop words is one of the most important tasks for search engines. Engineers design the algorithms of search engines in such a way that they ignore the use of stop words. This helps show the relevant search result for a query.\nBecome an expert in Natural Language Processing (NLP). Enroll now in NLP training in New York.",
        "reference": "intellipaat.com"
    },
    {
        "question": "4. What is NLTK?",
        "answer": "NLTK is a Python library, which stands for Natural Language Toolkit. We use NLTK to process data in human-spoken languages. NLTK allows us to apply techniques such as parsing, tokenization, lemmatization, stemming, and more to understand natural languages. It helps in categorizing text, parsing linguistic structure, analyzing documents, etc.\nA few of the libraries of the NLTK package that we often use in NLP are:\nSequentialBackoffTagger\nDefaultTagger\nUnigramTagger\ntreebank\nwordnet\nFreqDist\npatterns\nRegexpTagger\nbackoff_tagger\nUnigramTagger, BigramTagger, and TrigramTagger\nTo know how to use NLP properly with real-world experience projects. Enroll in NLP Training in Chennai now.\nGet 100% Hike!\nMaster Most in Demand Skills Now !\nBy providing your contact details, you agree to our Terms of Use & Privacy Policy",
        "reference": "intellipaat.com"
    },
    {
        "question": "5. What is Syntactic Analysis?",
        "answer": "Syntactic analysis is a technique of analyzing sentences to extract meaning from them. Using syntactic analysis, a machine can analyze and understand the order of words arranged in a sentence. NLP employs grammar rules of a language that helps in the syntactic analysis of the combination and order of words in documents.\nThe techniques used for syntactic analysis are as follows:\n\nParsing: It helps in deciding the structure of a sentence or text in a document. It helps analyze the words in the text based on the grammar of the language.\nWord segmentation: The segmentation of words segregates the text into small significant units.\nMorphological segmentation: The purpose of morphological segmentation is to break words into their base form.\nStemming: It is the process of removing the suffix from a word to obtain its root word.\nLemmatization: It helps combine words using suffixes, without altering the meaning of the word.",
        "reference": "intellipaat.com"
    },
    {
        "question": "6. What is Semantic Analysis?",
        "answer": "Semantic analysis helps make a machine understand the meaning of a text. It uses various algorithms for the interpretation of words in sentences. It also helps understand the structure of a sentence.\nTechniques used for semantic analysis are as given below:\n\nNamed entity recognition: This is the process of information retrieval that helps identify entities such as the name of a person, organization, place, time, emotion, etc.\nWord sense disambiguation: It helps identify the sense of a word used in different sentences.\nNatural language generation: It is a process used by the software to convert structured data into human-spoken languages. By using NLG, organizations can automate content for custom reports.\nIf you want to learn Artificial Intelligence then enroll in Artificial Intelligence Training now!",
        "reference": "intellipaat.com"
    },
    {
        "question": "7. List the components of Natural Language Processing.",
        "answer": "The major components of NLP are as follows:\n\nEntity extraction: Entity extraction refers to the retrieval of information such as place, person, organization, etc. by the segmentation of a sentence. It helps in the recognition of an entity in a text.\nSyntactic analysis: Syntactic analysis helps draw the specific meaning of a text.\nPragmatic analysis: To find useful information from a text, we implement pragmatic analysis techniques.\nMorphological and lexical analysis: It helps in explaining the structure of words by analyzing them through parsing.",
        "reference": "intellipaat.com"
    },
    {
        "question": "8. What is Latent Semantic Indexing (LSI)?",
        "answer": "Latent semantic indexing is a mathematical technique used to improve the accuracy of the information retrieval process. The design of LSI algorithms allows machines to detect the hidden (latent) correlation between semantics (words). To enhance information understanding, machines generate various concepts that associate with the words of a sentence.\nThe technique used for information understanding is called singular value decomposition. It is generally used to handle static and unstructured data. The matrix obtained for singular value decomposition contains rows for words and columns for documents. This method is best suited to identify components and group them according to their types.\nThe main principle behind LSI is that words carry a similar meaning when used in a similar context. Computational LSI models are slow in comparison to other models. However, they are good at contextual awareness which helps improve the analysis and understanding of a text or a document.",
        "reference": "intellipaat.com"
    },
    {
        "question": "9. What are Regular Expressions?",
        "answer": "A regular expression is used to match and tag words. It consists of a series of characters for matching strings.\nSuppose, if A and B are regular expressions, then the following are true for them:\nIf {\u025b} is a regular language, then \u025b is a regular expression for it.\nIf A and B are regular expressions, then A + B is also a regular expression within the language {A, B}.\nIf A and B are regular expressions, then the concatenation of A and B (A.B) is a regular expression.\nIf A is a regular expression, then A* (A occurring multiple times) is also a regular expression.",
        "reference": "intellipaat.com"
    },
    {
        "question": "10. What is Regular Grammar?",
        "answer": "Regular grammar is used to represent a regular language.\nRegular grammar comprises rules in the form of A -> a, A -> aB, and many more. The rules help detect and analyze strings by automated computation.\nRegular grammar consists of four tuples:\n\u2018N\u2019 is used to represent the non-terminal set.\n\u2018\u2211\u2019 represents the set of terminals.\n\u2018P\u2019 stands for the set of productions.\n\u2018S \u20ac N\u2019 denotes the start of non-terminal.\nRegular grammar is of 2 types:\n(a) Left Linear Grammar(LLG)\n(b) Right Linear Grammar(RLG)",
        "reference": "intellipaat.com"
    },
    {
        "question": "Watch this video on Natural Language Processing Interview Questions for Beginners:",
        "answer": "",
        "reference": "intellipaat.com"
    },
    {
        "question": "11. What is Parsing in the context of NLP?",
        "answer": "Parsing in NLP refers to the understanding of a sentence and its grammatical structure by a machine. Parsing allows the machine to understand the meaning of a word in a sentence and the grouping of words, phrases, nouns, subjects, and objects in a sentence. Parsing helps analyze the text or the document to extract useful insights from it. To understand parsing, refer to the below diagram:\n\nIn this, \u2018Jonas ate an orange\u2019 is parsed to understand the structure of the sentence.\nGet job assistance and get trained in NLP by enrolling in NLP Training in Bangalore.\nIntermediate NLP Interview Questions",
        "reference": "intellipaat.com"
    },
    {
        "question": "12. What is TF-IDF?",
        "answer": "TFIDF or Term Frequency-Inverse Document Frequency indicates the importance of a word in a set. It helps in information retrieval with numerical statistics. For a specific document, TF-IDF shows a frequency that helps identify the keywords in a document. The major use of TF-IDF in NLP is the extraction of useful information from crucial documents by statistical data. It is ideally used to classify and summarize the text in documents and filter out stop words.\nTF helps calculate the ratio of the frequency of a term in a document and the total number of terms. Whereas, IDF denotes the importance of the term in a document.\nThe formula for calculating TF-IDF:\nTF(W) = (Frequency of W in a document)/(The total number of terms in the document)\nIDF(W) = log_e(The total number of documents/The number of documents having the term W)\nWhen TF*IDF is high, the frequency of the term is less and vice versa.\nGoogle uses TF-IDF to decide the index of search results according to the relevancy of pages. The design of the TF-IDF algorithm helps optimize the search results in Google. It helps quality content rank up in search results.\nIf you want to know more about \u2018What is Natural Language Processing?\u2019 you can go through this Natural Language Processing Using Python course!",
        "reference": "intellipaat.com"
    },
    {
        "question": "13. Define the terminology in NLP.",
        "answer": "This is one of the most often asked NLP interview questions.\nThe interpretation of Natural Language Processing depends on various factors, and they are:\n\nWeights and Vectors\nUse of TF-IDF for information retrieval\nLength (TF-IDF and doc)\nGoogle Word Vectors\nWord Vectors\nStructure of the Text\nPOS tagging\nHead of the sentence\nNamed Entity Recognition (NER)\nSentiment Analysis\nKnowledge of the characteristics of sentiment\nKnowledge about entities and the common dictionary available for sentiment analysis\nClassification of Text\nSupervised learning algorithm\nTraining set\nValidation set\nTest set\nFeatures of the text\nLDA\nMachine Reading\nRemoval of possible entities\nJoining with other entities\nDBpedia\nFRED (lib) Pikes",
        "reference": "intellipaat.com"
    },
    {
        "question": "14. Explain Dependency Parsing in NLP.",
        "answer": "Dependency parsing helps assign a syntactic structure to a sentence. Therefore, it is also called syntactic parsing. Dependency parsing is one of the critical tasks in NLP. It allows the analysis of a sentence using parsing algorithms. Also, by using the parse tree in dependency parsing, we can check the grammar and analyze the semantic structure of a sentence.\nFor implementing dependency parsing, we use the spaCy package. It implements token properties to operate the dependency parse tree.\nThe below diagram shows the dependency parse tree:",
        "reference": "intellipaat.com"
    },
    {
        "question": "15. What is the difference between NLP and NLU?",
        "answer": "The below table shows the difference between NLP and NLU:",
        "reference": "intellipaat.com"
    },
    {
        "question": "16. What is the difference between NLP and CI?",
        "answer": "The below table shows the difference between NLP and CI:",
        "reference": "intellipaat.com"
    },
    {
        "question": "17. What is Pragmatic Analysis?",
        "answer": "Pragmatic analysis is an important task in NLP for interpreting knowledge that is lying outside a given document. The aim of implementing pragmatic analysis is to focus on exploring a different aspect of the document or text in a language. This requires a comprehensive knowledge of the real world. The pragmatic analysis allows software applications for the critical interpretation of the real-world data to know the actual meaning of sentences and words.\nExample:\nConsider this sentence: \u2018Do you know what time it is?\u2019\nThis sentence can either be asked for knowing the time or for yelling at someone to make them note the time. This depends on the context in which we use the sentence.",
        "reference": "intellipaat.com"
    },
    {
        "question": "18. What is Pragmatic Ambiguity?",
        "answer": "Pragmatic ambiguity refers to the multiple descriptions of a word or a sentence. An ambiguity arises when the meaning of the sentence is not clear. The words of the sentence may have different meanings. Therefore, in practical situations, it becomes a challenging task for a machine to understand the meaning of a sentence. This leads to pragmatic ambiguity.\nExample:\nCheck out the below sentence.\n\u2018Are you feeling hungry?\u2019\nThe given sentence could be either a question or a formal way of offering food.",
        "reference": "intellipaat.com"
    },
    {
        "question": "19. What are unigrams, bigrams, trigrams, and n-grams in NLP?",
        "answer": "When we parse a sentence one word at a time, then it is called a unigram. The sentence parsed two words at a time is a bigram.\nWhen the sentence is parsed three words at a time, then it is a trigram. Similarly, n-gram refers to the parsing of n words at a time.\nExample: To understand unigrams, bigrams, and trigrams, you can refer to the below diagram:\n\nTherefore, parsing allows machines to understand the individual meaning of a word in a sentence. Also, this type of parsing helps predict the next word and correct spelling errors.\nAre you interested in learning Artificial Intelligence from experts? Enroll in our AI Course in Bangalore now!",
        "reference": "intellipaat.com"
    },
    {
        "question": "20. What are the steps involved in solving an NLP problem?",
        "answer": "Below are the steps involved in solving an NLP problem:\nGather the text from the available dataset or by web scraping\nApply stemming and lemmatization for text cleaning\nApply feature engineering techniques\nEmbed using word2vec\nTrain the built model using neural networks or other Machine Learning techniques\nEvaluate the model\u2019s performance\nMake appropriate changes in the model\nDeploy the model",
        "reference": "intellipaat.com"
    },
    {
        "question": "21. What is Feature Extraction in NLP?",
        "answer": "Features or characteristics of a word help in text or document analysis. They also help in sentiment analysis of a text. Feature extraction is one of the techniques that are used by recommendation systems. Reviews such as \u2018excellent,\u2019 \u2018good,\u2019 or \u2018great\u2019 for a movie are positive reviews, recognized by a recommender system. The recommender system also tries to identify the features of the text that help in describing the context of a word or a sentence. Then, it makes a group or category of the words that have some common characteristics. Now, whenever a new word arrives, the system categorizes it as per the labels of such groups.",
        "reference": "intellipaat.com"
    },
    {
        "question": "22. What are precision and recall?",
        "answer": "The metrics used to test an NLP model are precision, recall, and F1. Also, we use accuracy for evaluating the model\u2019s performance. The ratio of prediction and the desired output yields the accuracy of the model.\nPrecision is the ratio of true positive instances and the total number of positively predicted instances.\n\nRecall is the ratio of true positive instances and the total actual positive instances.",
        "reference": "intellipaat.com"
    },
    {
        "question": "23. What is F1 score in NLP?",
        "answer": "F1 score evaluates the weighted average of recall and precision. It considers both false negative and false positive instances while evaluating the model. F1 score is more accountable than accuracy for an NLP model when there is an uneven distribution of class. Let us look at the formula for calculating F1 score:\n\nIf you are preparing for an Artificial Intelligence job interview, go through this top Artificial Intelligence Interview Questions and Answers!\n\nAdvanced NLP Interview Questions",
        "reference": "intellipaat.com"
    },
    {
        "question": "24. How to tokenize a sentence using the nltk package?",
        "answer": "Tokenization is a process used in NLP to split a sentence into tokens. Sentence tokenization refers to splitting a text or paragraph into sentences.\nFor tokenizing, we will import sent_tokenize from the nltk package:\n  from nltk.tokenize import sent_tokenize<>\nWe will use the below paragraph for sentence tokenization:\nPara = \u201cHi Guys. Welcome to Intellipaat. This is a blog on the NLP interview questions and answers.\u201d\n  sent_tokenize(Para)\nOutput:\n  [ 'Hi Guys.' ,\n  'Welcome to Intellipaat. ',\n  'This is a blog on the NLP interview questions and answers. ' ] \nTokenizing a word refers to splitting a sentence into words.\nNow, to tokenize a word, we will import word_tokenize from the nltk package.\n  from nltk.tokenize import word_tokenize\nPara = \u201cHi Guys. Welcome to Intellipaat. This is a blog on the NLP interview questions and answers.\u201d\n  word_tokenize(Para)\nOutput:\n  [ 'Hi' , 'Guys' , ' . ' , 'Welcome' , 'to' , 'Intellipaat' , ' . ' , 'This' , 'is' ,   'a', 'blog' , 'on' , 'the' , 'NLP' , 'interview' , 'questions' , 'and' , 'answers' , ' . ' ]",
        "reference": "intellipaat.com"
    },
    {
        "question": "25. Explain how we can do parsing.",
        "answer": "Parsing is the method to identify and understand the syntactic structure of a text. It is done by analyzing the individual elements of the text. The machine parses the text one word at a time, then two at a time, further three, and so on.\nWhen the machine parses the text one word at a time, then it is a unigram.\nWhen the text is parsed two words at a time, it is a bigram.\nThe set of words is a trigram when the machine parses three words at a time.\nLook at the below diagram to understand unigram, bigram, and trigram.\n\nNow, let\u2019s implement parsing with the help of the nltk package.\n  import nltk\n  text = \u201dTop 30 NLP interview questions and answers\u201d\nWe will now tokenize the text using word_tokenize.\n  text_token= word_tokenize(text)\nNow, we will use the function for extracting unigrams, bigrams, and trigrams.\n  list(nltk.unigrams(text))\nOutput:\n  [ \"Top 30 NLP interview questions and answer\"]\n \n  list(nltk.bigrams(text))\nOutput:\n  [\"Top 30\", \"30 NLP\", \"NLP interview\", \"interview questions\",   \"questions and\", \"and answer\"]\n \n  list(nltk.trigrams(text))\nOutput:\n  [\"Top 30 NLP\", \"NLP interview questions\", \"questions and answers\"]\nFor extracting n-grams, we can use the function nltk.ngrams and give the argument n for the number of parsers.\n  list(nltk.ngrams(text,n))",
        "reference": "intellipaat.com"
    },
    {
        "question": "26. Explain Stemming with the help of an example.",
        "answer": "In Natural Language Processing, stemming is the method to extract the root word by removing suffixes and prefixes from a word.\nFor example, we can reduce \u2018stemming\u2019 to \u2018stem\u2019 by removing \u2018m\u2019 and \u2018ing.\u2019\nWe use various algorithms for implementing stemming, and one of them is PorterStemmer.\nFirst, we will import PorterStemmer from the nltk package.\n  from nltk.stem import PorterStemmer\nCreating an object for PorterStemmer\n  pst=PorterStemmer()\n  pst.stem(\u201crunning\u201d), pst.stem(\u201ccookies\u201d), pst.stem(\u201cflying\u201d)\nOutput:\n  (\u2018run\u2019, \u2018cooki', \u2018fly\u2019 )",
        "reference": "intellipaat.com"
    },
    {
        "question": "27. Explain Lemmatization with the help of an example.",
        "answer": "We use stemming and lemmatization to extract root words. However, stemming may not give the actual word, whereas lemmatization generates a meaningful word.\nIn lemmatization, rather than just removing the suffix and the prefix, the process tries to find out the root word with its proper meaning.\nExample: \u2018Bricks\u2019 becomes \u2018brick,\u2019 \u2018corpora\u2019 becomes \u2018corpus,\u2019 etc.\nLet\u2019s implement lemmatization with the help of some nltk packages.\nFirst, we will import the required packages.\n  from nltk.stem import wordnet\n  from nltk.stem import WordnetLemmatizer\nCreating an object for WordnetLemmatizer()\n  lemma= WordnetLemmatizer()\n  list = [\u201cDogs\u201d, \u201cCorpora\u201d, \u201cStudies\u201d]\n  for n in list:\n  print(n + \u201c:\u201d + lemma.lemmatize(n))\nOutput:\n  Dogs: Dog\n  Corpora: Corpus\n  Studies: Study\nInterested in learning Artificial Intelligence? Go through this Artificial Intelligence Tutorial!",
        "reference": "intellipaat.com"
    },
    {
        "question": "28. What is Parts-of-speech Tagging?",
        "answer": "The parts-of-speech (POS) tagging is used to assign tags to words such as nouns, adjectives, verbs, and more. The software uses the POS tagging to first read the text and then differentiate the words by tagging. The software uses algorithms for the parts-of-speech tagging. POS tagging is one of the most essential tools in Natural Language Processing. It helps in making the machine understand the meaning of a sentence.\nWe will look at the implementation of the POS tagging using stop words.\nLet\u2019s import the required nltk packages.\n  import nltk\n  from nltk.corpus import stopwords\n  from nltk.tokenize import word_tokenize, sent_tokenize\n  stop_words = set(stopwords.words('english'))\n  txt = \"Sourav, Pratyush, and Abhinav are good friends.\"\nTokenizing using sent_tokenize\n  tokenized_text = sent_tokenize(txt)\nTo find punctuation and words in a string, we will use word_tokenizer and then remove the stop words.\n  for n in tokenized_text:\n  wordsList = nltk.word_tokenize(i)\n  wordsList = [w for w in wordsList if not w instop_words]\nNow, we will use the POS tagger.\n  tagged_words = nltk.pos_tag(wordsList)\n  print(tagged_words)\nOutput:\n  [('Sourav', 'NNP'), ('Pratyush', 'NNP'), ('Abhinav', 'NNP'), ('good',  'JJ'), ('friends', 'NNS')]",
        "reference": "intellipaat.com"
    },
    {
        "question": "29. Explain Named Entity Recognition by implementing it.",
        "answer": "Named Entity Recognition (NER) is an information retrieval process. NER helps classify named entities such as monetary figures, location, things, people, time, and more. It allows the software to analyze and understand the meaning of the text. NER is mostly used in NLP, Artificial Intelligence, and Machine Learning. One of the real-life applications of NER is chatbots used for customer support.\nLet\u2019s implement NER using the spaCy package.\nImporting the spaCy package:\n  import spacy\n  nlp = spacy.load('en_core_web_sm')\n  Text = \"The head office of Google is in California\"\n  document = nlp(text)for ent in document.ents:\n  print(ent.text, ent.start_char, ent.end_char, ent.label_)\nOutput:\n  Office 9 15 Place\n  Google 19 25 ORG\n  California 32 41 GPE\nNote: Office 9 15 Place means word starts at 9th position when tokenized and ends at 15, this is inclusive of spaces.",
        "reference": "intellipaat.com"
    }
]