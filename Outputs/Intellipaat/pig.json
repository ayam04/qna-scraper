[
    {
        "question": "1. Compare Pig and Hive.",
        "answer": "Criteria Pig Hive\nLanguage Pig Latin SQL-like\nApplication Programming purposes Report creation\nOperation Client Side Server side\nData support Semi-structured Structured\nConnectivity Can be called by other applications JDBC & BI tool integration",
        "reference": "intellipaat.com"
    },
    {
        "question": "2. Does Pig differ from MapReduce? If yes, how?",
        "answer": "Yes, Pig differs from MapReduce because, in MapReduce, the group by operation is performed at reducer side and filter, and also in the map phase the projection is implemented. Pig Latin provides the operations that are similar to MapReduce, such as groupby, orderby, and filters. We can analyze the Pig script and data flow to find the error checking. Pig Latin is lower in cost to write and maintain compared to MapReduce Java code.\nGo through this Apache Pig Tutorial to get a better understanding of the concepts.",
        "reference": "intellipaat.com"
    },
    {
        "question": "3. Explain the uses of Map Reduce in Pig.",
        "answer": "Apache Pig programs are written in Pig Latin query language which is similar to the SQL query language. To execute this queries, there requires an execution engine. The Pig engine enables to convert the queries into MapReduce jobs and thus MapReduce acts as the execution engine and is designed to run the programs as per the requirements.\nPigs\u2019 operators are using Hadoops\u2018 API depending upon the configurations the job is executed in local mode or Hadoop cluster. Pig is never passes any outputs to Hadoop instead set the inputs and data locations for map-reduce.\nPig Latin provides a set of standard Data-processing operations, such as join, filter, group by, order by, union, etc which are mapped to do the map-reduce tasks. A Pig Latin script describes a (DAG) directed acyclic graph, where the edges are data flows and the nodes are operators that process the data.",
        "reference": "intellipaat.com"
    },
    {
        "question": "4. Explain the uses of PIG.",
        "answer": "We can use Pig in three categories, they are\nETL data pipeline : ETL helps to populate our data warehouse. Pig can pipeline the data to an external application, it will wait until it\u2019s finished, so that it has receive the processed data and continue from there. It is the most common use case for Pig.\nResearch on raw data.\nIterative processing.\nGet 100% Hike!\nMaster Most in Demand Skills Now !\nBy providing your contact details, you agree to our Terms of Use & Privacy Policy",
        "reference": "intellipaat.com"
    },
    {
        "question": "5. Name the scalar data type and complex data types in Pig.",
        "answer": "The scalar data types in pig are int, float, double, long, chararray, and bytearray.\nThe complex data types in Pig are map, tuple, and bag.\nMap: The data element with the data type chararray where element has pig data type include complex data type\nExample- [city\u2019#\u2019bang\u2019,\u2019pin\u2019#560001]\nIn this city and pin are data element mapping to values.\nTuple : It is a collection of data types and it has fixed length. Tuple is having multiple fields and these are ordered.\nBag : It is a collection of tuples, but it is unordered, tuples in the bag are separated by comma\nExample: {(\u2018Bangalore\u2019, 560001),(\u2018Mysore\u2019,570001),(\u2018Mumbai\u2019,400001)",
        "reference": "intellipaat.com"
    },
    {
        "question": "6. State the usage of \u2018filters\u2019, \u2018group\u2019 , \u2018orderBy\u2019, \u2018distinct\u2019 keywords in pig scripts.",
        "answer": "Filters : Filters has the similar functionality as where clause in SQL. Filters contain predicate and if it evaluates true for a given record, then that record will be passed down the pipeline. Otherwise, it will not predicate the results and thus contains different operators like ==,>=, <=,!=.so,== and != which is been applied in creating maps and tuples.\nA= load \u2018inputs\u2019 as (name,address)\nB=filter A by symbol matches \u2018CM.*';\nGroupBy : The group statement collects various records with the same key. In SQL database GroupBy creates a group which feeds directly to one or more aggregate functions. But in Pig Latin has no direct connection between group and aggregate functions.\nInput 2 = load \u2018daily\u2019 as(exchanges,stocks);\ngrpds = group input2 by stocks;\nOrder : The Order statement sorts the data producing a total order of output data. The Order syntax is similar to Group. Give a key or set of keys to order your data as per requirement. The following are the examples for the same:\nInput 2 = load \u2018daily\u2019 as(exchanges,stocks);\ngrpds = order input2 by exchanges;\nDistinct : The distinct statement is very simple to understand and implement. It removes duplicate records and the original data will be secured. It is implemented only on entire records, not on individual fields. Consider the below examples which explains the same:\nInput 2 = load \u2018daily\u2019 as(exchanges,stocks);\ngrpds = distinct exchanges;",
        "reference": "intellipaat.com"
    },
    {
        "question": "7. Explain the LOAD keyword in Pig script.",
        "answer": "Load helps to load data from the file system. It is a relational operator\nIn the first step in data-flow language we need to mention the input, which is completed by using \u2018load\u2019 keyword.\nThe LOAD syntax is\nLOAD \u2018mydata\u2019 [USING function] [AS schema];\nExample- A = LOAD \u2018intellipaat.txt\u2019;\nA = LOAD \u2018intellipaat.txt\u2019 USINGPigStorage(\u2018t\u2019);",
        "reference": "intellipaat.com"
    },
    {
        "question": "8. What are the relation operations in Pig? Explain any two with examples.",
        "answer": "The relational operations in Pig:\nforeach, order by, filters, group, distinct, join, limit.foreach: It takes a set of expressions and applies them to all records in the data pipeline to the next operator.A =LOAD \u2018input\u2019 as (emp_name :charrarray, emp_id : long, emp_add : chararray, phone : chararray, preferences : map [] );B = foreach A generate emp_name, emp_id;Filters: It contains a predicate and it allows us to select which records will be retained in our data pipeline.\nSyntax: alias = FILTER alias BY expression;\nAlias indicates the name of the relation, By indicates required keyword and the expression has Boolean.\nExample: M = FILTER N BY F5 == 4;\nTake this Hadoop online training to learn about Pig, Pig Latin and everything to deploy Pig in real world scenario.",
        "reference": "intellipaat.com"
    },
    {
        "question": "9. Does Pig support multi-line commands?",
        "answer": "Yes, pig supports both single line and multi-line commands. In single line command it executes the data, but it doesn\u2019t store in the file system, but in multiple lines commands it stores the data into \u2018/output\u2019;/* , so it can store the data in HDFS.",
        "reference": "intellipaat.com"
    },
    {
        "question": "10. Explain different execution modes available in Pig.",
        "answer": "Three different execution modes available in Pig they are,\nInteractive mode or Grunt mode.\nBatch mode or Script mode.\nEmbedded mode\nInteractive mode or grunt mode: Pig\u2019s interactive shell is known as grunt shell. If no file is specified to run in Pig it will start.\ngrunt> run scriptfile.pig\ngrunt> exec scriptfile.pig\nBatch mode or Script mode : Pig executes the specified commands in the script file.Embedded mode : We can embed Pig programs in Java and we can run the programs from Java.",
        "reference": "intellipaat.com"
    },
    {
        "question": "11. What are the exception handling operators in Pig script?",
        "answer": "Following operators are used for handling the exception in pig script.\nDUMP : It helps to display the results on screen.\nDESCRIBE : It helps to display the schema of aparticular relation.\nILLUSTRATE : It helps to display step by step execution of a sequence of pig statements\nEXPLAIN : It helps to display the execution plan for Pig Latin statements.",
        "reference": "intellipaat.com"
    },
    {
        "question": "12. Differentiate between the physical plan and logical plan in Pig script.",
        "answer": "Both plans are created while to execute the pig script.\nPhysical plan : It is a series of MapReduce jobs while creating the physical plan.It\u2019s divided into three physical operators such as Local Rearrange, Global Rearrange, and package. It illustrates the physical operators Pig will use to execute the script without referring to how they will execute in MapReduce Loading and storing functions are resolved in physical plan.\nExample- A: Load(/emp:PigStorage(\u2018 \u2018))\nLogical plan : The Logical plan is a plan which is created for each line in the Pig scripts. It is produced after semantic checking and basic parsing. With every line, the logical plan for that particular program becomes extended and larger because each and every statement has its own logical plan.Loading and storing function are not resolved in logical plan.\nExample: X: (Name: LOLoad schema: emp_id#36:bytearray,emp_name#37:bytearray,city#38:bytearray,salary#39:bytearray)Required Fields:null",
        "reference": "intellipaat.com"
    },
    {
        "question": "13. Is Pig script case sensitive?",
        "answer": "Pig script is both case sensitive and case insensitive. For example, in user defined functions, the field name, and relations are case sensitive ,i.e., INTELLIPAAT is not same as intellipaat or M=load \u2018test\u2019 is not same as m=load \u2018test\u2019. And Pig script keywords are case insensitive i.e., LOAD is same as a load.",
        "reference": "intellipaat.com"
    },
    {
        "question": "14. Highlight the difference between group and Cogroup operators in Pig.",
        "answer": "Both the operators can work with one or more relations. Group and Cogroup operators are identical. Group operator collects all records with the same key. Cogroup is a combination of group and join, it is a generalization of a group instead of collecting records of one input depends on a key, it collects records of n inputs based on a key. At a time we can Cogroup upto 127 relations.",
        "reference": "intellipaat.com"
    },
    {
        "question": "15. What is the function of UNION and SPLIT operators? Give examples.",
        "answer": "Union operator helps to merge the contents of two or more relations.\nSyntax: grunt> Relation_name3 = UNION Relation_name1, Relation_name2\n \nExample: grunt> INTELLIPAAT = UNION intellipaat_data1.txt intellipaat_data2.txt\nSPLIT operator helps to divide the contents of two or more relations.\nSyntax: grunt> SPLIT Relationa1_name INTO Relationa2_name IF (condition1), Relation2_name (condition2);\n \nExample: SPLIT student_details into student_details1 if marks<35, student_details2 if (8590); [/code]",
        "reference": "intellipaat.com"
    },
    {
        "question": "16. How can we see only top 15 records from the student.txt out of 100 records in the HDFS directory?",
        "answer": "We should change the name student.txt into STUDENT it is the relation name. We can see the top 15 records in using limit operator\nResult = limit student 15.",
        "reference": "intellipaat.com"
    },
    {
        "question": "17. What is the use of BloomMapFile?",
        "answer": "It is an extended class of MapFile. Its functionality is similar to MapFile. It is used in the Hbase table format, Bloom Map File uses dynamic Bloom filters to provide rapid membership test for the keys.",
        "reference": "intellipaat.com"
    },
    {
        "question": "18. How does the Pig platform handle relational systems data?",
        "answer": "There are two ways Pig can work with relational datasets.\nLoad relational data directly into the Hadoop framework, where Pig can access it.\nUsing database connectors, Pig can load data directly from a relational database system and we can access it.",
        "reference": "intellipaat.com"
    },
    {
        "question": "19. What are the drawbacks of Pig?",
        "answer": "Some of the drawbacks of Pig are:\nPig is not really a convenient option for real-time use cases.\nPig does not prove to be useful when you need to fetch single record from a huge dataset.\nSince it works on MapReduce, it works in batches.",
        "reference": "intellipaat.com"
    },
    {
        "question": "20. Mention the common features in Pig and Hive.",
        "answer": "The common features in Both Hive and Pig are\nInternally both are converted the commands into MapReduce.\nBoth the technologies provide high-level abstractions.\nBoth do not support low-latency queries.\nBoth do not support OLAP or OLTP.",
        "reference": "intellipaat.com"
    },
    {
        "question": "21. Differentiate between Pig Latin and Pig Engine.",
        "answer": "Pig Latin is scripting language like Perl for searching huge data sets and it is made up of a series of transformations and operations that are applied to the input data to produce data.\nPig engine is an environment to execute the Pig Latin programs. It converts Pig Latin operators into a series of MapReduce jobs.",
        "reference": "intellipaat.com"
    },
    {
        "question": "22. Explain the terms in the below syntax.EXPLAIN [-script pigscript] [-out path] [-brief] [-dot] [-paramparam_name = param_value] [-param_filefile_name] alias;",
        "answer": "script: It is used to specify a Pig script\nout : Used to specify the output path (directory)\nbrief :Does not expand nested plans\ndot : outputs a format that can be passed to the dot utility for graphical display \u2013 will generate a directed-acyclic-graph (DAG) of the plans in any supported format (.gif, .jpg \u2026).\nAlias : name of a relation.\nparamparam_name = param_value : used to see the parameters According to IBM, processing your data is simple with Apache Pig.",
        "reference": "intellipaat.com"
    }
]