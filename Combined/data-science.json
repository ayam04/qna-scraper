[
    {
        "question": "1. Differentiate between Data Analytics and Data Science",
        "answer": "Aspect Data Analytics Data Science\nScope  Analyzing the historical data for insights and trends.  Focuses on descriptive, predictive modeling, and decision-making with the data. \nMethods Analyses the structured data with the help of statistical techniques.  Uses statistical models, machine learning, and other advanced algorithms. \nPurpose Finding and solving the answers to specific problems. Predicting future outcomes and making data-driven decisions. \nTechniques Statistical Analysis, Data mining, visualization  Machine learning, Deep learning, and Natural language processing.  \nExample Analyze the website traffic data to understand the behavior of users. Building a model to predict future sales trends.\nBecome an expert in Data Science. Enroll now in PG program in Data Science and Machine Learning from MITxMicroMasters.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "2. What is Supervised Learning?",
        "answer": "Supervised learning can be defined as an approach to machine learning where the algorithm uses labeled data in order to make predictions or assign new, unseen data to predefined categories.\nFor instance, suppose that every day you receive hundreds of emails and some of them are spam. Email service providers use supervised learning algorithms to clear unwanted mail from your mailbox. By using marked sample spam and non-spam messages, they teach the system how to identify patterns suggesting spam. After being trained, the algorithm will automatically sort new mail into spam/non-spam according to its contents thereby saving time when checking your email box.\nHave a look at a few Data Science projects for Beginners",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "3. What is Unsupervised Learning?",
        "answer": "Unsupervised learning is a technique in machine learning where the algorithm learns patterns and relationships from unlabelled data without any kind of supervision. The algorithm searches for underlying patterns or groupings within the data as opposed to having labeled instances.\nFor instance, suppose you are an employee of an online retailer who wants to know more about its heterogeneous customer base. Employ unsupervised learning, i.e., clustering algorithms to scrutinize customers’ transaction histories devoid of pre-determined tags.\nGet 100% Hike!\nMaster Most in Demand Skills Now !\nBy providing your contact details, you agree to our Terms of Use & Privacy Policy",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "4. What is the difference between long format data and wide format data?",
        "answer": "Long Format Data Wide Format Data\nA long format data has a column for possible variable types and a column for the values of those variables. Whereas, Wide data has a column for each variable.\nEach row in the long format represents a one-time point per subject. As a result, each topic will contain many rows of data. The repeated responses of a subject will be in a single row, with each response in its own column, in the wide format.\nThis data format is most typically used in R analysis and for writing to log files at the end of each experiment. This data format is most widely used in data manipulations, and stats programs for repeated measures ANOVAs and is seldom used in R analysis.\nA long format contains values that repeat in the first column. A wide format contains values that do not repeat in the first column.\nUse df.melt() to convert the wide form to the long form use df.pivot().reset_index() to convert the long form into the wide form",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "5. Mention some techniques used for sampling. What is the main advantage of sampling?",
        "answer": "Sampling is defined as the process of selecting a sample from a group of people or from any particular kind for research purposes. It is one of the most important factors which decides the accuracy of a research/survey result.\nMainly, there are two types of sampling techniques:\nProbability sampling: It involves random selection which makes every element get a chance to be selected. Probability sampling has various subtypes in it, as mentioned below:\nSimple Random Sampling\nStratified sampling\nSystematic sampling\nCluster Sampling\nMulti-stage SamplingNon- Probability Sampling: Non-probability sampling follows non-random selection which means the selection is done based on your ease or any other required criteria. This helps to collect the data easily. The following are various types of sampling it:\nConvenience Sampling\nPurposive Sampling\nQuota Sampling\nReferral /Snowball Sampling\nCheck out this video on the Data Science Interview Questions:",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "6. What is bias in data science?",
        "answer": "Bias is a type of error that occurs in a data science model because of using an algorithm that is not strong enough to capture the underlying patterns or trends that exist in the data.\nImagine bias in a data science model as training the computer to recognize cats from pictures. If it does not know all the various shapes and colors that can be on cats, then it may mistake dogs for cats or fail to spot some of them. \nFor example, in data science, models like linear or logistic regression will miss something important about the data and end up with prediction bias. It’s like putting blinders on the horses; they only see straight ahead thus accuracy reduces due to simplified assumptions.\nWant to know about a few applications of Data Science, Have a look at the  Top 8 Data Science Applications",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "7. What is dimensionality reduction?",
        "answer": "Dimensionality reduction is a procedure used in data science for minimizing the number of features (or dimensions) in a dataset while retaining as much of the useful information as possible; this can be thought of as simplifying complex problems by focusing on what really matters.\nImagine you have a data set that has so many different types of features, but not all of them are equally important in order to understand underlying patterns or relationships. By only retaining the most pertinent elements while discarding irrelevant or less significant ones, dimensionality reduction comes into play.\nThere exist two main methods for reducing dimensionality:\nFeature Selection: In this method, we take a few representative features from the original set and keep the others based on their significance or importance to solving a particular problem. These could be done through correlation analysis, feature ranking, or expert knowledge.\nFeature Extraction: Rather than choosing existing features, feature extraction generates synthetic ones that contain vital information present within an initial dataset. Feature extraction often uses techniques like Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE).\nGo through the Data Science Course Syllabus to learn about the curriculum in detail.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "8. Why is Python used for data cleaning in Data Science?",
        "answer": "Data cleaning in data science is often done through Python for the following reasons:\nRich Libraries: Python offers rich libraries like Pandas, NumPy, and SciPy which are designed to assist in handling missing values and transforming datasets.\nReadable Syntax: Python’s clean syntax helps make it easier to read code. This simplifies complex data-cleaning tasks and encourages collaboration.\nInteractive Environment: With Python IDEs and Jupyter Notebooks, you can explore data interactively and perform manipulations on it. These support rapid prototyping and experimentation.\nIntegration: Python works perfectly with different tools and technologies used in data science thus enabling seamless end-to-end processes of data flow.\nCommunity Support: The large number of people using Python makes available many useful resources such as code snippets or help when you get stuck during learning or troubleshooting procedures concerning Data Cleaning or Data Science workflows.\nFor instance, you might decide to remove outliers that are beyond a certain standard deviation from the mean of a numerical column.\n1\n2\n3\n4\n5\n6\n7\nmean = df[\"Price\"].mean();\n std = df[\"Price\"].std();\n threshold = mean + (3 * std);  # Set a threshold for outliers\n df = df[df[\"Price\"] < threshold]  # Remove outliers\nLearn more about Data Cleaning in a Data Science Tutorial!",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "9. What do you understand about Linear Regression?",
        "answer": "Linear regression is a basic statistical method used to describe the relationship between a dependent variable and one or more independent variables by employing a linear equation of observed data. The aim of linear regression is to find the most suitable line (or hyperplane) that depicts the relationship between dependent and independent variables.\nIn simple linear regression, there is only one independent variable while in multiple linear regression.\ny = mx + b\nWhere,\ny → dependent variable\nx → independent variable\nm → slope\nb → y-intercept\nIn multiple linear regression, the equation extends to include multiple independent variables: \n\nLeast squares are among various methods used to estimate the coefficients in a regression equation so as to minimize the difference between actual and estimated values.\nFor example, in real estate, features such as house size, bedrooms, and location are analyzed using linear regression techniques in order to predict housing prices. Using historical information it provides immediate forecasts for new listings thereby facilitating decision-making processes by buyers and sellers.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "10. What do you understand by logistic regression?",
        "answer": "Logistic regression is a statistical method that employs a logistic function to model the relationship between a binary dependent variable or more independent variables. Linear regression predicts continuous outcomes while logistic regression predicts the probability of the occurrence of a categorical outcome.\nEquations of Logistic Regression are: \n\nLogistic regression coefficients are estimated by using methods such as maximum likelihood estimation, trying to find the group of coefficients that provide the best fit to observed data and maximize the likelihood of observing those data given the model\nFor example, we are trying to determine whether it will rain or not on the basis of temperature and humidity.\n\nTemperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S-shaped curve.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "11. What is a confusion matrix?",
        "answer": "The confusion matrix is an example of a table that is often used in describing the performance of a classification model on test data whose true values are known. This helps us see how well our algorithm performs, usually supervised learning algorithms.\nIt should be structured into 4 different parts: \nTrue Positive (TP): What we predicted to be positive and is actually positive.\nTrue Negative (TN): What we predicted to be negative and is actually negative.\nFalse Positive (FP): What we predicted to be positive but was actually negative (Type I error)\nFalse Negative (FN): What we predicted to be negative but was actually positive (Type II error)\nA confusion matrix helps evaluate how well a classification algorithm does by giving several metrics like accuracy, precision, recall, F1 score, and specificity. It proves useful especially when the classes are imbalanced or when different misclassification errors have different costs.\n\nAre you ready for your interview?\nTake a quick Quiz to check it out\nTake a Quiz",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "12. What are important functions used in Data Science?",
        "answer": "Important functions used in Data Science are: \nData Manipulating Functions:\npd.read_csv(): Read the data from a comma-separated values (CSV) file into DataFrame.\ndf.head(): Returns the first n rows of a DataFrame.\ndf.tail(): Return the last n rows of a DataFrame.\ndf.info(): It provides concise summary of your data set, data types, and missing values for each attribute.\ndf.describe(): Displays statistics on numerical columns in a data frame.\nData Cleaning Functions:\ndf.dropna(): This function drops rows with missing values in them\ndf.fillna(): Fills all na’s present in the data frame with specified values. \ndf.drop_duplicates(): Drops duplicate rows from the data frame.\ndf.replace(): Replaces specified value(s) in place of another value(s) in a particular column.\nData Visualization Functions:\nplt.plot(): Plot lines through linear interpolation.\nplt.scatter(): Scatter plot showing the relationship between two variables\nplt.hist(): Plot histogram using the number of bins and range parameters specified by user\nsns.boxplot(): Draw a box plot to show distributions with respect to categories.\nsns.heatmap(): Draw a 2-D heat map using different colors corresponding to different intensity levels within that grid cell.\nStatistical Functions:\nnp.mean(): Compute the arithmetic mean along the specified axis.\nnp.median(): Compute the median along the specified axis.\nnp.std(): Calculates standard deviation based on the entire numpy array or given axis.\nnp.corrcoef(): It is used to calculate the coefficient matrix for correlation coefficients of each variable with every other variable\nMachine Learning functions:\ntrain_test_split(): It divides the data into training and testing sets.\nfit(): Matches a machine learning model to the training data.\npredict(): Utilizing a well-trained machine learning model produces forecasts.\nscore(): Evaluate the performance of machine learning algorithms.\nHave a look at our Data Science vs. Data Analytics blog to understand the key differences.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "13. What is k-fold cross-validation?",
        "answer": "In k-fold cross-validation, we divide the dataset into k equal parts. After this, we loop over the entire dataset k times. In each iteration of the loop, one of the k parts is used for testing, and the other k − 1 parts are used for training. Using k-fold cross-validation, each one of the k parts of the dataset ends up being used for training and testing purposes.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "14. Explain how a recommender system works.",
        "answer": "A recommender system is a system that many consumer-facing, content-driven, online platforms employ to generate recommendations for users from a library of available content. These systems generate recommendations based on what they know about the users’ tastes from their activities on the platform.\nFor example, imagine that we have a movie streaming platform, similar to Netflix or Amazon Prime. If a user has previously watched and liked movies from action and horror genres, then it means that the user likes watching movies of these genres. In that case, it would be better to recommend such movies to this particular user. These recommendations can also be generated based on what users with similar tastes like watching.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "15. What is Data Science?",
        "answer": "Imagine you have a big box of puzzle pieces, each piece is like a bit of information. Data Science is like a puzzle master. You take the pieces, organize them, and find the patterns or images hiding inside them. Then, you use those patterns to make smart decisions, like guessing where the next piece of the puzzle goes. \nIn simple terms, Data Science is like being a detective, finding the cool stuff hidden in pieces of data to help us understand the world and make a smart choice. \nData Science involves several steps: \nData collection: gathering data from different sources like databases, social media, API, and many more.\nData cleaning and preparation: Processing the raw data to remove the noise, and missing values and transform the data in a proper format.\nExploratory data analysis (EDA): Exploring the data to understand the relationship, and patterns through statistics and visualization. \nFeature Engineering: Selecting or creating new features from the data to improve the model performance.\nMachine Learning and Statistics: Building various machine learning models to find out valuable information and make predictions.\nCheck out this comprehensive Data Scientist Course!",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "16. How is Python Useful?",
        "answer": "Python offers various features which make it useful in the field of Data Science:\nFlexibility: Python can be applied in many different ways including web development, data analysis, AI, scientific computing, automation, and plenty of others. It is a versatile programming language that suits several purposes.\nReadability: Python’s syntax has been made to make it easy to learn and understand even for beginners since it is simple and readable. Code collaboration and maintenance are also easily done with this kind of readability.\nAbundant ecosystem: There are vast collections of libraries and frameworks that address almost every aspect of programming in Python, like data (NumPy, Pandas), machine learning (TensorFlow, scikit-learn), and web development (Django, Flask).\nCommunity-driven support: A huge number of developers take part in Python’s development process as well as creating libraries while they also provide tutorials through forums hence making large community support available so far.\nCross-platform language: Python is a platform-independent language. You can write code on one platform and run it on another without changing it. This makes it flexible and highly accessible to other operating systems.\nEase of integration: Python is easily integrated with other languages and systems, and frequently used for scripting and automation tasks; thus it can interface with C/C++, Java libraries, or APIs.\nUnderstand How Data Science and AI were used to Fight Covid-19",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "17. How R is Useful in the Data Science Domain?",
        "answer": "Here are some ways in which R is useful in the data science domain:\nData Manipulation and Analysis: R offers a comprehensive collection of libraries and functions that facilitate proficient data manipulation, transformation, and statistical analysis.\nStatistical Modeling and Machine Learning: R offers a wide range of packages for advanced statistical modeling and machine learning tasks, empowering data scientists to build predictive models and perform complex analyses.\nData Visualization: R’s extensive visualization libraries enable the creation of visually appealing and insightful plots, charts, and graphs.\nReproducible Research: R supports the integration of code, data, and documentation, facilitating reproducible workflows and ensuring transparency in data science projects.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "18. What do you understand about the true-positive rate and false-positive rate?",
        "answer": "True positive rate: In Machine Learning, true-positive rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives that are correctly identified. \nFormula: \n\nFalse positive rate: The false positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false-positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. \nFormula: \n\nCheck out this comprehensive Data Science Course in India!",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "19. How is Data Science different from traditional application programming?",
        "answer": "Unlike traditional programming, data science has the following key differences:\nData Focus: Whereas classical programming focuses on developing software applications, data science is about extracting actionable insights from large datasets.\nProblem-solving: Although conventional coding employs algorithms to solve predefined problems, data science employs a variety of approaches that are based on data.\nInterdisciplinary: On the other hand, traditional programming is more focused on coding while data science combines skills from math, statistics, and computer science.\nExploration vs. Determinism: Data science rather guesses patterns when looking at a dataset while traditional programming follows a predetermined logical order.\nPrediction vs. Execution: In this case, execution involves following directions for data analysis and prediction into future trends respectively carried out by software developers in such a field as information technology nowadays is typically done by scientists using common tools like Python or R script files.\nTools and Technologies: However, it should be noted that some basic computing can be used to code in different languages including C++, Java, or ADA but specialized tools like Python along with machine learning libraries are commonly employed in the field of data science.\nInterested in learning Data Science skills? Check out our Data Science course in Bangalore  Now!",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "20. Explain the differences between supervised and unsupervised learning.",
        "answer": "Supervised and unsupervised learning are two types of Machine Learning techniques. They both allow us to build models. However, they are used for solving different kinds of problems.\nSupervised Learning Unsupervised Learning\nWorks on the data that contains both inputs and the expected output, i.e., the labeled data Works on the data that contains no mappings from input to output, i.e., the unlabeled data\nUsed to create models that can be employed to predict or classify things Used to extract meaningful information out of large volumes of data\nCommonly used supervised learning algorithms: Linear regression, decision tree, etc. Commonly used unsupervised learning algorithms: K-means clustering, Apriori algorithm, etc.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "21. Why is R used in Data Visualization?",
        "answer": "There are several reasons R used in Data Visualization are: \nInclusive Libraries: R offers strong ggplot2 powerful visualization libraries, which has extensive capabilities for creating customizable and publication-quality visualizations.\nGrammar of Graphics: The ggplot2 package adheres to the principles underlying the “Grammar of Graphics”, allowing users to easily construct complex visualizations using less code.\nInteractivity and Flexibility: R visualization libraries are interactive and provide customization options that are flexible enough for the creation of personalized or interactive visuals.\nIntegration with Statistical Analysis: Visualization is an inherent part of statistical analysis in R; hence, it can visualize results emanating from statistical modeling and analysis.\nCommunity and Resources: This means that R has a wide community of data scientists and visualization experts who contribute to its development, share codes, and give support towards learning thereby helping people learn from others’ experiences.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "22. What are the popular libraries used in Data Science?",
        "answer": "Popular Libraries used in Data Science are:\nLibraries  Description \nNumPy Support for arrays, matrices, and mathematical functions necessary for numerical computing in Python.\nPandas Provides data structures and tools that are important in manipulating and analyzing data sets. They are designed to handle structured data like tables.\nMatplotlib The comprehensive library is used for creating static, animated, and interactive visualizations using Python.\nSeaborn Seaborn is based on Matplotlib The purpose of it is to give users an easy way of designing statistical graphics.\nScikit-learn It contains simple yet efficient tools for data mining and data analysis including machine learning algorithms, preprocessing, and model evaluation. \nTensorFlow TensorFlow is an open-source machine learning framework developed by Google specifically for building deep learning models as well as training them.\nKeras Keras provides a high-level neural networks API running on top of TensorFlow which was developed with the goal of enabling fast experimentation with Deep Learning models.\nPyTorch PyTorch is another popular deep learning framework used in research and production because of its flexible nature as well as dynamic computation graphs.\nStatsmodels This module offers classes and functions that estimate statistical models as desired by the user or perform various statistical tests.\nNLTK (Natural Language Toolkit) NLTK is a platform written in Python that helps human beings process natural language. It’s particularly effective when working with text processing and analysis. \nInterested to learn more about Data Science, check out our Data Science Course in Chennai!",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "23. What is Poisson Distribution?",
        "answer": "The Poisson distribution is a statistical probability distribution used to represent the occurrence of events within a specific interval of time or space. It is commonly employed to characterize infrequent events that happen independently and at a consistent average rate, such as quantifying the number of incoming phone calls received within a given hour.\nLearn how to make sure people type in the right email on your website. It’s easy with JavaScript – read email validation in JavaScript",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "24. What is a normal distribution?",
        "answer": "Data distribution is a visualization tool to analyze how data is spread out or distributed. Data can be distributed in various ways. For instance, it could be with a bias to the left or the right, or it could all be jumbled up.\nData may also be distributed around a central value, i.e., mean, median, etc. This kind of distribution has no bias either to the left or to the right and is in the form of a bell-shaped curve. This distribution also has its mean equal to the median. This kind of distribution is called a normal distribution.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "25. What is Deep Learning?",
        "answer": "Deep Learning is a kind of Machine Learning, in which neural networks are used to imitate the structure of the human brain, and just like how a brain learns from information, machines are also made to learn from the information that is provided to them.\nDeep Learning is an advanced version of neural networks to make the machines learn from data. In Deep Learning, the neural networks comprise many hidden layers (which is why it is called ‘deep’ learning) that are connected to each other, and the output of the previous layer is the input of the current layer.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "26. What is CNN (Convolutional Neural Network)?",
        "answer": "A Convolutional Neural Network (CNN) is an advanced deep learning architecture designed specifically for analyzing visual data, such as images and videos. It is composed of interconnected layers of neurons that utilize convolutional operations to extract meaningful features from the input data. CNNs exhibit remarkable effectiveness in tasks like image classification, object detection, and image recognition, thanks to their inherent ability to autonomously learn hierarchical representations and capture spatial relationships within the data, eliminating the need for explicit feature engineering.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "27. What is an RNN (recurrent neural network)?",
        "answer": "A recurrent neural network, or RNN for short, is a kind of Machine Learning algorithm that makes use of the artificial neural network. RNNs are used to find patterns from a sequence of data, such as time series, stock market, temperature, etc. RNNs are a kind of feedforward network, in which information from one layer passes to another layer, and each node in the network performs mathematical operations on the data. These operations are temporal, i.e., RNNs store contextual information about previous computations in the network. It is called recurrent because it performs the same operations on some data every time it is passed. However, the output may be different based on past computations and their results.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "28. Explain selection bias.",
        "answer": "Selection bias is the bias that occurs during the sampling of data. This kind of bias occurs when a sample is not representative of the population, which is going to be analyzed in a statistical study.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "29. Between Python and R, which one will you choose for analyzing the text, and why?",
        "answer": "Due to the following factors, Python will outperform R for text analytics:\nPython’s Pandas module provides high-performance data analysis capabilities as well as simple-to-use data structures.\nPython does all sorts of text analytics more quickly.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "30. Explain the purpose of data cleaning",
        "answer": "Data cleaning’s primary goal is to rectify or eliminate inaccurate, corrupted, improperly formatted, duplicate, or incomplete data from a dataset. This often yields better outcomes and a higher return on investment for marketing and communications efforts.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "31. What do you understand from Recommender System? and State its application",
        "answer": "Recommender Systems are a subclass of information filtering systems designed to forecast the preferences or ratings given to a product by a user.\nThe Amazon product suggestions page is an example of a recommender system in use. Based on the user’s search history and previous orders, this area contains products.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "32. What is Gradient Descent?",
        "answer": "An iterative first-order optimization process called gradient descent (GD) is used to locate the local minimum and maximum of a given function. This technique is frequently applied in machine learning (ML) and deep learning (DL) to minimize a cost/loss function (for example, in linear regression).",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "33. What are the various skills required to become Data Scientist?",
        "answer": "The following skills are necessary to become a certified Data Scientist:\nHaving familiarity with built-in data types like lists, tuples, sets, and related.\nN-dimensional NumPy array knowledge is required.\nBeing able to use Pandas and Dataframes.\nStrong holdover performance in vectors with only one element.\nHands-on experience with Tableau and PowerBI.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "34. What is TensorFlow?",
        "answer": "A free and open-source software library for machine learning and artificial intelligence is called TensorFlow. It enables programmers to build dataflow graphs, which are representations of the flow of data among processing nodes in a graph.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "35. What is Dropout?",
        "answer": "In Data Science, the term “dropout” refers to the process of randomly removing visible and hidden network units. By eliminating up to 20% of the nodes, they avoid overfitting the data and allow for the necessary space to be set up for the network’s iterative convergence process.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "36. State any five Deep Learning Frameworks.",
        "answer": "Some of the Deep Learning frameworks are:\nCaffe\nKeras\nTensorFlow\nPytorch\nChainer\nMicrosoft Cognitive Toolkit",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "37. Define Neural Networks and its types",
        "answer": "Neural Networks are computational models that derive their principles from the structure and functionality of the human brain. Consisting of interconnected artificial neurons organized in layers, Neural Networks exhibit remarkable capacities in learning and discerning patterns within datasets. Consequently, they assume a pivotal role in diverse domains including pattern recognition, classification, and optimization, thereby providing invaluable solutions in the realm of artificial intelligence.\nThere exist various types of Neural Networks, including:\nFeedforward Neural Networks: These networks facilitate a unidirectional information flow, progressing from input to output. They find frequent application in tasks involving pattern recognition and classification.\nConvolutional Neural Networks (CNNs): Specifically tailored for grid-like data, such as images or videos, CNNs leverage convolutional layers to extract meaningful features. Their prowess lies in tasks like image classification and object detection.\nRecurrent Neural Networks (RNNs): RNNs are particularly adept at handling sequential data, wherein the present output is influenced by past inputs. They are extensively utilized in domains such as language modeling and time series analysis.\nLong Short-Term Memory (LSTM) Networks: This variation of RNNs addresses the issue of vanishing gradients and excels at capturing long-term dependencies in data. LSTM networks find wide-ranging applications in areas like speech recognition and natural language processing.\nGenerative Adversarial Networks (GANs): GANs consist of a generator and a discriminator that is trained in a competitive manner. They are employed to generate new data samples and are helpful for tasks like image generation and text synthesis.\nThese examples represent only a fraction of the available variations and architectures tailored to specific data types and problem domains.\n\nData Science Interview Questions For Intermediate",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "38. What is the ROC curve?",
        "answer": "It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model. You can see this in the below graph:",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "39. What do you understand by a decision tree?",
        "answer": "A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.\n\nHere, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label. So, in this case, we have a series of test conditions that give the final decision according to the condition.\nAre you interested in learning Data Science from experts? Enroll in our Data Science Course in Hyderabad now!",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "40. What do you understand by a random forest model?",
        "answer": "It combines multiple models together to get the final output or, to be more precise, it combines multiple decision trees together to get the final output. So, decision trees are the building blocks of the random forest model.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "41. Two candidates, Aman and Mohan appear for a Data Science Job interview. The probability of Aman cracking the interview is 1/8 and that of Mohan is 5/12. What is the probability that at least one of them will crack the interview?",
        "answer": "The probability of Aman getting selected for the interview is 1/8\nP(A) = 1/8\nThe probability of Mohan getting selected for the interview is 5/12\nP(B)=5/12\nNow, the probability of at least one of them getting selected can be denoted at the Union of A and B, which means\nP(A U B) =P(A)+ P(B) – (P(A ∩ B)) ………………………(1)\nWhere P(A ∩ B) stands for the probability of both Aman and Mohan getting selected for the job.\nTo calculate the final answer, we first have to find out the value of P(A ∩ B)\nSo, P(A ∩ B) = P(A) * P(B)\n1/8 * 5/12\n5/96\nNow, put the value of P(A ∩ B) into equation (1)\nP(A U B) =P(A)+ P(B) – (P(A ∩ B))\n1/8 + 5/12 -5/96\nSo, the answer will be 47/96.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "42. How is Data modeling different from Database design?",
        "answer": "Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques.\nDatabase Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "43. What is precision?",
        "answer": "Precision: When we are implementing algorithms for the classification of data or the retrieval of information, precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions. Below is the formula to calculate precision:",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "44. What is a recall?",
        "answer": "Recall: It is the set of all positive predictions out of the total number of positive instances. Recall helps us identify the misclassified positive predictions. We use the below formula to calculate recall:",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "45. What is the F1 score and how to calculate it?",
        "answer": "F1 score helps us calculate the harmonic mean of precision and recall that gives us the test’s accuracy. If F1 = 1, then precision and recall are accurate. If F1 < 1 or equal to 0, then precision or recall is less accurate, or they are completely inaccurate. See below for the formula to calculate the F1 score:",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "46. What is a p-value?",
        "answer": "P-value is the measure of the statistical importance of an observation. It is the probability that shows the significance of output to the data. We compute the p-value to know the test statistics of a model. Typically, it helps us choose whether we can accept or reject the null hypothesis.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "47. Why do we use p-value?",
        "answer": "We use the p-value to understand whether the given data really describes the observed effect or not. We use the below formula to calculate the p-value for the effect ‘E’ and the null hypothesis ‘H0’ is true:",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "48. What is the difference between an error and a residual error?",
        "answer": "An error occurs in values while the prediction gives us the difference between the observed values and the true values of a dataset. Whereas, the residual error is the difference between the observed values and the predicted values. The reason we use the residual error to evaluate the performance of an algorithm is that the true values are never known. Hence, we use the observed values to measure the error using residuals. It helps us get an accurate estimate of the error.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "49. Why do we use the summary function?",
        "answer": "The summary function in R gives us the statistics of the implemented algorithm on a particular dataset. It consists of various objects, variables, data attributes, etc. It provides summary statistics for individual objects when fed into the function. We use a summary function when we want information about the values present in the dataset. It gives us the summary statistics in the following form:\n\nHere, it gives the minimum and maximum values from a specific column of the dataset. Also, it provides the median, mean, 1st quartile, and 3rd quartile values that help us understand the values better.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "50. How are Data Science and Machine Learning related to each other?",
        "answer": "Data Science and Machine Learning are two terms that are closely related but are often misunderstood. Both of them deal with data. However, there are some fundamental distinctions that show us how they are different from each other.\nData Science is a broad field that deals with large volumes of data and allows us to draw insights from this voluminous data. The entire process of data science takes care of multiple steps that are involved in drawing insights out of the available data. This process includes crucial steps such as data gathering, data analysis, data manipulation, data visualization, etc.\nMachine Learning, on the other hand, can be thought of as a sub-field of data science. It also deals with data, but here, we are solely focused on learning how to convert the processed data into a functional model, which can be used to map inputs to outputs, e.g., a model that can expect an image as an input and tell us if that image contains a flower as an output.\nIn short, data science deals with gathering data, processing it, and finally, drawing insights from it. The field of data science that deals with building models using algorithms is called machine learning. Therefore, machine learning is an integral part of data science.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "51. Explain univariate, bivariate, and multivariate analyses.",
        "answer": "When we are dealing with data analysis, we often come across terms such as univariate, bivariate, and multivariate. Let’s try and understand what these mean.\nUnivariate analysis: Univariate analysis involves analyzing data with only one variable or, in other words, a single column or a vector of the data. This analysis allows us to understand the data and extract patterns and trends from it. Example: Analyzing the weight of a group of people.\nBivariate analysis: Bivariate analysis involves analyzing the data with exactly two variables or, in other words, the data can be put into a two-column table. This kind of analysis allows us to figure out the relationship between the variables. Example: Analyzing the data that contains temperature and altitude.\nMultivariate analysis: Multivariate analysis involves analyzing the data with more than two variables. The number of columns of the data can be anything more than two. This kind of analysis allows us to figure out the effects of all other variables (input variables) on a single variable (the output variable).\nExample: Analyzing data about house prices, which contains information about the houses, such as locality, crime rate, area, the number of floors, etc.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "52. How can we handle missing data?",
        "answer": "To be able to handle missing data, we first need to know the percentage of data missing in a particular column so that we can choose an appropriate strategy to handle the situation.\nFor example, if in a column the majority of the data is missing, then dropping the column is the best option, unless we have some means to make educated guesses about the missing values. However, if the amount of missing data is low, then we have several strategies to fill them up.\nOne way would be to fill them all up with a default value or a value that has the highest frequency in that column, such as 0 or 1, etc. This may be useful if the majority of the data in that column contains these values.\nAnother way is to fill up the missing values in the column with the mean of all the values in that column. This technique is usually preferred as the missing values have a higher chance of being closer to the mean than to the mode.\nFinally, if we have a huge dataset and a few rows have values missing in some columns, then the easiest and fastest way is to drop those columns. Since the dataset is large, dropping a few columns should not be a problem anyway.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "53. What is the benefit of dimensionality reduction?",
        "answer": "Dimensionality reduction reduces the dimensions and size of the entire dataset. It drops unnecessary features while retaining the overall information in the data intact. Reduction in dimensions leads to faster processing of the data.\nThe reason why data with high dimensions is considered so difficult to deal with is that it leads to high time consumption while processing the data and training a model on it. Reducing dimensions speeds up this process, removes noise, and also leads to better model accuracy.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "54. What is a bias-variance trade-off in Data Science?",
        "answer": "When building a model using Data Science or Machine Learning, our goal is to build one that has low bias and variance. We know that bias and variance are both errors that occur due to either an overly simplistic model or an overly complicated model. Therefore, when we are building a model, the goal of getting high accuracy is only going to be accomplished if we are aware of the tradeoff between bias and variance.\nBias is an error that occurs when a model is too simple to capture the patterns in a dataset. To reduce bias, we need to make our model more complex. Although making the model more complex can lead to reducing bias, if we make the model too complex, it may end up becoming too rigid, leading to high variance. So, the tradeoff between bias and variance is that if we increase the complexity, the bias reduces and the variance increases, and if we reduce complexity, the bias increases and the variance reduces. Our goal is to find a point at which our model is complex enough to give low bias but not so complex to end up having high variance.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "55. What is RMSE?",
        "answer": "RMSE stands for the root mean square error. It is a measure of accuracy in regression. RMSE allows us to calculate the magnitude of error produced by a regression model. The way RMSE is calculated is as follows:\nFirst, we calculate the errors in the predictions made by the regression model. For this, we calculate the differences between the actual and the predicted values. Then, we square the errors.\nAfter this step, we calculate the mean of the squared errors, and finally, we take the square root of the mean of these squared errors. This number is the RMSE and a model with a lower value of RMSE is considered to produce lower errors, i.e., the model will be more accurate.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "56. What is a kernel function in SVM?",
        "answer": "In the SVM algorithm, a kernel function is a special mathematical function. In simple terms, a kernel function takes data as input and converts it into a required form. This transformation of the data is based on something called a kernel trick, which is what gives the kernel function its name. Using the kernel function, we can transform the data that is not linearly separable (cannot be separated using a straight line) into one that is linearly separable.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "57. How can we select an appropriate value of k in k-means?",
        "answer": "Selecting the correct value of k is an important aspect of k-means clustering. We can make use of the elbow method to pick the appropriate k value. To do this, we run the k-means algorithm on a range of values, e.g., 1 to 15. For each value of k, we compute an average score. This score is also called inertia or the inter-cluster variance.\nThis is calculated as the sum of squares of the distances of all values in a cluster. As k starts from a low value and goes up to a high value, we start seeing a sharp decrease in the inertia value. After a certain value of k, in the range, the drop in the inertia value becomes quite small. This is the value of k that we need to choose for the k-means clustering algorithm.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "58. How can we deal with outliers?",
        "answer": "Outliers can be dealt with in several ways. One way is to drop them. We can only drop the outliers if they have values that are incorrect or extreme. For example, if a dataset with the weights of babies has a value 98.6-degree Fahrenheit, then it is incorrect. Now, if the value is 187 kg, then it is an extreme value, which is not useful for our model.\nIn case the outliers are not that extreme, then we can try:\nA different kind of model. For example, if we were using a linear model, then we can choose a non-linear model\nNormalizing the data, which will shift the extreme values closer to other data points\nUsing algorithms that are not so affected by outliers, such as random forest, etc.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "59. How to calculate the accuracy of a binary classification algorithm using its confusion matrix?",
        "answer": "In a binary classification algorithm, we have only two labels, which are True and False. Before we can calculate the accuracy, we need to understand a few key terms:\nTrue positives: Number of observations correctly classified as True\nTrue negatives: Number of observations correctly classified as False\nFalse positives: Number of observations incorrectly classified as True\nFalse negatives: Number of observations incorrectly classified as False\nTo calculate the accuracy, we need to divide the sum of the correctly classified observations by the number of total observations.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "60. What is ensemble learning?",
        "answer": "When we are building models using Data Science and Machine Learning, our goal is to get a model that can understand the underlying trends in the training data and can make predictions or classifications with a high level of accuracy.\nHowever, sometimes some datasets are very complex, and it is difficult for one model to be able to grasp the underlying trends in these datasets. In such situations, we combine several individual models together to improve performance. This is what is called ensemble learning.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "61. Explain collaborative filtering in recommender systems.",
        "answer": "Collaborative filtering is a technique used to build recommender systems. In this technique, to generate recommendations, we make use of data about the likes and dislikes of users similar to other users. This similarity is estimated based on several varying factors, such as age, gender, locality, etc.\nIf User A, similar to User B, watched and liked a movie, then that movie will be recommended to User B, and similarly, if User B watched and liked a movie, then that would be recommended to User A.\nIn other words, the content of the movie does not matter much. When recommending it to a user what matters is if other users similar to that particular user liked the content of the movie or not.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "62. Explain content-based filtering in recommender systems.",
        "answer": "Content-based filtering is one of the techniques used to build recommender systems. In this technique, recommendations are generated by making use of the properties of the content that a user is interested in.\nFor example, if a user is watching movies belonging to the action and mystery genre and giving them good ratings, it is a clear indication that the user likes movies of this kind. If shown movies of a similar genre as recommendations, there is a higher probability that the user would like those recommendations as well.\nIn other words, here, the content of the movie is taken into consideration when generating recommendations for users.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "63. Explain bagging in Data Science.",
        "answer": "Bagging is an ensemble learning method. It stands for bootstrap aggregating. In this technique, we generate some data using the bootstrap method, in which we use an already existing dataset and generate multiple samples of the N size. This bootstrapped data is then used to train multiple models in parallel, which makes the bagging model more robust than a simple model.\nOnce all the models are trained, then it’s time to make a prediction, we make predictions using all the trained models and then average the result in the case of regression, and for classification, we choose the result, generated by models, that have the highest frequency.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "64. Explain boosting in data science.",
        "answer": "Boosting is one of the ensemble learning methods. Unlike bagging, it is not a technique used to parallelly train our models. In boosting, we create multiple models and sequentially train them by combining weak models iteratively in a way that training a new model depends on the models trained before it.\nIn doing so, we take the patterns learned by a previous model and test them on a dataset when training the new model. In each iteration, we give more importance to observations in the dataset that are incorrectly handled or predicted by previous models. Boosting is useful in reducing bias in models as well.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "65. Explain stacking in data science.",
        "answer": "Just like bagging and boosting, stacking is also an ensemble learning method. In bagging and boosting, we could only combine weak models that used the same learning algorithms, e.g., logistic regression. These models are called homogeneous learners.\nHowever, in stacking, we can combine weak models that use different learning algorithms as well. These learners are called heterogeneous learners. Stacking works by training multiple (and different) weak models or learners and then using them together by training another model, called a meta-model, to make predictions based on the multiple outputs of predictions returned by these multiple weak models.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "66. Explain how machine learning is different from deep learning.",
        "answer": "A field of computer science, machine learning is a subfield of data science that deals with using existing data to help systems automatically learn new skills to perform different tasks without having rules to be explicitly programmed.\nDeep Learning, on the other hand, is a field in machine learning that deals with building machine learning models using algorithms that try to imitate the process of how the human brain learns from the information in a system for it to attain new capabilities. In deep learning, we make heavy use of deeply connected neural networks with many layers.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "67. What does the word ‘Naive’ mean in Naive Bayes?",
        "answer": "Naive Bayes is a data science algorithm. It has the word ‘Bayes’ in it because it is based on the Bayes theorem, which deals with the probability of an event occurring given that another event has already occurred.\nIt has ‘naive’ in it because it makes the assumption that each variable in the dataset is independent of the other. This kind of assumption is unrealistic for real-world data. However, even with this assumption, it is very useful for solving a range of complicated problems, e.g., spam email classification, etc.\nTo learn more about Data Science, check out our Data Science Course in Mumbai.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "68. What is batch normalization?",
        "answer": "One method for attempting to enhance the functionality and stability of the neural network is batch normalization. To do this, normalize the inputs in each layer such that the mean output activation stays at 0 and the standard deviation is set to 1.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "69. What do you understand from cluster sampling and systematic sampling?",
        "answer": "Cluster sampling is also known as the probability sampling approach where you can divide a population into groups, such as districts or schools, and then select a representative sample from among these groups at random. A modest representation of the population as a whole should be present in each cluster.\nA probability sampling strategy called systematic sampling involves picking people from the population at regular intervals, such as every 15th person on a population list. The population can be organized randomly to mimic the benefits of simple random sampling.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "70. What is the Computational Graph?",
        "answer": "A directed graph with variables or operations as nodes is a computational graph. Variables can contribute to operations with their value, and operations can contribute their output to other operations. In this manner, each node in the graph establishes a function of the variables.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "71. What is the difference between Batch and Stochastic Gradient Descent?",
        "answer": "The differences between Batch and Stochastic Gradient Descent are as follows:\nBatch  Stochastic Gradient Descent\nProvides assistance in calculating the gradient utilizing the entire set of data. Helps in calculating the gradient using only a single sample.\nTakes time to converge. Takes less time to converge.\nThe volume is substantial enough for analysis. The volume is lower for analysis purposes.\nUpdates the weight infrequently. Updates the weight more frequently.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "72. What is an activation function?",
        "answer": "An activation function is a function that is incorporated into an artificial neural network to aid in the network’s learning of complicated patterns in the input data. In contrast to a neuron-based model seen in human brains, the activation function determines what signals should be sent to the following neuron at the very end.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "73. How Do You Build a random forest model?",
        "answer": "The steps for creating a random forest model are as follows: \nChoose n from a dataset of k records. \nCreate distinct decision trees for each of the n data values being taken into account. From each of them, a projected result is obtained.  \nEach of the findings is subjected to a voting mechanism.  \nThe final outcome is determined by whose prediction received the most support.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "74. Can you avoid overfitting your model? If yes, then how?",
        "answer": "In actuality, data models may be overfitting. For it, the strategies listed below can be applied:\nIncrease the amount of data in the dataset under study to make it simpler to separate the links between the input and output variables. \nTo discover important traits or parameters that need to be examined, use feature selection. \nUse regularization strategies to lessen the variation of the outcomes a data model generates. \nRarely, datasets are stabilized by adding a little amount of noisy data. This practice is called data augmentation.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "75. What is Cross Validation?",
        "answer": "Cross-validation is a model validation method used to assess the generalizability of statistical analysis results to other data sets. It is frequently applied when forecasting is the main objective and one wants to gauge how well a model will work in real-world applications.\nIn order to prevent overfitting and gather knowledge on how the model will generalize to different data sets, cross-validation aims to establish a data set to test the model during the training phase (i.e. validation data set).",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "76. What is variance in Data Science?",
        "answer": "Variance is a type of error that occurs in a Data Science model when the model ends up being too complex and learns features from data, along with the noise that exists in it. This kind of error can occur if the algorithm used to train the model has high complexity, even though the data and the underlying patterns and trends are quite easy to discover. This makes the model a very sensitive one that performs well on the training dataset but poorly on the testing dataset, and on any kind of data that the model has not yet seen. Variance generally leads to poor accuracy in testing and results in overfitting.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "77. What is pruning in a decision tree algorithm?",
        "answer": "Pruning a decision tree is the process of removing the sections of the tree that are not necessary or are redundant. Pruning leads to a smaller decision tree, which performs better and gives higher accuracy and speed.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "78. What is entropy in a decision tree algorithm?",
        "answer": "In a decision tree algorithm, entropy is the measure of impurity or randomness. The entropy of a given dataset tells us how pure or impure the values of the dataset are. In simple terms, it tells us about the variance in the dataset.\n1\n2\n3\n4\n5\nEntropy(D) = - p * log2(p) - (1 - p) * log2(1 - p)\nwhere: \nEntropy(D) represents the entropy of the dataset D\np represents the proportion of positive class instances in D\nlog2 represents the logarithm to the base 2.\nFor example, suppose we are given a box with 10 blue marbles. Then, the entropy of the box is 0 as it contains marbles of the same color, i.e., there is no impurity. If we need to draw a marble from the box, the probability of it being blue will be 1.0. However, if we replace 4 of the blue marbles with 4 red marbles in the box, then the entropy increases to 0.4 for drawing blue marbles.\nAdditionally, In a decision tree algorithm, multi-class entropy is a measure used to evaluate the impurity or disorder of a dataset with respect to the class labels when there are multiple classes involved. It is commonly used as a criterion to make decisions about splitting nodes in a decision tree.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "79. What information is gained in a decision tree algorithm?",
        "answer": "When building a decision tree, at each step, we have to create a node that decides which feature we should use to split data, i.e., which feature would best separate our data so that we can make predictions. This decision is made using information gain, which is a measure of how much entropy is reduced when a particular feature is used to split the data. The feature that gives the highest information gain is the one that is chosen to split the data.\nLet’s consider a practical example to gain a better understanding of how information gain operates within a decision tree algorithm. Imagine we have a dataset containing customer information such as age, income, and purchase history. Our objective is to predict whether a customer will make a purchase or not.\nTo determine which attribute provides the most valuable information, we calculate the information gain for each attribute. If splitting the data based on income leads to subsets with significantly reduced entropy, it indicates that income plays a crucial role in predicting purchase behavior. Consequently, income becomes a crucial factor in constructing the decision tree as it offers valuable insights.\nBy maximizing information gain, the decision tree algorithm identifies attributes that effectively reduce uncertainty and enable accurate splits. This process enhances the model’s predictive accuracy, enabling informed decisions pertaining to customer purchases.\nExplore this Data Science Course in Delhi and master the decision tree algorithm.\n\nData Science Interview Questions For Experienced",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "80. From the below given ‘diamonds’ dataset, extract only those rows where the ‘price’ value is greater than 1000 and the ‘cut’ is ideal.",
        "answer": "First, we will load the ggplot2 package:\n1\nlibrary(ggplot2)\nNext, we will use the dplyr package:\n1\nlibrary(dplyr)// It is based on the grammar of data manipulation.\nTo extract those particular records, use the below command:\n1\ndiamonds %>% filter(price>1000 & cut==”Ideal”)-> diamonds_1000_idea",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "81. Make a scatter plot between ‘price’ and ‘carat’ using ggplot. ‘Price’ should be on the y-axis, ’carat’ should be on the x-axis, and the ‘color’ of the points should be determined by ‘cut.’",
        "answer": "We will implement the scatter plot using ggplot.\nThe ggplot is based on the grammar of data visualization, and it helps us stack multiple layers on top of each other.\nSo, we will start with the data layer, and on top of the data layer we will stack the aesthetic layer. Finally, on top of the aesthetic layer we will stack the geometry layer.\nCode:\n1\n>ggplot(data=diamonds, aes(x=caret, y=price, col=cut))+geom_point()",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "82. Introduce 25 percent missing values in this ‘iris’ dataset and impute the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median.’",
        "answer": "To introduce missing values, we will be using the missForest package:\n1\nlibrary(missForest)\nUsing the prodNA function, we will be introducing 25 percent of missing values:\n1\nIris.mis<-prodNA(iris,noNA=0.25)\nFor inputing the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median,’ we will be using the Hmisc package and the impute function:\n1\n2\n3\nlibrary(Hmisc)\niris.mis$Sepal.Length<-with(iris.mis, impute(Sepal.Length,mean))\niris.mis$Petal.Length<-with(iris.mis, impute(Petal.Length,median))",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "83. Implement simple linear regression in R on this ‘mtcars’ dataset, where the dependent variable is ‘mpg’ and the independent variable is ‘disp.’",
        "answer": "Here, we need to find how ‘mpg’ varies w.r.t displacement of the column.\nWe need to divide this data into the training dataset and the testing dataset so that the model does not overfit the data.\nSo, what happens is when we do not divide the dataset into these two components, it overfits the dataset. Hence, when we add new data, it fails miserably on that new data.\nTherefore, to divide this dataset, we would require the caret package. This caret package comprises the createdatapartition() function. This function will give the true or false labels.\nHere, we will use the following code:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\nlibrary(caret)\n split_tag<-createDataPartition(mtcars$mpg, p=0.65, list=F)\n mtcars&#91;split_tag,&#93;->train\n mtcars[-split_tag,]->test\n lm(mpg-data,data=train)->mod_mtcars\n predict(mod_mtcars,newdata=test)->pred_mtcars\n >head(pred_mtcars)\nExplanation:\nParameters of the createDataPartition function: First is the column which determines the split (it is the mpg column).\nSecond is the split ratio which is 0.65, i.e., 65 percent of records will have true labels and 35 percent will have false labels. We will store this in a split_tag object.\nOnce we have the split_tag object ready, from this entire mtcars dataframe, we will select all those records where the split tag value is true and store those records in the training set.\nSimilarly, from the mtcars dataframe, we will select all those record where the split_tag value is false and store those records in the test set.\nSo, the split tag will have true values in it, and when we put ‘-’ symbol in front of it, ‘-split_tag’ will contain all of the false labels. We will select all those records and store them in the test set.\nWe will go ahead and build a model on top of the training set, and for the simple linear model we will require the lm function.\n1\nlm(mpg-data,data=train)->mod_mtcars\nNow, we have built the model on top of the train set. It’s time to predict the values on top of the test set. For that, we will use the predict function that takes in two parameters: first is the model which we have built and the second is the dataframe on which we have to predict values.\nThus, we have to predict values for the test set and then store them in pred_mtcars.\n1\npredict(mod_mtcars,newdata=test)->pred_mtcars\nOutput:\nThese are the predicted values of mpg for all of these cars.\n\nSo, this is how we can build a simple linear model on top of this mtcars dataset.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "84. Calculate the RMSE values for the model building.",
        "answer": "When we build a regression model, it predicts certain y values associated with the given x values, but there is always an error associated with this prediction. So, to get an estimate of the average error in prediction, RMSE is used.\nCode:\n1\n2\n3\n4\n5\n6\n7\n8\n9\ncbind(Actual=test$mpg, predicted=pred_mtcars)->final_data\n as.data.frame(final_data)->final_data\n error<-(final_data$Actual-final_data$Prediction)\n cbind(final_data,error)->final_data\n sqrt(mean(final_data$error)^2)\nExplanation: We have the actual and the predicted values. We will bind both of them into a single dataframe. For that, we will use the cbind function:\n1\ncbind(Actual=test$mpg, predicted=pred_mtcars)->final_data\nOur actual values are present in the mpg column from the test set, and our predicted values are stored in the pred_mtcars object which we have created in the previous question. Hence, we will create this new column and name the column actual. Similarly, we will create another column and name it predicted which will have predicted values, and then store the predicted values in the new object which is final_data. After that, we will convert a matrix into a dataframe. So, we will use the as.data.frame function and convert this object (predicted values) into a dataframe:\n1\nas.data.frame(final_data)->final_data\nWe will pass this object which is final_data and store the result in final_data again. We will then calculate the error in prediction for each of the records by subtracting the predicted values from the actual values:\n1\nerror <-(final_data$Actual-final_data$Prediction)\nThen, store this result on a new object and name that object as error. After this, we will bind this error calculated to the same final_data dataframe:\n1\ncbind(final_data,error)->final_data //binding error object to this final_data\nHere, we bind the error object to this final_data, and store this into final_data again. Calculating RMSE:\n1\nSqrt(mean(final_data$error)^2)\nOutput:\n1\n[1] 4.334423\nNote: Lower the value of RMSE, the better the model.\nR and Python are two of the most important programming languages for Machine Learning Algorithms.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "85. Implement simple linear regression in Python on this ‘Boston’ dataset where the dependent variable is ‘medv’ and the independent variable is ‘lstat.’",
        "answer": "Simple Linear Regression\n1\n2\n3\n4\n5\n6\n7\nimport pandas as pd\n data=pd.read_csv(‘Boston.csv’)     //loading the Boston dataset\n data.head()  //having a glance at the head of this data\n data.shape\nLet us take out the dependent and the independent variables from the dataset:\n1\n2\n3\ndata1=data.loc[:,[‘lstat’,’medv’]]\n data1.head()\nVisualizing Variables\n1\n2\n3\n4\n5\n6\n7\n8\n9\nimport matplotlib.pyplot as plt\n data1.plot(x=’lstat’,y=’medv’,style=’o’)\n plt.xlabel(‘lstat’)\n plt.ylabel(‘medv’)\n plt.show()\nHere, ‘medv’ is basically the median value of the price of the houses, and we are trying to find out the median values of the price of the houses with respect to to the lstat column.\nWe will separate the dependent and the independent variable from this entire dataframe:\n1\ndata1=data.loc[:,[‘lstat’,’medv’]]\nThe only columns we want from all of this record are ‘lstat’ and ‘medv,’ and we need to store these results in data1.\nNow, we would also do a visualization w.r.t to these two columns:\n1\n2\n3\n4\n5\n6\n7\n8\n9\nimport matplotlib.pyplot as plt\n data1.plot(x=’lstat’,y=’medv’,style=’o’)\n plt.xlabel(‘lstat’)\n plt.ylabel(‘medv’)\n plt.show()\nPreparing the Data\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\nX=pd.Dataframe(data1[‘lstat’])\n Y=pd.Dataframe(data1[‘medv’])\n from sklearn.model_selection import train_test_split\n X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n from sklearn.linear_model import LinearRegression\n regressor=LinearRegression()\n regressor.fit(X_train,y_train)\n \n1\nprint(regressor.intercept_)\nOutput :\n1\n2\n3\n34.12654201\n print(regressor.coef_)//this is the slope\nOutput :\n1\n[[-0.913293]]\nBy now, we have built the model. Now, we have to predict the values on top of the test set:\n1\ny_pred=regressor.predict(X_test)//using the instance and the predict function and pass the X_test object inside the function and store this in the y_pred object\nNow, let’s have a glance at the rows and columns of the actual values and the predicted values:\n1\nY_pred.shape, y_test.shape\nOutput :\n1\n((102,1),(102,1))\nFurther, we will go ahead and calculate some metrics so that we can find out the Mean Absolute Error, Mean Squared Error, and RMSE.\n1\n2\n3\n4\n5\n6\n7\nfrom sklearn import metrics import NumPy as np\n print(‘Mean Absolute Error: ’, metrics.mean_absolute_error(y_test, y_pred))\n print(‘Mean Squared Error: ’, metrics.mean_squared_error(y_test, y_pred))\n print(‘Root Mean Squared Error: ’, np.sqrt(metrics.mean_absolute_error(y_test, y_pred))\nOutput:\n1\n2\n3\n4\n5\nMean Absolute Error: 4.692198\n Mean Squared Error: 43.9198\n Root Mean Squared Error: 6.6270",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "86. Implement logistic regression on this ‘heart’ dataset in R where the dependent variable is ‘target’ and the independent variable is ‘age.’",
        "answer": "For loading the dataset, we will use the read.csv function:\n1\n2\nread.csv(“D:/heart.csv”)->heart\nstr(heart)\nIn the structure of this dataframe, most of the values are integers. However, since we are building a logistic regression model on top of this dataset, the final target column is supposed to be categorical. It cannot be an integer. So, we will go ahead and convert them into a factor.\nThus, we will use the as.factor function and convert these integer values into categorical data.\nWe will pass on heart$target column over here and store the result in heart$target as follows:\n1\nas.factor(heart$target)->;heart$target\nNow, we will build a logistic regression model and see the different probability values for the person to have heart disease on the basis of different age values.\nTo build a logistic regression model, we will use the glm function:\n1\nglm(target~age, data=heart, family=”binomial”)->;log_mod1\nHere, target~age indicates that the target is the dependent variable and the age is the independent variable, and we are building this model on top of the dataframe.\nfamily=”binomial” means we are basically telling R that this is the logistic regression model, and we will store the result in log_mod1.\nWe will have a glance at the summary of the model that we have just built:\n1\nsummary(log_mod1)\n\nWe can see Pr value here, and there are three stars associated with this Pr value. This basically means that we can reject the null hypothesis which states that there is no relationship between the age and the target columns. But since we have three stars over here, this null hypothesis can be rejected. There is a strong relationship between the age column and the target column.\nNow, we have other parameters like null deviance and residual deviance. Lower the deviance value, the better the model.\nThis null deviance basically tells the deviance of the model, i.e., when we don’t have any independent variable and we are trying to predict the value of the target column with only the intercept. When that’s the case, the null deviance is 417.64.\nResidual deviance is wherein we include the independent variables and try to predict the target columns. Hence, when we include the independent variable which is age, we see that the residual deviance drops. Initially, when there are no independent variables, the null deviance was 417. After we include the age column, we see that the null deviance is reduced to 401.\nThis basically means that there is a strong relationship between the age column and the target column and that is why the deviance is reduced.\nAs we have built the model, it’s time to predict some values:\n1\n2\n3\n4\n5\npredict(log_mod1, data.frame(age=30), type=”response”)\n predict(log_mod1, data.frame(age=50), type=”response”)\n predict(log_mod1, data.frame(age=29:77), type=”response”)\nNow, we will divide this dataset into train and test sets and build a model on top of the train set and predict the values on top of the test set:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n>library(caret)\n Split_tag<- createDataPartition(heart$target, p=0.70, list=F) heart[split_tag,]->train\n heart[-split_tag,]->test\n glm(target~age, data=train,family=”binomial”)->log_mod2\n predict(log_mod2, newdata=test, type=”response”)->pred_heart\n range(pred_heart)",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "87. Build a ROC curve for the model built",
        "answer": "The below code will help us in building the ROC curve:\n1\n2\n3\n4\n5\n6\n7\nlibrary(ROCR)\n prediction(pred_heart, test$target)-> roc_pred_heart\n performance(roc_pred_heart, “tpr”, “fpr”)->roc_curve\n plot(roc_curve, colorize=T)\nGraph:\nGo through this Data Science Course in Pune to get a clear understanding of Data Science!",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "88. Build a confusion matrix for the model where the threshold value for the probability of predicted values is 0.6, and also find the accuracy of the model.",
        "answer": "Accuracy is calculated as:\nAccuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)\nTo build a confusion matrix in R, we will use the table function:\n1\ntable(test$target,pred_heart > 0.6)\nHere, we are setting the probability threshold as 0.6. So, wherever the probability of pred_heart is greater than 0.6, it will be classified as 0, and wherever it is less than 0.6 it will be classified as 1.\nThen, we calculate the accuracy by the formula for calculating Accuracy.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "89. Build a logistic regression model on the ‘customer_churn’ dataset in Python. The dependent variable is ‘Churn’ and the independent variable is ‘MonthlyCharges.’ Find the log_loss of the model.",
        "answer": "First, we will load the pandas dataframe and the customer_churn.csv file:\n1\ncustomer_churn=pd.read_csv(“customer_churn.csv”)\n\nAfter loading this dataset, we can have a glance at the head of the dataset by using the following command:\n1\ncustomer_churn.head()\nNow, we will separate the dependent and the independent variables into two separate objects:\n1\n2\n3\n4\n5\n6\n7\n8\n9\nx=pd.Dataframe(customer_churn[‘MonthlyCharges’])\n y=customer_churn[‘ Churn’]\n #Splitting the data into training and testing sets\n from sklearn.model_selection import train_test_split\n x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3, random_state=0)\nNow, we will see how to build the model and calculate log_loss.\n1\n2\n3\n4\n5\n6\n7\nfrom sklearn.linear_model, we have to import LogisticRegression\n l=LogisticRegression()\n l.fit(x_train,y_train)\n y_pred=l.predict_proba(x_test)\nAs we are supposed to calculate the log_loss, we will import it from sklearn.metrics:\n1\n2\n3\nfrom sklearn.metrics import log_loss\n print(log_loss(y_test,y_pred)//actual values are in y_test and predicted are in y_pred\nOutput:\n1\n0.5555020595194167\nBecome a master of Data Science by going through this online Data Science Course in Gurgaon!",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "90. Build a decision tree model on ‘Iris’ dataset where the dependent variable is ‘Species,’ and all other columns are independent variables. Find the accuracy of the model built.",
        "answer": "To build a decision tree model, we will be loading the party package:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n#party package\n library(party)\n #splitting the data\n library(caret)\n split_tag<-createDataPartition(iris$Species, p=0.65, list=F)\n iris&#91;split_tag,&#93;->train\n iris[~split_tag,]->test\n #building model\n mytree<-ctree(Species~.,train)&#91;/code&#93;\n Now we will plot the model\n &#91;code language=\"javascript\"&#93;plot(mytree)&#91;/code&#93;\n <strong>Model:</strong>\n<img class=\"alignnone size-full wp-image-204079\" src=\"https://intellipaat.com/blog/wp-content/uploads/2015/09/Graphics-06.jpg\" alt=\"Data Science Interview Questions and Answers - Intellipaat\" width=\"800\" height=\"380\" />\n [code language=\"javascript\"]#predicting the values\n predict(mytree,test,type=’response’)->mypred\nAfter this, we will predict the confusion matrix and then calculate the accuracy using the table function:\n1\ntable(test$Species, mypred)",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "91. Build a random forest model on top of this ‘CTG’ dataset, where ‘NSP’ is the dependent variable and all other columns are independent variables.",
        "answer": "We will load the CTG dataset by using read.csv:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\ndata<read.csv(“C:/Users/intellipaat/Downloads/CTG.csv”,header=True)\n str(data)&#91;/code&#93;\n Converting the integer type to a factor\n &#91;code language=\"javascript\"&#93;data$NSP<-as.factor(data$NSP)\n table(data$NSP)\n #data partition\n set.seed(123)\n split_tag<-createDataPartition(data$NSP, p=0.65, list=F)\n data&#91;split_tag,&#93;->train\n data[~split_tag,]->test\n #random forest -1\n library(randomForest)\n set.seed(222)\n rf-<-randomForest(NSP~.,data=train)\n rf\n #prediction\n predict(rf,test)->p1\nBuilding confusion matrix and calculating accuracy:\n1\ntable(test$NSP,p1)\n\nIf you have any doubts or queries related to Data Science, get them clarified from Data Science experts on our Data Science Community!",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "92. Write a function to calculate the Euclidean distance between two points.",
        "answer": "The formula for calculating the Euclidean distance between two points (x1, y1) and (x2, y2) is as follows:\n1\n√(((x1 - x2) ^ 2) + ((y1 - y2) ^ 2))\nCode for calculating the Euclidean distance is as given below:\n1\n2\ndef euclidean_distance(P1, P2):\nreturn (((P1[0] - P2[0]) ** 2) + ((P1[1] - P2[1]) ** 2)) ** .5",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "93. Write code to calculate the root mean square error (RMSE) given the lists of values as actual and predicted.",
        "answer": "To calculate the root mean square error (RMSE), we have to:\nCalculate the errors, i.e., the differences between the actual and the predicted values\nSquare each of these errors\nCalculate the mean of these squared errors\nReturn the square root of the mean\nThe code in Python for calculating RMSE is given below:\n1\n2\n3\n4\n5\ndef rmse(actual, predicted):\nerrors = [abs(actual[i] - predicted[i]) for i in range(0, len(actual))]\nsquared_errors = [x ** 2 for x in errors]\nmean = sum(squared_errors) / len(squared_errors)\nreturn mean ** .5\nCheck out this Machine Learning Course to get an in-depth understanding of Machine Learning.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "94. Mention the different kernel functions that can be used in SVM.",
        "answer": "In SVM, there are four types of kernel functions:\n \nLinear kernel\nIn SVM (Support Vector Machines), a linear kernel is a type of kernel function used to transform input data into a higher-dimensional feature space. It is represented by the equation K(x, y) = x • y, where x and y are feature vectors. The linear kernel calculates the dot product between the two vectors to measure their similarity or dissimilarity.\nPolynomial kernel\nA polynomial kernel is a type of kernel function used to transform input data into a higher-dimensional feature space. It is represented by the equation K(x, y) = (x • y + c)^d, where x and y are feature vectors, c is a constant, and d is the degree of the polynomial. The polynomial kernel captures nonlinear relationships between data points by raising the dot product to a specified power.\nRadial basis kernel\nIn SVM (Support Vector Machines), a radial basis kernel, also known as the Gaussian kernel, is a popular kernel function used for non-linear classification. It is represented by the equation K(x, y) = exp(-gamma * ||x – y||^2), where x and y are feature vectors, and gamma is a parameter that determines the influence of each training example. The radial basis kernel measures the similarity between data points based on their Euclidean distance in the feature space.\nSigmoid kernel\nThe sigmoid kernel is a type of non-linear kernel function commonly employed for classification tasks. It can be mathematically described by the equation K(x, y) = tanh(alpha * x * y + c), where x and y represent feature vectors, and alpha and c are parameters determining the sigmoid function’s shape. By utilizing the sigmoid kernel, Support Vector Machines (SVMs) can project data onto a higher-dimensional space, enabling the creation of non-linear decision boundaries for accurate classification.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "95. How to detect if the time series data is stationary?",
        "answer": "Time series data is considered stationary when variance or mean is constant with time. If the variance or mean does not change over a period of time in the dataset, then we can draw the conclusion that, for that period, the data is stationary.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "96. Write code to calculate the accuracy of a binary classification algorithm using its confusion matrix.",
        "answer": "We can use the code given below to calculate the accuracy of a binary classification algorithm:\n1\n2\n3\n4\n5\ndef accuracy_score(matrix):\ntrue_positives = matrix[0][0]\ntrue_negatives = matrix[1][1]\ntotal_observations = sum(matrix[0]) + sum(matrix[1])\nreturn (true_positives + true_negatives) / total_observations",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "97. What does root cause analysis mean?",
        "answer": "Root cause analysis is the process of figuring out the root causes that lead to certain faults or failures. A factor is considered to be a root cause if, after eliminating it, a sequence of operations, leading to a fault, error, or undesirable result, ends up working correctly. Root cause analysis is a technique that was initially developed and used in the analysis of industrial accidents, but now, it is used in a wide variety of areas.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "98. What is A/B testing?",
        "answer": "A/B testing is a kind of statistical hypothesis testing for randomized experiments with two variables. These variables are represented as A and B. A/B testing is used when we wish to test a new feature in a product. In the A/B test, we give users two variants of the product, and we label these variants as A and B.\nThe A variant can be the product with the new feature added, and the B variant can be the product without the new feature. After users use these two products, we capture their ratings for the product.\nIf the rating of product variant A is statistically and significantly higher, then the new feature is considered an improvement and useful and is accepted. Otherwise, the new feature is removed from the product.\nCheck out this Python Course to get deeper into Python programming.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "99. Out of collaborative filtering and content-based filtering, which one is considered better, and why?",
        "answer": "Content-based filtering is considered to be better than collaborative filtering for generating recommendations. It does not mean that collaborative filtering generates bad recommendations.\nHowever, as collaborative filtering is based on the likes and dislikes of other users we cannot rely on it much. Also, users’ likes and dislikes may change in the future.\nFor example, there may be a movie that a user likes right now but did not like 10 years ago. Moreover, users who are similar in some features may not have the same taste in the kind of content that the platform provides.\nIn the case of content-based filtering, we make use of users’ own likes and dislikes which are much more reliable and yield more positive results. This is why platforms such as Netflix, Amazon Prime, Spotify, etc. make use of content-based filtering for generating recommendations for their users.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "100. In the following confusion matrix, calculate precision and recall.",
        "answer": "Total = 510 Actual\nPredicted P N\nP 156 11\nN 16 327\nThe formulae for precision and recall are given below.\n1\n2\n3\n4\n5\n6\n7\nPrecision:\n(True Positive) / (True Positive + False Positive)\nRecall:\n(True Positive) / (True Positive + False Negative)\nBased on the given data, precision and recall are:\nPrecision: 156 / (156 + 11) = 93.4\nRecall: 156 / (156 + 16) = 90.7",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "101. Write a function that when called with a confusion matrix for a binary classification model returns a dictionary with its precision and recall.",
        "answer": "We can use the below for this purpose:\n1\n2\n3\n4\n5\n6\n7\ndef calculate_precsion_and_recall(matrix):\ntrue_positive  = matrix[0][0]\nfalse_positive  = matrix[0][1]\nfalse_negative = matrix[1][0]\nreturn {\n   'precision': (true_positive) / (true_positive + false_positive),\n   'recall': (true_positive) / (true_positive + false_negative)",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "102. What is reinforcement learning?",
        "answer": "Reinforcement learning is a kind of Machine Learning, which is concerned with building software agents that perform actions to attain the most cumulative rewards.\nA reward here is used for letting the model know (during training) if a particular action leads to the attainment of or brings it closer to the goal. For example, if we are creating an ML model that plays a video game, the reward is going to be either the points collected during the play or the level reached in it.\nReinforcement learning is used to build these kinds of agents that can make real-world decisions that should move the model toward the attainment of a clearly defined goal.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "103. Explain TF/IDF vectorization.",
        "answer": "The expression ‘TF/IDF’ stands for the Term Frequency–Inverse Document Frequency. It is a numerical measure that allows us to determine how important a word is to a document in a collection of documents called a corpus. TF/IDF is used often in text mining and information retrieval.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "104. What are the assumptions required for linear regression?",
        "answer": "There are several assumptions required for linear regression. They are as follows:\nThe data, which is a sample drawn from a population, used to train the model should be representative of the population.\nThe relationship between independent variables and the mean of dependent variables is linear.\nThe variance of the residual is going to be the same for any value of an independent variable. It is also represented as X.\nEach observation is independent of all other observations.\nFor any value of an independent variable, the independent variable is normally distributed.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "105. What happens when some of the assumptions required for linear regression are violated?",
        "answer": "These assumptions may be violated lightly (i.e., some minor violations) or strongly (i.e., the majority of the data has violations). Both of these violations will have different effects on a linear regression model.\nStrong violations of these assumptions make the results entirely redundant. Light violations of these assumptions make the results have greater bias or variance.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "106. How to deal with unbalanced binary classification?",
        "answer": "Given below are the following points that will teach you to deal with unbalanced binary classification:\nUse other formulas to determine the model’s performance, such as precision/recall, F1 score, etc.\nRe-sample the data using strategies such as undersampling (decreasing the sample size of the bigger class), oversampling (raising the sample size of the smaller class using repetition, SMOTE, and other similar strategies), and so on.\nK-fold cross-validation is used\nUse ensemble learning such that each decision tree only takes into account a portion of the bigger class and the complete sample of the smaller class.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "107. Which cross-validation method would you use for a batch of time series data?",
        "answer": "Instead of utilizing k-fold cross-validation, you should be aware that a time series is fundamentally organized by chronological order and is not made up of randomly dispersed data. Use approaches like forward-chaining, where you model on previous data and then look at forward-facing data, when dealing with time series data.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "108. How can time-series data be declared as stationery?",
        "answer": "The time series is considered stationary when its essential constituents don’t change over time. These variables might be variance or mean. Static time series exhibit no trends nor seasonal impacts. Data from stationary time series are required for data science models.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "109. Difference between Point Estimates and Confidence Interval.",
        "answer": "Point Estimates: A specific number known as the point estimate provides an estimate of the population parameter. The Maximum Likelihood estimator and the Method of Moments are two common techniques used to produce Population Parameter Point, estimators.\nConfidence Interval: The confidence interval provides a range of values that most likely contain the population parameter. It even reveals the likelihood that the population parameter may be found in that specific period. The likelihood or similarity is represented by the Confidence Coefficient (or Confidence level), which is indicated by 1-alpha. The significance level is indicated by alpha.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "110. Define the terms KPI, lift, model fitting, robustness, and DOE.",
        "answer": "KPI: KPI stands for Key Performance Indicator, which evaluates how successfully a company accomplishes its goals.\nLift: Lift is a performance indicator for the target model compared to a random selection model. Lift measures how well the model predicts in comparison to no model.\nModel fitting: This describes how well the proposed model conforms to the available data.\nRobustness: This refers to how well the system can manage variations and changes.\nDOE: DOE refers to the task design with the goal of describing and explaining information variance under postulated circumstances to reflect variables.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "111. What are LLMs?",
        "answer": "Large Language Models, abbreviated as LLMs, are sophisticated artificial intelligence models designed to process and generate text that resembles human language based on the input they receive. They employ advanced techniques like deep learning, particularly neural networks, to comprehend and produce language patterns, enabling them to answer questions, engage in conversations, and provide information on a broad array of topics.\nLLMs undergo training using extensive sets of textual data from diverse sources, including books, websites, and other text-based materials. Through this training, they acquire the ability to recognize patterns, comprehend context, and generate coherent and contextually appropriate responses.\nNotable examples of LLMs, such as ChatGPT based on the GPT-3.5 architecture, have been trained on comprehensive and varied datasets to offer accurate and valuable information across different domains. These models possess natural language understanding capabilities and can undertake various tasks such as language translation, content generation, and text completion.\nTheir versatility allows them to assist users in diverse inquiries and tasks, making them valuable tools across numerous fields, including education, customer service, content creation, and research.",
        "reference": "intellipaat.com",
        "role": "data-science"
    },
    {
        "question": "1. What is Data Science?",
        "answer": "An interdisciplinary field that constitutes various scientific processes, algorithms, tools, and machine learning techniques working to help find common patterns and gather sensible insights from the given raw input data using statistical and mathematical analysis is called Data Science. The following figure represents the life cycle of data science.   It starts with gathering the business requirements and relevant data.\nOnce the data is acquired, it is maintained by performing data cleaning, data warehousing, data staging, and data architecture.\nData processing does the task of exploring the data, mining it, and analyzing it which can be finally used to generate the summary of the insights extracted from the data.\nOnce the exploratory steps are completed, the cleansed data is subjected to various algorithms like predictive analysis, regression, text mining, recognition patterns, etc depending on the requirements.\nIn the final stage, the results are communicated to the business in a visually appealing manner. This is where the skill of data visualization, reporting, and different business intelligence tools come into the picture. Learn More. It starts with gathering the business requirements and relevant data. Once the data is acquired, it is maintained by performing data cleaning, data warehousing, data staging, and data architecture. Data processing does the task of exploring the data, mining it, and analyzing it which can be finally used to generate the summary of the insights extracted from the data. Once the exploratory steps are completed, the cleansed data is subjected to various algorithms like predictive analysis, regression, text mining, recognition patterns, etc depending on the requirements. In the final stage, the results are communicated to the business in a visually appealing manner. This is where the skill of data visualization, reporting, and different business intelligence tools come into the picture. Learn More. Learn More Learn More",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "2. Define the terms KPI, lift, model fitting, robustness and DOE.",
        "answer": "KPI: KPI stands for Key Performance Indicator that measures how well the business achieves its objectives.\nLift: This is a performance measure of the target model measured against a random choice model. Lift indicates how good the model is at prediction versus if there was no model.\nModel fitting: This indicates how well the model under consideration fits given observations.\nRobustness: This represents the system’s capability to handle differences and variances effectively.\nDOE: stands for the design of experiments, which represents the task design aiming to describe and explain information variation under hypothesized conditions to reflect variables. KPI: KPI stands for Key Performance Indicator that measures how well the business achieves its objectives. KPI: Lift: This is a performance measure of the target model measured against a random choice model. Lift indicates how good the model is at prediction versus if there was no model. Lift: Model fitting: This indicates how well the model under consideration fits given observations. Model fitting: Robustness: This represents the system’s capability to handle differences and variances effectively. Robustness: DOE: stands for the design of experiments, which represents the task design aiming to describe and explain information variation under hypothesized conditions to reflect variables. DOE:",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "3. What is the difference between data analytics and data science?",
        "answer": "Data science involves the task of transforming data by using various technical analysis methods to extract meaningful insights using which a data analyst can apply to their business scenarios.\nData analytics deals with checking the existing hypothesis and information and answers questions for a better and effective business-related decision-making process.\nData Science drives innovation by answering questions that build connections and answers for futuristic problems. Data analytics focuses on getting present meaning from existing historical context whereas data science focuses on predictive modeling.\nData Science can be considered as a broad subject that makes use of various mathematical and scientific tools and algorithms for solving complex problems whereas data analytics can be considered as a specific field dealing with specific concentrated problems using fewer tools of statistics and visualization. Data science involves the task of transforming data by using various technical analysis methods to extract meaningful insights using which a data analyst can apply to their business scenarios. Data analytics deals with checking the existing hypothesis and information and answers questions for a better and effective business-related decision-making process. Data Science drives innovation by answering questions that build connections and answers for futuristic problems. Data analytics focuses on getting present meaning from existing historical context whereas data science focuses on predictive modeling. Data Science can be considered as a broad subject that makes use of various mathematical and scientific tools and algorithms for solving complex problems whereas data analytics can be considered as a specific field dealing with specific concentrated problems using fewer tools of statistics and visualization. The following Venn diagram depicts the difference between data science and data analytics clearly: data science and data analytics data science and data analytics  ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "4. What are some of the techniques used for sampling? What is the main advantage of sampling?",
        "answer": "Data analysis can not be done on a whole volume of data at a time especially when it involves larger datasets. It becomes crucial to take some data samples that can be used for representing the whole population and then perform analysis on it. While doing this, it is very much necessary to carefully take sample data out of the huge data that truly represents the entire dataset.   There are majorly two categories of sampling techniques based on the usage of statistics, they are: Probability Sampling techniques: Clustered sampling, Simple random sampling, Stratified sampling.\nNon-Probability Sampling techniques: Quota sampling, Convenience sampling, snowball sampling, etc. Probability Sampling techniques: Clustered sampling, Simple random sampling, Stratified sampling. Probability Sampling techniques: Non-Probability Sampling techniques: Quota sampling, Convenience sampling, snowball sampling, etc. Non-Probability Sampling techniques:",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "5. List down the conditions for Overfitting and Underfitting.",
        "answer": "Overfitting: The model performs well only for the sample training data. If any new data is given as input to the model, it fails to provide any result. These conditions occur due to low bias and high variance in the model. Decision trees are more prone to overfitting. Overfitting:   Underfitting: Here, the model is so simple that it is not able to identify the correct relationship in the data, and hence it does not perform well even on the test data. This can happen due to high bias and low variance. Linear regression is more prone to Underfitting. Underfitting:  ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "6. Differentiate between the long and wide format data.",
        "answer": "Long format Data Wide-Format Data\nHere, each row of the data represents the one-time information of a subject. Each subject would have its data in different/ multiple rows. Here, the repeated responses of a subject are part of separate columns.\nThe data can be recognized by considering rows as groups. The data can be recognized by considering columns as groups.\nThis data format is most commonly used in R analyses and to write into log files after each trial. This data format is rarely used in R analyses and most commonly used in stats packages for repeated measures ANOVAs. Long format Data Wide-Format Data\nHere, each row of the data represents the one-time information of a subject. Each subject would have its data in different/ multiple rows. Here, the repeated responses of a subject are part of separate columns.\nThe data can be recognized by considering rows as groups. The data can be recognized by considering columns as groups.\nThis data format is most commonly used in R analyses and to write into log files after each trial. This data format is rarely used in R analyses and most commonly used in stats packages for repeated measures ANOVAs. Long format Data Wide-Format Data Long format Data Wide-Format Data Long format Data Wide-Format Data Here, each row of the data represents the one-time information of a subject. Each subject would have its data in different/ multiple rows. Here, the repeated responses of a subject are part of separate columns.\nThe data can be recognized by considering rows as groups. The data can be recognized by considering columns as groups.\nThis data format is most commonly used in R analyses and to write into log files after each trial. This data format is rarely used in R analyses and most commonly used in stats packages for repeated measures ANOVAs. Here, each row of the data represents the one-time information of a subject. Each subject would have its data in different/ multiple rows. Here, the repeated responses of a subject are part of separate columns. Here, each row of the data represents the one-time information of a subject. Each subject would have its data in different/ multiple rows. Here, the repeated responses of a subject are part of separate columns. The data can be recognized by considering rows as groups. The data can be recognized by considering columns as groups. The data can be recognized by considering rows as groups. The data can be recognized by considering columns as groups. This data format is most commonly used in R analyses and to write into log files after each trial. This data format is rarely used in R analyses and most commonly used in stats packages for repeated measures ANOVAs. This data format is most commonly used in R analyses and to write into log files after each trial. This data format is rarely used in R analyses and most commonly used in stats packages for repeated measures ANOVAs. The following image depicts the representation of wide format and long format data:  ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "7. What are Eigenvectors and Eigenvalues?",
        "answer": "Eigenvectors are column vectors or unit vectors whose length/magnitude is equal to 1. They are also called right vectors. Eigenvalues are coefficients that are applied on eigenvectors which give these vectors different values for length or magnitude.   A matrix can be decomposed into Eigenvectors and Eigenvalues and this process is called Eigen decomposition. These are then eventually used in machine learning methods like PCA (Principal Component Analysis) for gathering valuable insights from the given matrix.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "8. What does it mean when the p-values are high and low?",
        "answer": "A p-value is the measure of the probability of having results equal to or more than the results achieved under a specific hypothesis assuming that the null hypothesis is correct. This represents the probability that the observed difference occurred randomly by chance. Low p-value which means values ≤ 0.05 means that the null hypothesis can be rejected and the data is unlikely with true null.\nHigh p-value, i.e values ≥ 0.05 indicates the strength in favor of the null hypothesis. It means that the data is like with true null.\np-value = 0.05 means that the hypothesis can go either way. Low p-value which means values ≤ 0.05 means that the null hypothesis can be rejected and the data is unlikely with true null. High p-value, i.e values ≥ 0.05 indicates the strength in favor of the null hypothesis. It means that the data is like with true null. p-value = 0.05 means that the hypothesis can go either way.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "9. When is resampling done?",
        "answer": "Resampling is a methodology used to sample data for improving accuracy and quantify the uncertainty of population parameters. It is done to ensure the model is good enough by training the model on different patterns of a dataset to ensure variations are handled. It is also done in the cases where models need to be validated using random subsets or when substituting labels on data points while performing tests.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "10. What do you understand by Imbalanced Data?",
        "answer": "Data is said to be highly imbalanced if it is distributed unequally across different categories. These datasets result in an error in model performance and result in inaccuracy.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "11. Are there any differences between the expected value and mean value?",
        "answer": "There are not many differences between these two, but it is to be noted that these are used in different contexts. The mean value generally refers to the probability distribution whereas the expected value is referred to in the contexts involving random variables.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "12. What do you understand by Survivorship Bias?",
        "answer": "This bias refers to the logical error while focusing on aspects that survived some process and overlooking those that did not work due to lack of prominence. This bias can lead to deriving wrong conclusions.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "13. What is a Gradient and Gradient Descent?",
        "answer": "Gradient: Gradient is the measure of a property that how much the output has changed with respect to a little change in the input. In other words, we can say that it is a measure of change in the weights with respect to the change in error. The gradient can be mathematically represented as the slope of a function. Gradient:   Gradient Descent: Gradient descent is a minimization algorithm that minimizes the Activation function. Well, it can minimize any function given to it but it is usually provided with the activation function only. Gradient Descent: Gradient descent, as the name suggests means descent or a decrease in something. The analogy of gradient descent is often taken as a person climbing down a hill/mountain. The following is the equation describing what gradient descent means: So, if a person is climbing down the hill, the next position that the climber has to come to is denoted by “b” in this equation. Then, there is a minus sign because it denotes the minimization (as gradient descent is a minimization algorithm). The Gamma is called a waiting factor and the remaining term which is the Gradient term itself shows the direction of the steepest descent. This situation can be represented in a graph as follows:   Here, we are somewhere at the “Initial Weights” and we want to reach the Global minimum. So, this minimization algorithm will help us do that.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "14. Define confounding variables.",
        "answer": "Confounding variables are also known as confounders. These variables are a type of extraneous variables that influence both independent and dependent variables causing spurious association and mathematical relationships between those variables that are associated but are not casually related to each other.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "15. Define and explain selection bias?",
        "answer": "The selection bias occurs in the case when the researcher has to make a decision on which participant to study. The selection bias is associated with those researches when the participant selection is not random. The selection bias is also called the selection effect. The selection bias is caused by as a result of the method of sample collection. Four types of selection bias are explained below: Sampling Bias: As a result of a population that is not random at all, some members of a population have fewer chances of getting included than others, resulting in a biased sample. This causes a systematic error known as sampling bias.\nTime interval: Trials may be stopped early if we reach any extreme value but if all variables are similar invariance, the variables with the highest variance have a higher chance of achieving the extreme value.\nData: It is when specific data is selected arbitrarily and the generally agreed criteria are not followed.\nAttrition: Attrition in this context means the loss of the participants. It is the discounting of those subjects that did not complete the trial. Sampling Bias: As a result of a population that is not random at all, some members of a population have fewer chances of getting included than others, resulting in a biased sample. This causes a systematic error known as sampling bias. Sampling Bias: Time interval: Trials may be stopped early if we reach any extreme value but if all variables are similar invariance, the variables with the highest variance have a higher chance of achieving the extreme value. Time interval: Data: It is when specific data is selected arbitrarily and the generally agreed criteria are not followed. Data: Attrition: Attrition in this context means the loss of the participants. It is the discounting of those subjects that did not complete the trial. Attrition:",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "16. Define bias-variance trade-off?",
        "answer": "Let us first understand the meaning of bias and variance in detail: Bias: It is a kind of error in a machine learning model when an ML Algorithm is oversimplified. When a model is trained, at that time it makes simplified assumptions so that it can easily understand the target function. Some algorithms that have low bias are Decision Trees, SVM, etc. On the other hand, logistic and linear regression algorithms are the ones with a high bias. Bias: Variance: Variance is also a kind of error. It is introduced into an ML Model when an ML algorithm is made highly complex. This model also learns noise from the data set that is meant for training. It further performs badly on the test data set. This may lead to over lifting as well as high sensitivity. Variance: When the complexity of a model is increased, a reduction in the error is seen. This is caused by the lower bias in the model. But, this does not happen always till we reach a particular point called the optimal point. After this point, if we keep on increasing the complexity of the model, it will be over lifted and will suffer from the problem of high variance. We can represent this situation with the help of a graph as shown below:   As you can see from the image above, before the optimal point, increasing the complexity of the model reduces the error (bias). However, after the optimal point, we see that the increase in the complexity of the machine learning model increases the variance. Trade-off Of Bias And Variance: So, as we know that bias and variance, both are errors in machine learning models, it is very essential that any machine learning model has low variance as well as a low bias so that it can achieve good performance. Trade-off Of Bias And Variance: Let us see some examples. The K-Nearest Neighbor Algorithm is a good example of an algorithm with low bias and high variance. This trade-off can easily be reversed by increasing the k value which in turn results in increasing the number of neighbours. This, in turn, results in increasing the bias and reducing the variance. K-Nearest Neighbor Algorithm Another example can be the algorithm of a support vector machine. This algorithm also has a high variance and obviously, a low bias and we can reverse the trade-off by increasing the value of parameter C. Thus, increasing the C parameter increases the bias and decreases the variance. So, the trade-off is simple. If we increase the bias, the variance will decrease and vice versa.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "17. Define the confusion matrix?",
        "answer": "It is a matrix that has 2 rows and 2 columns. It has 4 outputs that a binary classifier provides to it. It is used to derive various measures like specificity, error rate, accuracy, precision, sensitivity, and recall.   The test data set should contain the correct and predicted labels. The labels depend upon the performance. For instance, the predicted labels are the same if the binary classifier performs perfectly. Also, they match the part of observed labels in real-world scenarios. The four outcomes shown above in the confusion matrix mean the following: True Positive: This means that the positive prediction is correct.\nFalse Positive: This means that the positive prediction is incorrect.\nTrue Negative: This means that the negative prediction is correct.\nFalse Negative: This means that the negative prediction is incorrect. True Positive: This means that the positive prediction is correct. True Positive: False Positive: This means that the positive prediction is incorrect. False Positive: True Negative: This means that the negative prediction is correct. True Negative: False Negative: This means that the negative prediction is incorrect. False Negative: The formulas for calculating basic measures that comes from the confusion matrix are: Error rate: (FP + FN)/(P + N)\nAccuracy: (TP + TN)/(P + N)\nSensitivity = TP/P\nSpecificity = TN/N\nPrecision = TP/(TP + FP)\nF-Score  = (1 + b)(PREC.REC)/(b2 PREC + REC) Here, b is mostly 0.5 or 1 or 2. Error rate: (FP + FN)/(P + N) Error rate Accuracy: (TP + TN)/(P + N) Accuracy Sensitivity = TP/P Sensitivity Specificity = TN/N Specificity Precision = TP/(TP + FP) Precision F-Score  = (1 + b)(PREC.REC)/(b2 PREC + REC) Here, b is mostly 0.5 or 1 or 2. F-Score In these formulas: FP = false positive\nFN = false negative\nTP = true positive\nRN = true negative FP  FN  TP  RN Also, Sensitivity is the measure of the True Positive Rate. It is also called recall.\nSpecificity is the measure of the true negative rate.\nPrecision is the measure of a positive predicted value.\nF-score is the harmonic mean of precision and recall.   ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "18. What is logistic regression? State an example where you have recently used logistic regression.",
        "answer": "Logistic Regression is also known as the logit model. It is a technique to predict the binary outcome from a linear combination of variables (called the predictor variables). For example, let us say that we want to predict the outcome of elections for a particular political leader. So, we want to find out whether this leader is going to win the election or not. So, the result is binary i.e. win (1) or loss (0). However, the input is a combination of linear variables like the money spent on advertising, the past work done by the leader and the party, etc. For example",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "19. What is Linear Regression? What are some of the major drawbacks of the linear model?",
        "answer": "Linear regression is a technique in which the score of a variable Y is predicted using the score of a predictor variable X. Y is called the criterion variable. Some of the drawbacks of Linear Regression are as follows: The assumption of linearity of errors is a major drawback.\nIt cannot be used for binary outcomes. We have Logistic Regression for that.\nOverfitting problems are there that can’t be solved. The assumption of linearity of errors is a major drawback. It cannot be used for binary outcomes. We have Logistic Regression for that. Overfitting problems are there that can’t be solved.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "20. What is a random forest? Explain it’s working.",
        "answer": "Classification is very important in machine learning. It is very important to know to which class does an observation belongs. Hence, we have various classification algorithms in machine learning like logistic regression, support vector machine, decision trees, Naive Bayes classifier, etc. One such classification technique that is near the top of the classification hierarchy is the random forest classifier. random forest So, firstly we need to understand a decision tree before we can understand the random forest classifier and its works. So, let us say that we have a string as given below:   So, we have the string with 5 ones and 4 zeroes and we want to classify the characters of this string using their features. These features are colour (red or green in this case) and whether the observation (i.e. character) is underlined or not. Now, let us say that we are only interested in red and underlined observations. So, the decision tree would look something like this:   So, we started with the colour first as we are only interested in the red observations and we separated the red and the green-coloured characters. After that, the “No” branch i.e. the branch that had all the green coloured characters was not expanded further as we want only red-underlined characters. So, we expanded the “Yes” branch and we again got a “Yes” and a “No” branch based on the fact whether the characters were underlined or not. So, this is how we draw a typical decision tree. However, the data in real life is not this clean but this was just to give an idea about the working of the decision trees. Let us now move to the random forest. Random Forest Random Forest It consists of a large number of decision trees that operate as an ensemble. Basically, each tree in the forest gives a class prediction and the one with the maximum number of votes becomes the prediction of our model. For instance, in the example shown below, 4 decision trees predict 1, and 2 predict 0. Hence, prediction 1 will be considered.   The underlying principle of a random forest is that several weak learners combine to form a keen learner. The steps to build a random forest are as follows: Build several decision trees on the samples of data and record their predictions.\nEach time a split is considered for a tree, choose a random sample of mm predictors as the split candidates out of all the pp predictors. This happens to every tree in the random forest.\nApply the rule of thumb i.e. at each split m = p√m = p.\nApply the predictions to the majority rule. Build several decision trees on the samples of data and record their predictions. Each time a split is considered for a tree, choose a random sample of mm predictors as the split candidates out of all the pp predictors. This happens to every tree in the random forest. Apply the rule of thumb i.e. at each split m = p√m = p. Apply the predictions to the majority rule.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "21. In a time interval of 15-minutes, the probability that you may see a shooting star or a bunch of them is 0.2. What is the percentage chance of you seeing at least one star shooting from the sky if you are under it for about an hour?",
        "answer": "Let us say that Prob is the probability that we may see a minimum of one shooting star in 15 minutes. So, Prob = 0.2 Now, the probability that we may not see any shooting star in the time duration of 15 minutes is = 1 - Prob 1-0.2 = 0.8 The probability that we may not see any shooting star for an hour is: = (1-Prob)(1-Prob)(1-Prob)*(1-Prob)\n= 0.8 * 0.8 * 0.8 * 0.8 = (0.8)⁴  \n≈ 0.40   So, the probability that we will see one shooting star in the time interval of an hour is = 1-0.4 = 0.6 So, there are approximately 60% chances that we may see a shooting star in the time span of an hour.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "22. What is deep learning? What is the difference between deep learning and machine learning?",
        "answer": "Deep learning is a paradigm of machine learning. In deep learning,  multiple layers of processing are involved in order to extract high features from the data. The neural networks are designed in such a way that they try to simulate the human brain. Deep learning has shown incredible performance in recent years because of the fact that it shows great analogy with the human brain. The difference between machine learning and deep learning is that deep learning is a paradigm or a part of machine learning that is inspired by the structure and functions of the human brain called the artificial neural networks. Learn More. Learn More Learn More",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "1. How are the time series problems different from other regression problems?",
        "answer": "Time series data can be thought of as an extension to linear regression which uses terms like autocorrelation, movement of averages for summarizing historical data of y-axis variables for predicting a better future.\nForecasting and prediction is the main goal of time series problems where accurate predictions can be made but sometimes the underlying reasons might not be known.\nHaving Time in the problem does not necessarily mean it becomes a time series problem. There should be a relationship between target and time for a problem to become a time series problem.\nThe observations close to one another in time are expected to be similar to the ones far away which provide accountability for seasonality. For instance, today’s weather would be similar to tomorrow’s weather but not similar to weather from 4 months from today. Hence, weather prediction based on past data becomes a time series problem. Time series data can be thought of as an extension to linear regression which uses terms like autocorrelation, movement of averages for summarizing historical data of y-axis variables for predicting a better future. Forecasting and prediction is the main goal of time series problems where accurate predictions can be made but sometimes the underlying reasons might not be known. Having Time in the problem does not necessarily mean it becomes a time series problem. There should be a relationship between target and time for a problem to become a time series problem. The observations close to one another in time are expected to be similar to the ones far away which provide accountability for seasonality. For instance, today’s weather would be similar to tomorrow’s weather but not similar to weather from 4 months from today. Hence, weather prediction based on past data becomes a time series problem.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "2. What are RMSE and MSE in a linear regression model?",
        "answer": "RMSE: RMSE stands for Root Mean Square Error. In a linear regression model, RMSE is used to test the performance of the machine learning model. It is used to evaluate the data spread around the line of best fit. So, in simple words, it is used to measure the deviation of the residuals. RMSE: line of best RMSE is calculated using the formula:   Yi is the actual value of the output variable.\nY(Cap) is the predicted value and,\nN is the number of data points. Yi is the actual value of the output variable. Yi Y(Cap) is the predicted value and, Y(Cap) N is the number of data points. N MSE: Mean Squared Error is used to find how close is the line to the actual data. So, we make the difference in the distance of the data points from the line and the difference is squared. This is done for all the data points and the submission of the squared difference divided by the total number of data points gives us the Mean Squared Error (MSE). MSE: So, if we are taking the squared difference of N data points and dividing the sum by N, what does it mean? Yes, it represents the average of the squared difference of a data point from the line i.e. the average of the squared difference between the actual and the predicted values. The formula for finding MSE is given below:   Yi is the actual value of the output variable (the ith data point)\nY(cap) is the predicted value and,\nN is the total number of data points. Yi is the actual value of the output variable (the ith data point) Yi Y(cap) is the predicted value and, Y(cap) N is the total number of data points. N So, RMSE is the square root of MSE. RMSE MSE",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "3. What are Support Vectors in SVM (Support Vector Machine)?",
        "answer": "  In the above diagram, we can see that the thin lines mark the distance from the classifier to the closest data points (darkened data points). These are called support vectors. So, we can define the support vectors as the data points or vectors that are nearest (closest) to the hyperplane. They affect the position of the hyperplane. Since they support the hyperplane, they are known as support vectors.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "4. So, you have done some projects in machine learning and data science and we see you are a bit experienced in the field. Let’s say your laptop’s RAM is only 4GB and you want to train your model on 10GB data set.",
        "answer": "What will you do? Have you experienced such an issue before? In such types of questions, we first need to ask what ML model we have to train. After that, it depends on whether we have to train a model based on Neural Networks or SVM. The steps for Neural Networks are given below: The steps for Neural Networks are given below: The Numpy array can be used to load the entire data. It will never store the entire data, rather just create a mapping of the data.\nNow, in order to get some desired data, pass the index into the NumPy Array.\nThis data can be used to pass as an input to the neural network maintaining a small batch size. The Numpy array can be used to load the entire data. It will never store the entire data, rather just create a mapping of the data. Now, in order to get some desired data, pass the index into the NumPy Array. This data can be used to pass as an input to the neural network maintaining a small batch size. The steps for SVM are given below: The steps for SVM are given below: For SVM, small data sets can be obtained. This can be done by dividing the big data set.\nThe subset of the data set can be obtained as an input if using the partial fit function.\nRepeat the step of using the partial fit method for other subsets as well. For SVM, small data sets can be obtained. This can be done by dividing the big data set. The subset of the data set can be obtained as an input if using the partial fit function. Repeat the step of using the partial fit method for other subsets as well. Now, you may describe the situation if you have faced such an issue in your projects or working in machine learning/ data science.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "5. Explain Neural Network Fundamentals.",
        "answer": "In the human brain, different neurons are present. These neurons combine and perform various tasks. The Neural Network in deep learning tries to imitate human brain neurons. The neural network learns the patterns from the data and uses the knowledge that it gains from various patterns to predict the output for new data, without any human assistance. A perceptron is the simplest neural network that contains a single neuron that performs 2 functions. The first function is to perform the weighted sum of all the inputs and the second is an activation function.   There are some other neural networks that are more complicated. Such networks consist of the following three layers: Input Layer: The neural network has the input layer to receive the input.\nHidden Layer: There can be multiple hidden layers between the input layer and the output layer. The initially hidden layers are used for detecting the low-level patterns whereas the further layers are responsible for combining output from previous layers to find more patterns.\nOutput Layer: This layer outputs the prediction. Input Layer: The neural network has the input layer to receive the input. Input Layer: Hidden Layer: There can be multiple hidden layers between the input layer and the output layer. The initially hidden layers are used for detecting the low-level patterns whereas the further layers are responsible for combining output from previous layers to find more patterns. Hidden Layer: Output Layer: This layer outputs the prediction. Output Layer: An example neural network image is shown below:  ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "6. What is Generative Adversarial Network?",
        "answer": "This approach can be understood with the famous example of the wine seller. Let us say that there is a wine seller who has his own shop. This wine seller purchases wine from the dealers who sell him the wine at a low cost so that he can sell the wine at a high cost to the customers. Now, let us say that the dealers whom he is purchasing the wine from, are selling him fake wine. They do this as the fake wine costs way less than the original wine and the fake and the real wine are indistinguishable to a normal consumer (customer in this case). The shop owner has some friends who are wine experts and he sends his wine to them every time before keeping the stock for sale in his shop. So, his friends, the wine experts, give him feedback that the wine is probably fake. Since the wine seller has been purchasing the wine for a long time from the same dealers, he wants to make sure that their feedback is right before he complains to the dealers about it. Now, let us say that the dealers also have got a tip from somewhere that the wine seller is suspicious of them. So, in this situation, the dealers will try their best to sell the fake wine whereas the wine seller will try his best to identify the fake wine. Let us see this with the help of a diagram shown below:   From the image above, it is clear that a noise vector is entering the generator (dealer) and he generates the fake wine and the discriminator has to distinguish between the fake wine and real wine. This is a Generative Adversarial Network (GAN). Generative Adversarial Network In a GAN, there are 2 main components viz. Generator and Discrminator. So, the generator is a CNN that keeps producing images and the discriminator tries to identify the real images from the fake ones.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "7. What is a computational graph?",
        "answer": "A computational graph is also known as a “Dataflow Graph”. Everything in the famous deep learning library TensorFlow is based on the computational graph. The computational graph in Tensorflow has a network of nodes where each node operates. The nodes of this graph represent operations and the edges represent tensors.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "8. What are auto-encoders?",
        "answer": "Auto-encoders are learning networks. They transform inputs into outputs with minimum possible errors. So, basically, this means that the output that we want should be almost equal to or as close as to input as follows. Multiple layers are added between the input and the output layer and the layers that are in between the input and the output layer are smaller than the input layer. It received unlabelled input. This input is encoded to reconstruct the input later.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "9. What are Exploding Gradients and Vanishing Gradients?",
        "answer": "Exploding Gradients: Let us say that you are training an RNN. Say, you saw exponentially growing error gradients that accumulate, and as a result of this, very large updates are made to the neural network model weights. These exponentially growing error gradients that update the neural network weights to a great extent are called Exploding Gradients.\nVanishing Gradients: Let us say again, that you are training an RNN. Say, the slope became too small. This problem of the slope becoming too small is called Vanishing Gradient. It causes a major increase in the training time and causes poor performance and extremely low accuracy. Exploding Gradients: Let us say that you are training an RNN. Say, you saw exponentially growing error gradients that accumulate, and as a result of this, very large updates are made to the neural network model weights. These exponentially growing error gradients that update the neural network weights to a great extent are called Exploding Gradients. Exploding Gradients: Exploding Gradients Vanishing Gradients: Let us say again, that you are training an RNN. Say, the slope became too small. This problem of the slope becoming too small is called Vanishing Gradient. It causes a major increase in the training time and causes poor performance and extremely low accuracy. Vanishing Gradients: Vanishing Gradient",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "10. What is the p-value and what does it indicate in the Null Hypothesis?",
        "answer": "P-value is a number that ranges from 0 to 1. In a hypothesis test in statistics, the p-value helps in telling us how strong the results are. The claim that is kept for experiment or trial is called Null Hypothesis. A low p-value i.e. p-value less than or equal to 0.05 indicates the strength of the results against the Null Hypothesis which in turn means that the Null Hypothesis can be rejected. \nA high p-value i.e. p-value greater than 0.05 indicates the strength of the results in favour of the Null Hypothesis i.e. for the Null Hypothesis which in turn means that the Null Hypothesis can be accepted. A low p-value i.e. p-value less than or equal to 0.05 indicates the strength of the results against the Null Hypothesis which in turn means that the Null Hypothesis can be rejected. less than or equal to 0.05 A high p-value i.e. p-value greater than 0.05 indicates the strength of the results in favour of the Null Hypothesis i.e. for the Null Hypothesis which in turn means that the Null Hypothesis can be accepted. greater than 0.05",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "11. Since you have experience in the deep learning field, can you tell us why TensorFlow is the most preferred library in deep learning?",
        "answer": "Tensorflow is a very famous library in deep learning. The reason is pretty simple actually. It provides C++ as well as Python APIs which makes it very easier to work on. Also, TensorFlow has a fast compilation speed as compared to Keras and Torch (other famous deep learning libraries). Apart from that, Tenserflow supports both GPU and CPU computing devices. Hence, it is a major success and a very popular library for deep learning.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "12. Suppose there is a dataset having variables with missing values of more than 30%, how will you deal with such a dataset?",
        "answer": "Depending on the size of the dataset, we follow the below ways: In case the datasets are small, the missing values are substituted with the mean or average of the remaining data. In pandas, this can be done by using mean = df.mean() where df represents the pandas dataframe representing the dataset and mean() calculates the mean of the data. To substitute the missing values with the calculated mean, we can use df.fillna(mean).\nFor larger datasets, the rows with missing values can be removed and the remaining data can be used for data prediction. In case the datasets are small, the missing values are substituted with the mean or average of the remaining data. In pandas, this can be done by using mean = df.mean() where df represents the pandas dataframe representing the dataset and mean() calculates the mean of the data. To substitute the missing values with the calculated mean, we can use df.fillna(mean). mean = df.mean() df.fillna(mean) For larger datasets, the rows with missing values can be removed and the remaining data can be used for data prediction.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "13. What is Cross-Validation?",
        "answer": "Cross-Validation is a Statistical technique used for improving a model’s performance. Here, the model will be trained and tested with rotation using different samples of the training dataset to ensure that the model performs well for unknown data. The training data will be split into various groups and the model is run and validated against these groups in rotation.   The most commonly used techniques are: K- Fold method\nLeave p-out method\nLeave-one-out method\nHoldout method K- Fold method Leave p-out method Leave-one-out method Holdout method",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "14. What are the differences between correlation and covariance?",
        "answer": "Although these two terms are used for establishing a relationship and dependency between any two random variables, the following are the differences between them: Correlation: This technique is used to measure and estimate the quantitative relationship between two variables and is measured in terms of how strong are the variables related.\nCovariance: It represents the extent to which the variables change together in a cycle. This explains the systematic relationship between pair of variables where changes in one affect changes in another variable. Correlation: This technique is used to measure and estimate the quantitative relationship between two variables and is measured in terms of how strong are the variables related. Correlation: Covariance: It represents the extent to which the variables change together in a cycle. This explains the systematic relationship between pair of variables where changes in one affect changes in another variable. Covariance: Mathematically, consider 2 random variables, X and Y where the means are represented as \nμ\nX\n and \nμ\nY\n respectively and standard deviations are represented by \nσ\nX\n and \nσ\nY\n respectively and E represents the expected value operator, then: μ\nX             μ\nX μ\nX μ\nX μ\nX μ X  μ\nY             μ\nY μ\nY μ\nY μ\nY μ Y  σ\nX             σ\nX σ\nX σ\nX σ\nX σ X  σ\nY             σ\nY σ\nY σ\nY σ\nY σ Y  covarianceXY = E[(X-\nμ\nX\n),(Y-\nμ\nY\n)]\ncorrelationXY = E[(X-\nμ\nX\n),(Y-\nμ\nY\n)]/(\nσ\nX\nσ\nY\n)\nso that covarianceXY = E[(X-\nμ\nX\n),(Y-\nμ\nY\n)] μ\nX             μ\nX μ\nX μ\nX μ\nX μ X  μ\nY             μ\nY μ\nY μ\nY μ\nY μ Y  correlationXY = E[(X-\nμ\nX\n),(Y-\nμ\nY\n)]/(\nσ\nX\nσ\nY\n)\nso that μ\nX             μ\nX μ\nX μ\nX μ\nX μ X  μ\nY             μ\nY μ\nY μ\nY μ\nY μ Y  σ\nX             σ\nX σ\nX σ\nX σ\nX σ X  σ\nY             σ\nY σ\nY σ\nY σ\nY σ Y   correlation(X,Y) = covariance(X,Y)/(covariance(X) covariance(Y)) correlation(X,Y) = covariance(X,Y)/(covariance(X) covariance(Y)) Based on the above formula, we can deduce that the correlation is dimensionless whereas covariance is represented in units that are obtained from the multiplication of units of two variables. The following image graphically shows the difference between correlation and covariance:  ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "15. How do you approach solving any data analytics based project?",
        "answer": "Generally, we follow the below steps: The first step is to thoroughly understand the business requirement/problem\nNext, explore the given data and analyze it carefully. If you find any data missing, get the requirements clarified from the business.\nData cleanup and preparation step is to be performed next which is then used for modelling. Here, the missing values are found and the variables are transformed.\nRun your model against the data, build meaningful visualization and analyze the results to get meaningful insights.\nRelease the model implementation, and track the results and performance over a specified period to analyze the usefulness.\nPerform cross-validation of the model. The first step is to thoroughly understand the business requirement/problem Next, explore the given data and analyze it carefully. If you find any data missing, get the requirements clarified from the business. Data cleanup and preparation step is to be performed next which is then used for modelling. Here, the missing values are found and the variables are transformed. Run your model against the data, build meaningful visualization and analyze the results to get meaningful insights. Release the model implementation, and track the results and performance over a specified period to analyze the usefulness. Perform cross-validation of the model. Check out the list of data analytics projects. Check out the list of data analytics projects Check out the list of data analytics projects  ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "16. How regularly must we update an algorithm in the field of machine learning?",
        "answer": "We do not want to update and make changes to an algorithm on a regular basis as an algorithm is a well-defined step procedure to solve any problem and if the steps keep on updating, it cannot be said well defined anymore. Also, this brings in a lot of problems to the systems already implementing the algorithm as it becomes difficult to bring in continuous and regular changes. So, we should update an algorithm only in any of the following cases: If you want the model to evolve as data streams through infrastructure, it is fair to make changes to an algorithm and update it accordingly.\nIf the underlying data source is changing, it almost becomes necessary to update the algorithm accordingly.\nIf there is a case of non-stationarity, we may update the algorithm.\nOne of the most important reasons for updating any algorithm is its underperformance and lack of efficiency. So, if an algorithm lacks efficiency or underperforms it should be either replaced by some better algorithm or it must be updated. If you want the model to evolve as data streams through infrastructure, it is fair to make changes to an algorithm and update it accordingly. If the underlying data source is changing, it almost becomes necessary to update the algorithm accordingly. If there is a case of non-stationarity, we may update the algorithm. One of the most important reasons for updating any algorithm is its underperformance and lack of efficiency. So, if an algorithm lacks efficiency or underperforms it should be either replaced by some better algorithm or it must be updated.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "17. Why do we need selection bias?",
        "answer": "Selection Bias happens in cases where there is no randomization specifically achieved while picking a part of the dataset for analysis. This bias tells that the sample analyzed does not represent the whole population meant to be analyzed. For example, in the below image, we can see that the sample that we selected does not entirely represent the whole population that we have. This helps us to question whether we have selected the right data for analysis or not. For example, in the below image, we can see that the sample that we selected does not entirely represent the whole population that we have. This helps us to question whether we have selected the right data for analysis or not.  ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "18. Why is data cleaning crucial? How do you clean the data?",
        "answer": "While running an algorithm on any data, to gather proper insights, it is very much necessary to have correct and clean data that contains only relevant information. Dirty data most often results in poor or incorrect insights and predictions which can have damaging effects. For example, while launching any big campaign to market a product, if our data analysis tells us to target a product that in reality has no demand and if the campaign is launched, it is bound to fail. This results in a loss of the company’s revenue. This is where the importance of having proper and clean data comes into the picture. Data Cleaning of the data coming from different sources helps in data transformation and results in the data where the data scientists can work on.\nProperly cleaned data increases the accuracy of the model and provides very good predictions.\nIf the dataset is very large, then it becomes cumbersome to run data on it. The data cleanup step takes a lot of time (around 80% of the time) if the data is huge. It cannot be incorporated with running the model. Hence, cleaning data before running the model, results in increased speed and efficiency of the model.\nData cleaning helps to identify and fix any structural issues in the data. It also helps in removing any duplicates and helps to maintain the consistency of the data. Data Cleaning of the data coming from different sources helps in data transformation and results in the data where the data scientists can work on. Properly cleaned data increases the accuracy of the model and provides very good predictions. If the dataset is very large, then it becomes cumbersome to run data on it. The data cleanup step takes a lot of time (around 80% of the time) if the data is huge. It cannot be incorporated with running the model. Hence, cleaning data before running the model, results in increased speed and efficiency of the model. Data cleaning helps to identify and fix any structural issues in the data. It also helps in removing any duplicates and helps to maintain the consistency of the data. The following diagram represents the advantages of data cleaning:  ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "19. What are the available feature selection methods for selecting the right variables for building efficient predictive models?",
        "answer": "While using a dataset in data science or machine learning algorithms, it so happens that not all the variables are necessary and useful to build a model. Smarter feature selection methods are required to avoid redundant models to increase the efficiency of our model. Following are the three main methods in feature selection: Filter Methods:\nThese methods pick up only the intrinsic properties of features that are measured via univariate statistics and not cross-validated performance. They are straightforward and are generally faster and require less computational resources when compared to wrapper methods.\nThere are various filter methods such as the Chi-Square test, Fisher’s Score method, Correlation Coefficient, Variance Threshold, Mean Absolute Difference (MAD) method, Dispersion Ratios, etc. Filter Methods:\nThese methods pick up only the intrinsic properties of features that are measured via univariate statistics and not cross-validated performance. They are straightforward and are generally faster and require less computational resources when compared to wrapper methods.\nThere are various filter methods such as the Chi-Square test, Fisher’s Score method, Correlation Coefficient, Variance Threshold, Mean Absolute Difference (MAD) method, Dispersion Ratios, etc. Filter Methods: These methods pick up only the intrinsic properties of features that are measured via univariate statistics and not cross-validated performance. They are straightforward and are generally faster and require less computational resources when compared to wrapper methods.\nThere are various filter methods such as the Chi-Square test, Fisher’s Score method, Correlation Coefficient, Variance Threshold, Mean Absolute Difference (MAD) method, Dispersion Ratios, etc. These methods pick up only the intrinsic properties of features that are measured via univariate statistics and not cross-validated performance. They are straightforward and are generally faster and require less computational resources when compared to wrapper methods. There are various filter methods such as the Chi-Square test, Fisher’s Score method, Correlation Coefficient, Variance Threshold, Mean Absolute Difference (MAD) method, Dispersion Ratios, etc.   Wrapper Methods:\nThese methods need some sort of method to search greedily on all possible feature subsets, access their quality by learning and evaluating a classifier with the feature.\nThe selection technique is built upon the machine learning algorithm on which the given dataset needs to fit.\nThere are three types of wrapper methods, they are:\nForward Selection: Here, one feature is tested at a time and new features are added until a good fit is obtained.\nBackward Selection: Here, all the features are tested and the non-fitting ones are eliminated one by one to see while checking which works better.\nRecursive Feature Elimination: The features are recursively checked and evaluated how well they perform.\nThese methods are generally computationally intensive and require high-end resources for analysis. But these methods usually lead to better predictive models having higher accuracy than filter methods. Wrapper Methods:\nThese methods need some sort of method to search greedily on all possible feature subsets, access their quality by learning and evaluating a classifier with the feature.\nThe selection technique is built upon the machine learning algorithm on which the given dataset needs to fit.\nThere are three types of wrapper methods, they are:\nForward Selection: Here, one feature is tested at a time and new features are added until a good fit is obtained.\nBackward Selection: Here, all the features are tested and the non-fitting ones are eliminated one by one to see while checking which works better.\nRecursive Feature Elimination: The features are recursively checked and evaluated how well they perform.\nThese methods are generally computationally intensive and require high-end resources for analysis. But these methods usually lead to better predictive models having higher accuracy than filter methods. Wrapper Methods: These methods need some sort of method to search greedily on all possible feature subsets, access their quality by learning and evaluating a classifier with the feature.\nThe selection technique is built upon the machine learning algorithm on which the given dataset needs to fit.\nThere are three types of wrapper methods, they are:\nForward Selection: Here, one feature is tested at a time and new features are added until a good fit is obtained.\nBackward Selection: Here, all the features are tested and the non-fitting ones are eliminated one by one to see while checking which works better.\nRecursive Feature Elimination: The features are recursively checked and evaluated how well they perform.\nThese methods are generally computationally intensive and require high-end resources for analysis. But these methods usually lead to better predictive models having higher accuracy than filter methods. These methods need some sort of method to search greedily on all possible feature subsets, access their quality by learning and evaluating a classifier with the feature. The selection technique is built upon the machine learning algorithm on which the given dataset needs to fit. There are three types of wrapper methods, they are:\nForward Selection: Here, one feature is tested at a time and new features are added until a good fit is obtained.\nBackward Selection: Here, all the features are tested and the non-fitting ones are eliminated one by one to see while checking which works better.\nRecursive Feature Elimination: The features are recursively checked and evaluated how well they perform. Forward Selection: Here, one feature is tested at a time and new features are added until a good fit is obtained.\nBackward Selection: Here, all the features are tested and the non-fitting ones are eliminated one by one to see while checking which works better.\nRecursive Feature Elimination: The features are recursively checked and evaluated how well they perform. Forward Selection: Here, one feature is tested at a time and new features are added until a good fit is obtained. Forward Selection: Backward Selection: Here, all the features are tested and the non-fitting ones are eliminated one by one to see while checking which works better. Backward Selection: Recursive Feature Elimination: The features are recursively checked and evaluated how well they perform. Recursive Feature Elimination: These methods are generally computationally intensive and require high-end resources for analysis. But these methods usually lead to better predictive models having higher accuracy than filter methods.   Embedded Methods:\nEmbedded methods constitute the advantages of both filter and wrapper methods by including feature interactions while maintaining reasonable computational costs.\nThese methods are iterative as they take each model iteration and carefully extract features contributing to most of the training in that iteration.\nExamples of embedded methods: LASSO Regularization (L1), Random Forest Importance. Embedded Methods:\nEmbedded methods constitute the advantages of both filter and wrapper methods by including feature interactions while maintaining reasonable computational costs.\nThese methods are iterative as they take each model iteration and carefully extract features contributing to most of the training in that iteration.\nExamples of embedded methods: LASSO Regularization (L1), Random Forest Importance. Embedded Methods: Embedded methods constitute the advantages of both filter and wrapper methods by including feature interactions while maintaining reasonable computational costs.\nThese methods are iterative as they take each model iteration and carefully extract features contributing to most of the training in that iteration.\nExamples of embedded methods: LASSO Regularization (L1), Random Forest Importance. Embedded methods constitute the advantages of both filter and wrapper methods by including feature interactions while maintaining reasonable computational costs. These methods are iterative as they take each model iteration and carefully extract features contributing to most of the training in that iteration. Examples of embedded methods: LASSO Regularization (L1), Random Forest Importance.  ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "20. During analysis, how do you treat the missing values?",
        "answer": "To identify the extent of missing values, we first have to identify the variables with the missing values. Let us say a pattern is identified. The analyst should now concentrate on them as it could lead to interesting and meaningful insights. However, if there are no patterns identified, we can substitute the missing values with the median or mean values or we can simply ignore the missing values. If the variable is categorical, the common strategies for handling missing values include: Assigning a New Category: You can assign a new category, such as \"Unknown\" or \"Other,\" to represent the missing values.\nMode imputation: You can replace missing values with the mode, which represents the most frequent category in the variable.\nUsing a Separate Category: If the missing values carry significant information, you can create a separate category to indicate missing values. Assigning a New Category: You can assign a new category, such as \"Unknown\" or \"Other,\" to represent the missing values. Assigning a New Category: Mode imputation: You can replace missing values with the mode, which represents the most frequent category in the variable. Mode imputation: Using a Separate Category: If the missing values carry significant information, you can create a separate category to indicate missing values. Using a Separate Category: It's important to select an appropriate strategy based on the nature of the data and the potential impact on subsequent analysis or modelling. If 80% of the values are missing for a particular variable, then we would drop the variable instead of treating the missing values.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "21. Will treating categorical variables as continuous variables result in a better predictive model?",
        "answer": "Yes! A categorical variable is a variable that can be assigned to two or more categories with no definite category ordering. Ordinal variables are similar to categorical variables with proper and clear ordering defines. So, if the variable is ordinal, then treating the categorical value as a continuous variable will result in better predictive models.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "22. How will you treat missing values during data analysis?",
        "answer": "The impact of missing values can be known after identifying what type of variables have missing values. If the data analyst finds any pattern in these missing values, then there are chances of finding meaningful insights.\nIn case of patterns are not found, then these missing values can either be ignored or can be replaced with default values such as mean, minimum, maximum, or median values.\nIf the missing values belong to categorical variables, the common strategies for handling missing values include:\nAssigning a new category: You can assign a new category, such as \"Unknown\" or \"Other,\" to represent the missing values.\nMode imputation: You can replace missing values with the mode, which represents the most frequent category in the variable.\nUsing a separate category: If the missing values carry significant information, you can create a separate category to indicate the missing values.\nIt's important to select an appropriate strategy based on the nature of the data and the potential impact on subsequent analysis or modelling.\nIf 80% of values are missing, then it depends on the analyst to either replace them with default values or drop the variables. If the data analyst finds any pattern in these missing values, then there are chances of finding meaningful insights. In case of patterns are not found, then these missing values can either be ignored or can be replaced with default values such as mean, minimum, maximum, or median values. If the missing values belong to categorical variables, the common strategies for handling missing values include:\nAssigning a new category: You can assign a new category, such as \"Unknown\" or \"Other,\" to represent the missing values.\nMode imputation: You can replace missing values with the mode, which represents the most frequent category in the variable.\nUsing a separate category: If the missing values carry significant information, you can create a separate category to indicate the missing values.\nIt's important to select an appropriate strategy based on the nature of the data and the potential impact on subsequent analysis or modelling. Assigning a new category: You can assign a new category, such as \"Unknown\" or \"Other,\" to represent the missing values.\nMode imputation: You can replace missing values with the mode, which represents the most frequent category in the variable.\nUsing a separate category: If the missing values carry significant information, you can create a separate category to indicate the missing values.\nIt's important to select an appropriate strategy based on the nature of the data and the potential impact on subsequent analysis or modelling. Assigning a new category: You can assign a new category, such as \"Unknown\" or \"Other,\" to represent the missing values. Assigning a new category: Mode imputation: You can replace missing values with the mode, which represents the most frequent category in the variable. Mode imputation: Using a separate category: If the missing values carry significant information, you can create a separate category to indicate the missing values.\nIt's important to select an appropriate strategy based on the nature of the data and the potential impact on subsequent analysis or modelling. Using a separate category  If 80% of values are missing, then it depends on the analyst to either replace them with default values or drop the variables.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "23. What does the ROC Curve represent and how to create it?",
        "answer": "ROC (Receiver Operating Characteristic) curve is a graphical representation of the contrast between false-positive rates and true positive rates at different thresholds. The curve is used as a proxy for a trade-off between sensitivity and specificity. ROC (Receiver Operating Characteristic) The ROC curve is created by plotting values of true positive rates (TPR or sensitivity) against false-positive rates (FPR or (1-specificity)) TPR represents the proportion of observations correctly predicted as positive out of overall positive observations. The FPR represents the proportion of observations incorrectly predicted out of overall negative observations. Consider the example of medical testing, the TPR represents the rate at which people are correctly tested positive for a particular disease.  ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "24. What are the differences between univariate, bivariate and multivariate analysis?",
        "answer": "Statistical analyses are classified based on the number of variables processed at a given time. Univariate analysis Bivariate analysis Multivariate analysis\nThis analysis deals with solving only one variable at a time. This analysis deals with the statistical study of two variables at a given time. This analysis deals with statistical analysis of more than two variables and studies the responses.\nExample: Sales pie charts based on territory. Example: Scatterplot of Sales and spend volume analysis study. Example: Study of the relationship between human’s social media habits and their self-esteem which depends on multiple factors like age, number of hours spent, employment status, relationship status, etc. Univariate analysis Bivariate analysis Multivariate analysis\nThis analysis deals with solving only one variable at a time. This analysis deals with the statistical study of two variables at a given time. This analysis deals with statistical analysis of more than two variables and studies the responses.\nExample: Sales pie charts based on territory. Example: Scatterplot of Sales and spend volume analysis study. Example: Study of the relationship between human’s social media habits and their self-esteem which depends on multiple factors like age, number of hours spent, employment status, relationship status, etc. Univariate analysis Bivariate analysis Multivariate analysis Univariate analysis Bivariate analysis Multivariate analysis Univariate analysis Bivariate analysis Multivariate analysis This analysis deals with solving only one variable at a time. This analysis deals with the statistical study of two variables at a given time. This analysis deals with statistical analysis of more than two variables and studies the responses.\nExample: Sales pie charts based on territory. Example: Scatterplot of Sales and spend volume analysis study. Example: Study of the relationship between human’s social media habits and their self-esteem which depends on multiple factors like age, number of hours spent, employment status, relationship status, etc. This analysis deals with solving only one variable at a time. This analysis deals with the statistical study of two variables at a given time. This analysis deals with statistical analysis of more than two variables and studies the responses. This analysis deals with solving only one variable at a time. This analysis deals with the statistical study of two variables at a given time. This analysis deals with statistical analysis of more than two variables and studies the responses. Example: Sales pie charts based on territory. Example: Scatterplot of Sales and spend volume analysis study. Example: Study of the relationship between human’s social media habits and their self-esteem which depends on multiple factors like age, number of hours spent, employment status, relationship status, etc. Example: Sales pie charts based on territory. Example: Scatterplot of Sales and spend volume analysis study. Example: Study of the relationship between human’s social media habits and their self-esteem which depends on multiple factors like age, number of hours spent, employment status, relationship status, etc.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "25. What is the difference between the Test set and validation set?",
        "answer": "The test set is used to test or evaluate the performance of the trained model. It evaluates the predictive power of the model.\nThe validation set is part of the training set that is used to select parameters for avoiding model overfitting. ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "26. What do you understand by a kernel trick?",
        "answer": "Kernel functions are generalized dot product functions used for the computing dot product of vectors xx and yy in high dimensional feature space. Kernal trick method is used for solving a non-linear problem by using a linear classifier by transforming linearly inseparable data into separable ones in higher dimensions.  ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "27. Differentiate between box plot and histogram.",
        "answer": "Box plots and histograms are both visualizations used for showing data distributions for efficient communication of information.\nHistograms are the bar chart representation of information that represents the frequency of numerical variable values that are useful in estimating probability distribution, variations and outliers.\nBoxplots are used for communicating different aspects of data distribution where the shape of the distribution is not seen but still the insights can be gathered. These are useful for comparing multiple charts at the same time as they take less space when compared to histograms.    ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "28. How will you balance/correct imbalanced data?",
        "answer": "There are different techniques to correct/balance imbalanced data. It can be done by increasing the sample numbers for minority classes. The number of samples can be decreased for those classes with extremely high data points. Following are some approaches followed to balance data: Use the right evaluation metrics: In cases of imbalanced data, it is very important to use the right evaluation metrics that provide valuable information. \nSpecificity/Precision: Indicates the number of selected instances that are relevant.\nSensitivity: Indicates the number of relevant instances that are selected.\nF1 score: It represents the harmonic mean of precision and sensitivity.\nMCC (Matthews correlation coefficient): It represents the correlation coefficient between observed and predicted binary classifications.\nAUC (Area Under the Curve): This represents a relation between the true positive rates and false-positive rates. Use the right evaluation metrics: In cases of imbalanced data, it is very important to use the right evaluation metrics that provide valuable information. \nSpecificity/Precision: Indicates the number of selected instances that are relevant.\nSensitivity: Indicates the number of relevant instances that are selected.\nF1 score: It represents the harmonic mean of precision and sensitivity.\nMCC (Matthews correlation coefficient): It represents the correlation coefficient between observed and predicted binary classifications.\nAUC (Area Under the Curve): This represents a relation between the true positive rates and false-positive rates. Use the right evaluation metrics:  Specificity/Precision: Indicates the number of selected instances that are relevant.\nSensitivity: Indicates the number of relevant instances that are selected.\nF1 score: It represents the harmonic mean of precision and sensitivity.\nMCC (Matthews correlation coefficient): It represents the correlation coefficient between observed and predicted binary classifications.\nAUC (Area Under the Curve): This represents a relation between the true positive rates and false-positive rates. Specificity/Precision: Indicates the number of selected instances that are relevant. Specificity/Precision: Sensitivity: Indicates the number of relevant instances that are selected. Sensitivity: F1 score: It represents the harmonic mean of precision and sensitivity. F1 score: MCC (Matthews correlation coefficient): It represents the correlation coefficient between observed and predicted binary classifications. MCC (Matthews correlation coefficient): AUC (Area Under the Curve): This represents a relation between the true positive rates and false-positive rates. AUC (Area Under the Curve): For example, consider the below graph that illustrates training data: Here, if we measure the accuracy of the model in terms of getting \"0\"s, then the accuracy of the model would be very high -> 99.9%, but the model does not guarantee any valuable information. In such cases, we can apply different evaluation metrics as stated above.   Training Set Resampling: It is also possible to balance data by working on getting different datasets and this can be achieved by resampling. There are two approaches followed under-sampling that is used based on the use case and the requirements:\nUnder-sampling This balances the data by reducing the size of the abundant class and is used when the data quantity is sufficient. By performing this, a new dataset that is balanced can be retrieved and this can be used for further modeling.\nOver-sampling This is used when data quantity is not sufficient. This method balances the dataset by trying to increase the samples size. Instead of getting rid of extra samples, new samples are generated and introduced by employing the methods of repetition, bootstrapping, etc.\nPerform K-fold cross-validation correctly: Cross-Validation needs to be applied properly while using over-sampling. The cross-validation should be done before over-sampling because if it is done later, then it would be like overfitting the model to get a specific result. To avoid this, resampling of data is done repeatedly with different ratios. Training Set Resampling: It is also possible to balance data by working on getting different datasets and this can be achieved by resampling. There are two approaches followed under-sampling that is used based on the use case and the requirements:\nUnder-sampling This balances the data by reducing the size of the abundant class and is used when the data quantity is sufficient. By performing this, a new dataset that is balanced can be retrieved and this can be used for further modeling.\nOver-sampling This is used when data quantity is not sufficient. This method balances the dataset by trying to increase the samples size. Instead of getting rid of extra samples, new samples are generated and introduced by employing the methods of repetition, bootstrapping, etc. Training Set Resampling: Under-sampling This balances the data by reducing the size of the abundant class and is used when the data quantity is sufficient. By performing this, a new dataset that is balanced can be retrieved and this can be used for further modeling.\nOver-sampling This is used when data quantity is not sufficient. This method balances the dataset by trying to increase the samples size. Instead of getting rid of extra samples, new samples are generated and introduced by employing the methods of repetition, bootstrapping, etc. Under-sampling This balances the data by reducing the size of the abundant class and is used when the data quantity is sufficient. By performing this, a new dataset that is balanced can be retrieved and this can be used for further modeling. Under-sampling Over-sampling This is used when data quantity is not sufficient. This method balances the dataset by trying to increase the samples size. Instead of getting rid of extra samples, new samples are generated and introduced by employing the methods of repetition, bootstrapping, etc. Over-sampling Perform K-fold cross-validation correctly: Cross-Validation needs to be applied properly while using over-sampling. The cross-validation should be done before over-sampling because if it is done later, then it would be like overfitting the model to get a specific result. To avoid this, resampling of data is done repeatedly with different ratios. Perform K-fold cross-validation correctly:",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "29. What is better - random forest or multiple decision trees?",
        "answer": "Random forest is better than multiple decision trees as random forests are much more robust, accurate, and lesser prone to overfitting as it is an ensemble method that ensures multiple weak decision trees learn strongly.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "30. Consider a case where you know the probability of finding at least one shooting star in a 15-minute interval is 30%. Evaluate the probability of finding at least one shooting star in a one-hour duration?",
        "answer": "We know that,\nProbability of finding atleast 1 shooting star in 15 min = P(sighting in 15min) = 30% = 0.3\nHence, Probability of not sighting any \nshooting star in 15 min = 1-P(sighting in 15min)\n                       = 1-0.3\n                       = 0.7\n                       \nProbability of not finding shooting star in 1 hour\n                       = 0.7^4\n                       = 0.1372\nProbability of finding atleast 1 \nshooting star in 1 hour = 1-0.1372\n                       = 0.8628 We know that,\nProbability of finding atleast 1 shooting star in 15 min = P(sighting in 15min) = 30% = 0.3\nHence, Probability of not sighting any \nshooting star in 15 min = 1-P(sighting in 15min)\n                       = 1-0.3\n                       = 0.7\n                       \nProbability of not finding shooting star in 1 hour\n                       = 0.7^4\n                       = 0.1372\nProbability of finding atleast 1 \nshooting star in 1 hour = 1-0.1372\n                       = 0.8628 So the probability is 0.8628 = 86.28%",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "31. Toss the selected coin 10 times from a jar of 1000 coins. Out of 1000 coins, 999 coins are fair and 1 coin is double-headed, assume that you see 10 heads. Estimate the probability of getting a head in the next coin toss.",
        "answer": "We know that there are two types of coins - fair and double-headed. Hence, there are two possible ways of choosing a coin. The first is to choose a fair coin and the second is to choose a coin having 2 heads. P(selecting fair coin) = 999/1000 = 0.999\nP(selecting double headed coin) = 1/1000 = 0.001  Using Bayes rule, P(selecting 10 heads in row) = P(selecting fair coin)* Getting 10 heads + P(selecting double headed coin)\nP(selecting 10 heads in row) = P(A)+P(B)\n\nP (A)  =  0.999 * (1/2)^10  \n      =  0.999 * (1/1024)  \n      =  0.000976\nP (B)  =  0.001 * 1  =  0.001\nP( A / (A + B) )  = 0.000976 /  (0.000976 + 0.001)                   = 0.4939\nP( B / (A + B))   = 0.001 / 0.001976  \n                 = 0.5061\nP(selecting head in next toss) = P(A/A+B) * 0.5 + P(B/A+B) * 1 \n                              = 0.4939 * 0.5 + 0.5061  \n                              = 0.7531 P(selecting 10 heads in row) = P(selecting fair coin)* Getting 10 heads + P(selecting double headed coin)\nP(selecting 10 heads in row) = P(A)+P(B)\n\nP (A)  =  0.999 * (1/2)^10  \n      =  0.999 * (1/1024)  \n      =  0.000976\nP (B)  =  0.001 * 1  =  0.001\nP( A / (A + B) )  = 0.000976 /  (0.000976 + 0.001)                   = 0.4939\nP( B / (A + B))   = 0.001 / 0.001976  \n                 = 0.5061\nP(selecting head in next toss) = P(A/A+B) * 0.5 + P(B/A+B) * 1 \n                              = 0.4939 * 0.5 + 0.5061  \n                              = 0.7531 So, the answer is 0.7531 or 75.3%.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "32. What are some examples when false positive has proven important than false negative?",
        "answer": "Before citing instances, let us understand what are false positives and false negatives. False Positives are those cases that were wrongly identified as an event even if they were not. They are called Type I errors.\nFalse Negatives are those cases that were wrongly identified as non-events despite being an event. They are called Type II errors. False Positives are those cases that were wrongly identified as an event even if they were not. They are called Type I errors. False Negatives are those cases that were wrongly identified as non-events despite being an event. They are called Type II errors. Some examples where false positives were important than false negatives are: In the medical field: Consider that a lab report has predicted cancer to a patient even if he did not have cancer. This is an example of a false positive error. It is dangerous to start chemotherapy for that patient as he doesn’t have cancer as starting chemotherapy would lead to damage of healthy cells and might even actually lead to cancer.\nIn the e-commerce field: Suppose a company decides to start a campaign where they give $100 gift vouchers for purchasing $10000 worth of items without any minimum purchase conditions. They assume it would result in at least 20% profit for items sold above $10000. What if the vouchers are given to the customers who haven’t purchased anything but have been mistakenly marked as those who purchased $10000 worth of products. This is the case of false-positive error. In the medical field: Consider that a lab report has predicted cancer to a patient even if he did not have cancer. This is an example of a false positive error. It is dangerous to start chemotherapy for that patient as he doesn’t have cancer as starting chemotherapy would lead to damage of healthy cells and might even actually lead to cancer. In the e-commerce field: Suppose a company decides to start a campaign where they give $100 gift vouchers for purchasing $10000 worth of items without any minimum purchase conditions. They assume it would result in at least 20% profit for items sold above $10000. What if the vouchers are given to the customers who haven’t purchased anything but have been mistakenly marked as those who purchased $10000 worth of products. This is the case of false-positive error.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "33. Give one example where both false positives and false negatives are important equally?",
        "answer": "In Banking fields: Lending loans are the main sources of income to the banks. But if the repayment rate isn’t good, then there is a risk of huge losses instead of any profits. So giving out loans to customers is a gamble as banks can’t risk losing good customers but at the same time, they can’t afford to acquire bad customers. This case is a classic example of equal importance in false positive and false negative scenarios.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "34. Is it good to do dimensionality reduction before fitting a Support Vector Model?",
        "answer": "If the features number is greater than observations then doing dimensionality reduction improves the SVM (Support Vector Model).",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "35. What are various assumptions used in linear regression? What would happen if they are violated?",
        "answer": "Linear regression is done under the following assumptions: The sample data used for modeling represents the entire population.\nThere exists a linear relationship between the X-axis variable and the mean of the Y variable.\nThe residual variance is the same for any X values. This is called homoscedasticity\nThe observations are independent of one another.\nY is distributed normally for any value of X. The sample data used for modeling represents the entire population. There exists a linear relationship between the X-axis variable and the mean of the Y variable. The residual variance is the same for any X values. This is called homoscedasticity The observations are independent of one another. Y is distributed normally for any value of X. Extreme violations of the above assumptions lead to redundant results. Smaller violations of these result in greater variance or bias of the estimates.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "36. How is feature selection performed using the regularization method?",
        "answer": "The method of regularization entails the addition of penalties to different parameters in the machine learning model for reducing the freedom of the model to avoid the issue of overfitting.\nThere are various regularization methods available such as linear model regularization, Lasso/L1 regularization, etc. The linear model regularization applies penalty over coefficients that multiplies the predictors. The Lasso/L1 regularization has the feature of shrinking some coefficients to zero, thereby making it eligible to be removed from the model. ",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "37. How do you identify if a coin is biased?",
        "answer": "To identify this, we perform a hypothesis test as below:\nAccording to the null hypothesis, the coin is unbiased if the probability of head flipping is 50%. According to the alternative hypothesis, the coin is biased and the probability is not equal to 500. Perform the below steps:  Flip coin 500 times\nCalculate p-value.\nCompare the p-value against the alpha -> result of two-tailed test (0.05/2 = 0.025). Following two cases might occur:\np-value > alpha: Then null hypothesis holds good and the coin is unbiased.\np-value < alpha: Then the null hypothesis is rejected and the coin is biased. Flip coin 500 times Calculate p-value. Compare the p-value against the alpha -> result of two-tailed test (0.05/2 = 0.025). Following two cases might occur:\np-value > alpha: Then null hypothesis holds good and the coin is unbiased.\np-value < alpha: Then the null hypothesis is rejected and the coin is biased. p-value > alpha: Then null hypothesis holds good and the coin is unbiased.\np-value < alpha: Then the null hypothesis is rejected and the coin is biased. p-value > alpha: Then null hypothesis holds good and the coin is unbiased. p-value > alpha: p-value < alpha: Then the null hypothesis is rejected and the coin is biased. p-value < alpha:",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "38. What is the importance of dimensionality reduction?",
        "answer": "The process of dimensionality reduction constitutes reducing the number of features in a dataset to avoid overfitting and reduce the variance. There are mostly 4 advantages of this process: This reduces the storage space and time for model execution.\nRemoves the issue of multi-collinearity thereby improving the parameter interpretation of the ML model.\nMakes it easier for visualizing data when the dimensions are reduced.\nAvoids the curse of increased dimensionality. This reduces the storage space and time for model execution. Removes the issue of multi-collinearity thereby improving the parameter interpretation of the ML model. Makes it easier for visualizing data when the dimensions are reduced. Avoids the curse of increased dimensionality.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "39. How is the grid search parameter different from the random search tuning strategy?",
        "answer": "Tuning strategies are used to find the right set of hyperparameters. Hyperparameters are those properties that are fixed and model-specific before the model is tested or trained on the dataset. Both the grid search and random search tuning strategies are optimization techniques to find efficient hyperparameters. Grid Search:\nHere, every combination of a preset list of hyperparameters is tried out and evaluated.\nThe search pattern is similar to searching in a grid where the values are in a matrix and a search is performed. Each parameter set is tried out and their accuracy is tracked. after every combination is tried out, the model with the highest accuracy is chosen as the best one.\nThe main drawback here is that, if the number of hyperparameters is increased, the technique suffers. The number of evaluations can increase exponentially with each increase in the hyperparameter. This is called the problem of dimensionality in a grid search. Grid Search:\nHere, every combination of a preset list of hyperparameters is tried out and evaluated.\nThe search pattern is similar to searching in a grid where the values are in a matrix and a search is performed. Each parameter set is tried out and their accuracy is tracked. after every combination is tried out, the model with the highest accuracy is chosen as the best one.\nThe main drawback here is that, if the number of hyperparameters is increased, the technique suffers. The number of evaluations can increase exponentially with each increase in the hyperparameter. This is called the problem of dimensionality in a grid search. Grid Search Here, every combination of a preset list of hyperparameters is tried out and evaluated.\nThe search pattern is similar to searching in a grid where the values are in a matrix and a search is performed. Each parameter set is tried out and their accuracy is tracked. after every combination is tried out, the model with the highest accuracy is chosen as the best one.\nThe main drawback here is that, if the number of hyperparameters is increased, the technique suffers. The number of evaluations can increase exponentially with each increase in the hyperparameter. This is called the problem of dimensionality in a grid search. Here, every combination of a preset list of hyperparameters is tried out and evaluated. The search pattern is similar to searching in a grid where the values are in a matrix and a search is performed. Each parameter set is tried out and their accuracy is tracked. after every combination is tried out, the model with the highest accuracy is chosen as the best one. The main drawback here is that, if the number of hyperparameters is increased, the technique suffers. The number of evaluations can increase exponentially with each increase in the hyperparameter. This is called the problem of dimensionality in a grid search.   Random Search:\nIn this technique, random combinations of hyperparameters set are tried and evaluated for finding the best solution. For optimizing the search, the function is tested at random configurations in parameter space as shown in the image below.\nIn this method, there are increased chances of finding optimal parameters because the pattern followed is random. There are chances that the model is trained on optimized parameters without the need for aliasing.\nThis search works the best when there is a lower number of dimensions as it takes less time to find the right set. Random Search:\nIn this technique, random combinations of hyperparameters set are tried and evaluated for finding the best solution. For optimizing the search, the function is tested at random configurations in parameter space as shown in the image below.\nIn this method, there are increased chances of finding optimal parameters because the pattern followed is random. There are chances that the model is trained on optimized parameters without the need for aliasing.\nThis search works the best when there is a lower number of dimensions as it takes less time to find the right set. Random Search: In this technique, random combinations of hyperparameters set are tried and evaluated for finding the best solution. For optimizing the search, the function is tested at random configurations in parameter space as shown in the image below.\nIn this method, there are increased chances of finding optimal parameters because the pattern followed is random. There are chances that the model is trained on optimized parameters without the need for aliasing.\nThis search works the best when there is a lower number of dimensions as it takes less time to find the right set. In this technique, random combinations of hyperparameters set are tried and evaluated for finding the best solution. For optimizing the search, the function is tested at random configurations in parameter space as shown in the image below. In this method, there are increased chances of finding optimal parameters because the pattern followed is random. There are chances that the model is trained on optimized parameters without the need for aliasing. This search works the best when there is a lower number of dimensions as it takes less time to find the right set.   Conclusion: Data Science is a very vast field and comprises many topics like Data Mining, Data Analysis, Data Visualization, Machine Learning, Deep Learning, and most importantly it is laid on the foundation of mathematical concepts like Linear Algebra and Statistical analysis. Since there are a lot of pre-requisites for becoming a good professional Data Scientist, the perks and benefits are very big. Data Scientist has become the most sought job role these days. Looking for a comprehensive course on Data Science: Check out Scaler’s Data Science Course. Check out Scaler’s Data Science Course Scaler’s Data Science Course . Useful Resources: Useful Resources: Best Data Science Courses\nPython Data Science Interview Questions\nGoogle Data Scientist Salary\nSpotify Data Scientist Salary\nData Scientist Salary\nData Science Resume\nData Analyst: Career Guide\nTableau Interview\nAdditional Technical Interview Questions Best Data Science Courses Best Data Science Courses Python Data Science Interview Questions Python Data Science Interview Questions Google Data Scientist Salary Google Data Scientist Salary Spotify Data Scientist Salary Spotify Data Scientist Salary Data Scientist Salary Data Scientist Salary Data Science Resume Data Science Resume Data Analyst: Career Guide Data Analyst: Career Guide Tableau Interview Tableau Interview Additional Technical Interview Questions Additional Technical Interview Questions",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "1. How do I prepare for a data science interview?",
        "answer": "Some of the preparation tips for data science interviews are as follows: preparation tips Resume Building: Firstly, prepare your resume well. It is preferable if the resume is only a 1-page resume, especially for a fresher. You should give great thought to the format of the resume as it matters a lot. The data science interviews can be based more on the topics like linear and logistic regression, SVM, root cause analysis, random forest, etc. So, prepare well for the data science-specific questions like those discussed in this article, make sure your resume has a mention of such important topics and you have a good knowledge of them. Also, please make sure that your resume contains some Data Science-based Projects as well. It is always better to have a group project or internship experience in the field that you are interested to go for. However, personal projects will also have a good impact on the resume. So, your resume should contain at least 2-3 data science-based projects that show your skill and knowledge level in data science. Please do not write any such skill in your resume that you do not possess. If you are just familiar with some technology and have not studied it at an advanced level, you can mention a beginner tag for those skills.\nPrepare Well: Apart from the specific questions on data science, questions on Core subjects like Database Management systems (DBMS), Operating Systems (OS), Computer Networks(CN), and Object-Oriented Programming (OOPS) can be asked from the freshers especially. So, prepare well for that as well.\nData structures and Algorithms are the basic building blocks of programming. So, you should be well versed with that as well.\nResearch the Company: This is the tip that most people miss and it is very important. If you are going for an interview with any company, read about the company before and especially in the case of data science, learn which libraries the company uses, what kind of models are they building, and so on. This gives you an edge over most other people. Resume Building: Firstly, prepare your resume well. It is preferable if the resume is only a 1-page resume, especially for a fresher. You should give great thought to the format of the resume as it matters a lot. The data science interviews can be based more on the topics like linear and logistic regression, SVM, root cause analysis, random forest, etc. So, prepare well for the data science-specific questions like those discussed in this article, make sure your resume has a mention of such important topics and you have a good knowledge of them. Also, please make sure that your resume contains some Data Science-based Projects as well. It is always better to have a group project or internship experience in the field that you are interested to go for. However, personal projects will also have a good impact on the resume. So, your resume should contain at least 2-3 data science-based projects that show your skill and knowledge level in data science. Please do not write any such skill in your resume that you do not possess. If you are just familiar with some technology and have not studied it at an advanced level, you can mention a beginner tag for those skills. Resume Building: linear and logistic regression, SVM, root cause analysis, random forest, etc. Data Science-based Projects Data Science-based Projects Prepare Well: Apart from the specific questions on data science, questions on Core subjects like Database Management systems (DBMS), Operating Systems (OS), Computer Networks(CN), and Object-Oriented Programming (OOPS) can be asked from the freshers especially. So, prepare well for that as well. Prepare Well: Data structures and Algorithms are the basic building blocks of programming. So, you should be well versed with that as well. Data structures and Algorithms Research the Company: This is the tip that most people miss and it is very important. If you are going for an interview with any company, read about the company before and especially in the case of data science, learn which libraries the company uses, what kind of models are they building, and so on. This gives you an edge over most other people. Research the Company:",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "2. Are data science interviews hard?",
        "answer": "An honest reply will be “YES”. This is because of the fact that this field is newly emerging and will keep on emerging forever. In almost every interview, you have to answer many tough and challenging questions with full confidence and your concepts should be strong to satisfy the interviewer. However, with great practice, anything can be achieved. So, follow the tips discussed above and keep practising and learning. You will definitely succeed.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "3. What are the top 3 technical skills of a data scientist?",
        "answer": "The top 3 skills of a data scientist are: Mathematics: Data science requires a lot of mathematics and a good data scientist is strong in it. It is not possible to become a good data scientist if you are weak in mathematics.\nMachine Learning and Deep Learning: A data scientist should be very skilled in Artificial Intelligence technologies like deep learning and machine learning. Some good projects and a lot of hands-on practice will help in achieving excellence in that field.\nProgramming: This is an obvious yet the most important skill. If a person is good at programming it does mean that he/she can solve complex problems as that is just a problem-solving skill. Programming is the ability to write clean and industry-understandable code. This is the skill that most freshers slack because of the lack of exposure to industry-level code. This also improves with practice and experience. Mathematics: Data science requires a lot of mathematics and a good data scientist is strong in it. It is not possible to become a good data scientist if you are weak in mathematics. Mathematics: Machine Learning and Deep Learning: A data scientist should be very skilled in Artificial Intelligence technologies like deep learning and machine learning. Some good projects and a lot of hands-on practice will help in achieving excellence in that field. Machine Learning and Deep Learning Programming: This is an obvious yet the most important skill. If a person is good at programming it does mean that he/she can solve complex problems as that is just a problem-solving skill. Programming is the ability to write clean and industry-understandable code. This is the skill that most freshers slack because of the lack of exposure to industry-level code. This also improves with practice and experience. Programming:",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "4. Is data science a good career?",
        "answer": "Yes, data science is one of the most futuristic and great career fields. Today and tomorrow or even years later, this field is just going to expand and never end. The reason is simple. Data can be compared to gold today as it is the key to selling everything in the world. Data scientists know how to play with this data to generate some tremendous outputs that are not even imaginable today making it a great career.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "5. Are coding questions asked in data science interviews?",
        "answer": "Yes, coding questions are asked in data science interviews. One more important thing to note here is that the data scientists are very good problem solvers as they are indulged in a lot of strict mathematics-based activities. Hence, the interviewer expects the data science interview candidates to know data structures and algorithms and at least come up with the solutions to most of the problems.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "6. Is python and SQL enough for data science?",
        "answer": "Yes. Python and SQL are sufficient for the data science roles. However, knowing the R programming Language can have also have a better impact. If you know these 3 languages, you have got the edge over most of the competitors. However, Python and SQL are enough for data science interviews.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "7. What are Data Science tools?",
        "answer": "There are various data science tools available in the market nowadays. Various tools can be of great importance. Tensorflow is one of the most famous data science tools. Some of the other famous tools are BigML, SAS (Statistical Analysis System), Knime, Scikit, Pytorch, etc.",
        "reference": "interviewbit.com",
        "role": "data-science"
    },
    {
        "question": "1) What do you understand by the term Data Science?",
        "answer": "ADVERTISEMENT\nData science is a multidisciplinary field that combines statistics, data analysis, machine learning, Mathematics, computer science, and related methods, to understand the data and to solve complex problems.\nData Science is a deep study of the massive amount of data, and finding useful information from raw, structured, and unstructured data.\nData science is similar to data mining or big data techniques, which deals with a huge amount of data and extract insights from data.\nIt uses various tools, powerful programming, scientific methods, and algorithms to solve the data-related problems.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "2) What are the differences between Data Science, Machine Learning, and Artificial intelligence?",
        "answer": "Data science, Machine learning, and Artificial Intelligence are the three related and most confusing concepts of computer science. Below diagram is showing the relation between AI, ML, and Data Science.\n\nFollowing are some main points to differentiate between these three terms:\nData Science Artificial Intelligence Machine Learning\nData science is a multidisciplinary field that is used for deep study of data and finding useful insights from it. Artificial Intelligence is a branch of computer science that build intelligent machines which can mimic the human brain. Machine learning is a branch of computer science which enables machines to learn from the data automatically.\nData Science is not exactly a subset of artificial intelligence and machine learning, but it uses ML algorithms for data analysis and future prediction. Artificial Intelligence is a wide field which ranges from natural language processing to deep learning. Machine learning is a subset of Artificial Intelligence and a part of data science.\nThe goal of Data science is to find hidden patterns from the raw data. The goal of artificial intelligence is to make intelligent machines. The goal of machine learning is to allow a machine to learn from data automatically.\nData science finds meaningful insights from data to solve complex problems. Artificial intelligence creates intelligent machines to solve complex problems. Machine learning uses data and train models to solve some specific problems.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "3) Discuss Linear Regression?",
        "answer": "Linear Regression is one of the popular machine learning algorithms based on supervised learning, which is used for understanding the relationship between input and output numerical variables.\nIt applies regression analysis, a predictive modeling technique that finds a relationship between the dependent and independent variables.\nIt shows the linear relationship between independent and dependent variables, hence it is called a linear regression algorithm.\nLinear Regression is used for prediction of continuous numerical variables such as sales/day, temperature, etc.\nIt can be divided into two categories:\nSimple Linear Regression\nMultiple Linear Regression\nIf we talk about simple linear regression algorithm, then it shows a linear relationship between the variables, which can be understood using the below equation, and graph plot.\ny=mx + c",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "4) Differentiate between Supervised and Unsupervised Learning?",
        "answer": "Supervised and Unsupervised learning are types of Machine learning.\nSupervised Learning:\nSupervised learning is based on the supervision concept. In supervised learning, we train our machine learning model using sample data, and on the basis of that training data, the model predicts the output.\nUnsupervised learning:\nUnsupervised learning does not have any supervision concept. Hence, in unsupervised learning machine learns without any supervision. In unsupervised learning, we provide data which is not labeled, classified, or categorized.\nBelow are some main differences between supervised and unsupervised learning:\nSr. No. Supervised Learning Unsupervised learning\n1. In supervised learning, the machine learns in supervision using training data. In unsupervised learning, the machine learns without any supervision.\n2. Supervised learning uses labeled data to train the model. Unsupervised learning uses unlabeled data to train the model.\n3. It uses known input data with the corresponding output. It uses unknown data without any corresponding output.\n4. It can be grouped into Classification and Regression algorithms. It can be grouped into Clustering and Association algorithms.\n5. It has more complex computation than Unsupervised learning. It has less complex computation than supervised learning.\n6. It provides more accurate and reliable output. It provides less reliable and less accurate output.\n7. It can also use Off-line data analysis. It uses real-time data analysis.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "5) What do you understand by bias, variance trade-off?",
        "answer": "When we work with a supervised machine learning algorithm, the model learns from the training data. The model always tries to best estimate the mapping function between the output variable(Y) and the input variable(X). The estimation for target function may generate the prediction error, which can be divided mainly into Bias error, and Variance error. These errors can be explained as:\nBias Error: Bias is a prediction error which is introduced in the model due to oversimplifying the machine learning algorithms. It is the difference of predicted output and actual output. There are two types of bias:\nHigh Bias: If the suggested predicted values are much different from actual value, then it is called as high bias. Due to high bias, an algorithm may miss the relevant relationships between the input features and target output, which is called underfitting.\nLow Bias: If the suggested predicted values are less different from actual value, then it is called as low bias.\nVariance Error: If the machine learning model performs well with training dataset, but does not perform well with test dataset, then variance occurs. It can also be defined as an error caused by the model's sensitivity to small fluctuation in training dataset. The high variance would cause Overfitting in machine learning model, which means an algorithm introduce noise along with the underlying pattern in data to the model.\nBias Variance tradeoff:\nIn the machine learning model, we always try to have low bias and low variance, and\nIf we try to increase the bias, the variance decreases\nIf we try to increase the variance, the bias decreases.\nHence, trying to get an optimal bias and variance is called bias-variance trade-off. We can define it using the Bull eye diagram given below. There are four cases of bias and variances:\n\nIf there is low bias and low variance, the predicted output is mostly close to the desired output.\nIf there is low bias and high variance, the model is not consistent.\nIf there is high variance and low bias, the model is consistent but predicted results are far away from the actual output.\nIf there is high bias and high variance, then the model is inconsistent, and also predictions are much different with actual value. It is the worst case of bias and variance.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "6) Define Naive Bayes?",
        "answer": "Naive Bayes is a popular classification algorithm used for predictive modeling. It is a supervised machine learning algorithm which is based on Bayes theorem.\nIt is easy to build a model using Naive Bayes algorithm when working with a large dataset. It is comprised of two words, Naive and Bayes, where Naive means features are unrelated to each other.\nIn simple words, we can say that \"Naive Bayes classifier assumes that the features present in a class are statistically independent to the other features.\"",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "7) What is the SVM algorithm?",
        "answer": "SVM stands for Support Vector Machine. It is a supervised machine learning algorithm which is used for classification and regression analysis.\nIt works with labeled data as it is a part of supervised learning. The goal of support vector machine algorithm is to construct a hyperplane in an N-dimensional space. The hyperplane is a dividing line which distinct the objects of two different classes, it is also known as a decision boundary.\nIf there are only two distinct classes, then it is called as Binary SVM classifier. A schematic example of binary SVM classifier is given below.\n\n\nThe data point of a class which is nearest to the other class is called a support vector.\nThere are two types of SVM classifier:\nLinear SVM classifier: A classifier by which we can separate the set of objects into their respective group by drawing a single line, i.e., hyperplane, called as linear SVM classifier.\nNon-Linear SVM classifier: Non-linear SVM classifier applies on those objects which cannot be classified into two groups by a single line.\n\nOn the basis of error function, we can divide a SVM model into four categories:\nClassification SVM Type1\nClassification SVM Type2\nRegression SVM Type1\nRegression SVM Type1",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "8) What do you understand by Normal distribution?",
        "answer": "If the given data is distributed around a central value in the bell-shaped curve without any left or right bias, then it is called Normal distribution. It is also called a Bell Curve because it looks like a bell?shaped curve.\nThe normal distribution has a mean value, half of the data lies to the left of the curve, and half of the data lies right of the curve.\nIn probability theory, the normal distribution is also called a Gaussian distribution, which is used for the probability distribution.\nIt is a probability distribution function used to see the distribution of data over the given range.\nNormal distribution has two important parameters: mean(µ) and standard deviation(σ).",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "",
        "answer": "Reinforcement learning is a type of machine learning where an agent interacts with the environment and learns by his actions and outcomes. On each good action, he gets a positive reward, and for each bad action, he gets a negative reward. Consider the below image:\n\nThe goal of an agent in reinforcement learning is to maximize positive rewards.\nIn reinforcement learning, algorithms are not explicitly programmed for tasks but learns with experiences without any human intervention.\nThe reinforcement learning algorithms is different from supervised learning algorithms as there is no any training dataset is provided to the algorithm. Hence the algorithm automatically learns from experiences.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "10) What do you mean by p-value?",
        "answer": "The p-value is the probability value which is used to determine the statistical significance in a hypothesis test.\nHypothesis tests are used to check the validity of the null hypothesis (claim).\nP-values can be calculated using p-value tables or statistical software.\nThe p-values lies between 0 and 1. It can have mainly two cases:\n(p-value<0.05): A small p-value indicates strong evidence against the null hypothesis, so we can reject the null hypothesis.\n(p-value>0.05): A large p-value indicates weak evidence against the null hypothesis, so we consider the null hypothesis as true.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "11) Differentiate between Regression and Classification algorithms?",
        "answer": "Classification and Regression both are the supervised learning algorithms in machine learning, and uses the same concept of training datasets for making predictions. The main difference between both the algorithms is that the output variable in regression algorithms is Numerical or continuous, whereas in Classification algorithm output variables are Categorical or discrete.\nRegression Algorithm: A regression algorithm is about mapping the input variable x to some real numbers such as percentage, age, etc. Or we can say regression algorithms are used if the required output is continuous. Linear regression is a famous example of the regression algorithm.\nRegression Algorithms are used in weather forecasting, population growth prediction, market forecasting, etc.\nClassification Algorithm: A classification algorithm is about mapping the input variable x with a discrete number of labels such as true or false, yes or no, male-female, etc. Or we can say Classification algorithm is used if the required output is a discrete label. Logistic regression and decision trees are popular examples of a classification algorithm. The classification algorithm is used for image classification, spam detection, identity fraud detection, etc.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "12) Which is the best suitable language among Python and R for text analytics?",
        "answer": "Both R and Python are the suitable language for text analytics, but the preferred language is Python, because:\nPython has Pandas library, by which we can easily use data structure and data analysis tools.\nPython performs fast execution for all types of text analytics.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "13) What do you understand by L1 and L2 regularization methods?",
        "answer": "Regularization is a technique to reduce the complexity of the model. It helps to solve the over-fitting problem in a model when we have a large number of features in a dataset. Regularization controls the model complexity by adding a penalty term to the objective function.\nThere are two main regularization methods:\nL1 Regularization:\nL1 regularization method is also known as Lasso Regularization. L1 regularization adds a penalty term to the error function, where penalty term is the sum of the absolute values of weights.\nIt performs feature selection by providing 0 weight to unimportant features and non-zero weight to important features.\nIt is given below:\n\nHere is the sum of the squared difference between the actual value and the predicted value.\nis regularization term, and λ is penalty parameter which determines how much to penalize the weights.\nL2 Regularization:\nL2 regularization method is also known as Ridge Regularization. L2 regularization does the same as L1 regularization except that penalty term in L2 regularization is the sum of the squared values of weights.\nIt performs well if all the input features affect the output and all weights are of approximately equal size.\nIt is given as:\n\nHere, is the sum of the squared difference between actual value and predicted value.\nis the regularization term, and λ is the penalty parameter which determines how much to penalize the weights.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "14) What is the 80/20 rule? Explain its importance in model validation?",
        "answer": "In machine learning, we usually split the dataset into two parts:\nTraining set: Part of the dataset used to train the model.\nTest set: Part of the dataset used to test the performance of the model.\nThe best ratio to split the dataset is 80-20%, to create the validation set for machine learning model. Here, 80% is assigned for the training dataset, and 20% is for the test dataset. This ratio maybe 90-20%, 70-30%, 60-40%, but these ratios would not be preferable.\n\nImportance of 80/20 rule in model validation:\n\nThe process of evaluating a trained model on the test dataset is called as model validation in machine learning. In model validation, the ratio of splitting dataset is important to avoid Overfitting problem. The best preferable ration is 80-20%, which is also known as 80/20 rule, but it also depends upon the amount of data in a dataset.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "15) What do you understand by confusion matrix?",
        "answer": "Confusion matrix is a unique concept of the statistical classification problem.\nConfusion matrix is a type of table which is used for describing or measuring the performance of Binary classification model in machine learning.\nThe confusion matrix is itself easy to understand, but the terminologies used in the matrix can be confusing. It is also known as Error matrix.\nIt is used in statistics, data mining, machine learning, and different Artificial Intelligence applications.\nIt is a table with two dimensions, \"actual and predicted\" and identical set of classes in both dimensions of the table.\nThe confusion matrix has four following cases:\nTrue Positive(TP): The predictions is positive and its actually true.\nFalse Positive(FP): The prediction is positive but its actually false.\nTrue Negative(TN): The prediction is negative but its actually true.\nFalse Negative(FN): The prediction is negative and its false.\nThe classification accuracy can be obtained by the below formula:",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "16) What is the ROC curve?",
        "answer": "ROC curve stands for Receiver Operating Characteristics curve, which graphically represents the performance of a binary classifier model at all classification threshold. The curve is a plot of true positive rate (TPR) against false positive rate (FPR) for different threshold points.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "17) Explain the Decision Tree algorithm, and how is it different from the random forest algorithm?",
        "answer": "Decision tree algorithm belongs to supervised learning which solves both classifications and Regression problems in machine learning.\nDecision tree solves problems using a tree-type structure which has leaves, decision nodes, and links between nodes. Each node represents an attribute or feature, each branch of the tree represent the decision, and each leaf represents the outcomes.\nDecision tree algorithm often mimic human thinking hence, it can be easily understood as compared to other classifications algorithm.\nDifference between Decision Tree and Random Forest algorithm:\nDecision Tree Algorithm Random Forest Algorithm\nDecision tree algorithm is a tree-like structure to solve classification and regression problems. Random forest algorithm is a combination of various decision trees which gives the final output based on the average of each tree output.\nDecision tree may have a chance of Overfitting problem. Random Forest reduces the chance of Overfitting problem by averaging out several trees predictions.\nSimpler to understand as it is based on human thinking. This algorithm is comparatively complex.\nIt gives less accurate result as compared to the random forest algorithm. It gives a more accurate result.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "",
        "answer": "The data warehouse is a system which is used for analysis and reporting of data collected from operational systems and different data sources. Data warehouse plays an important role in Business Intelligence.\nIn a data warehouse, data is extracted from various sources, transformed (cleaned and integrated) according to decision support system needs, and stored into a data warehouse.\nThe data present in the data warehouse after analysis does not change, and it is directly used by end-users or for data visualization.\nAdvantages of Data Warehouse:\nData Warehouse makes data more readable, hence, strategic questions can be easily answered using various graphs, trends, plots, etc.\nData warehouse makes data analysis and operation faster and more accurate.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "19) What do you understand by clustering?",
        "answer": "Clustering is a way of dividing the data points into a number of groups such that data points within a group are more similar to each other than data points of other groups. These groups are called clusters, and hence, the similarities within the clusters is high, and similarities between the clusters is less.\nThe clustering techniques are used in various fields such as machine learning, data mining, image analysis, pattern recognition, etc.\n\nClustering is a type of supervised learning problems in machine learning. It can be divided into two types:\nHard Clustering\nSoft Clustering",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "20) How to determine the number of clusters in k-means clustering algorithm?",
        "answer": "In k-means clustering algorithm, the number of clusters depends on the value of k.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "21) Differentiate between K-means clustering and hierarchical clustering?",
        "answer": "The K-means clustering and Hierarchical Clustering both are the machine learning algorithms. Below are some main differences between both the clustering:\nK-means clustering Hierarchal Clustering\nK-means clustering is a simple clustering algorithm in which objects are divided into clusters. Hierarchal clustering shows the hierarchal or parent-child relationship between the clusters.\nIn k-means clustering, we need prior knowledge of k to define the number of clusters which sometimes may be difficult. In hierarchal clustering, we don't need prior knowledge of the number of clusters, and we can choose as per our requirement.\nK-means clustering can handle big data better than hierarchal clustering. Hierarchal clustering cannot handle big data in a better way.\nTime complexity of K-means is O(n) (Linear). Time complexity of hierarchal clustering is O(n2)(Clustering).",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "22) What do you understand by Ensemble Learning?",
        "answer": "In machine learning, Ensemble learning is a process of combining several diverse base models in order to produce one better predictive model. By combining all the predictions, ensemble learning improves the stability of the model.\nThe concept of ensemble learning is that various weak learners come together to make a strong learner. Ensemble methods help in reducing the variance, and bias error which causes a difference in actual value and predicted value. Ensemble learning can also be used for selecting optimal features, data fusion, error correction, incremental learning, etc.\nBelow are the two popular ensemble learning techniques:\nBagging:\nBootstrap Aggregation is called Bagging, which is a powerful method of ensemble. Bagging is an application of Bootstrap technique to create a high-variance machine learning algorithm, such as decision trees. It takes the various sampled datasets from the original datasets and trains each dataset to increase the model variance. The bagging concept can be easily understood by the below diagram:\n\nBoosting:\nBoosting is sequential ensemble method of machine learning. It helps to exploit the dependencies between the models, and mainly reduces the bias and variance in machine learning algorithms. It is an iterative technique that adjust the weight of the instances in a dataset based on the previous classification. If the instance was classified incorrectly, then it increases the weight of that instance. In short, it converts the weak learner to strong learners. Sometimes boosting shows better accuracy than bagging, but it may also introduce the over-fitting in the training data. The common type of boosting is Adaboost.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "23) Explain Box Cox transformation?",
        "answer": "A Box-Cox transformation is a statistical technique to transform the non-normal dependent variable into a normal shape. We usually need normally distributed data to use in various statistical analysis tools such as control charts, Cp/Cpk analysis, and analysis of variance. If the data is not normally distributed, we need to determine the cause for non-normality and need to take the required actions to make the data normal. So for making data normal and transforming non-normal dependent variable into a normal shape, box cox transformation technique is used.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "24) What is the aim of A/B testing?",
        "answer": "A/B testing is a way of comparing two versions of a webpage to determine which webpage version is performing better than other. It is a statistical hypothesis testing which determines any changes to a webpage in order to increase the outcome of strategy.",
        "reference": "javatpoint.com",
        "role": "data-science"
    },
    {
        "question": "25) How is Data Science different from Data Analytics?",
        "answer": "When we deal with data science, there are various other terms also which can be used as data science. Data Analytics is one of those terms. The data science and data analytics both deal with the data, but the difference is how they deal with it. So to clear the confusion between data science and data analytics, there are some differences given:\nData Science:\nData Science is a broad term which deals with structured, unstructured, and raw data. It includes everything related to data such as data analysis, data preparation, data cleansing, etc.\nData science is not focused on answering particular queries. Instead, it focuses on exploring a massive amount of data, sometimes in an unstructured way.\nData Analytics:\nData analytics is a process of analysis of raw data to draw conclusions and meaningful insights from the data. To draw insights from data, data analytics involves the application of algorithms and mechanical process.\nData analytics basically focus on inference which is a process of deriving conclusions from the observations.\nData Analytics mainly focuses on answering particular queries and also perform better when it is focused.",
        "reference": "javatpoint.com",
        "role": "data-science"
    }
]