[
    {
        "question": "1. Compare Kafka and Flume.",
        "answer": "Criteria Kafka Flume\nData flow Pull Push\nHadoop integration Loose Tight\nFunctionality A Publish–Subscribe model messaging system A system for data collection, aggregation, and movement",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "2. What are the elements of Kafka?",
        "answer": "The most important elements of Kafka are as follows:\nTopic: It is a bunch of similar kinds of messages.\nProducer: Using this, one can issue communications to the topic.\nConsumer: It endures to a variety of topics and takes data from brokers.\nBroker: This is the place where the issued messages are stored.\nGet a detailed understanding of Kafka from this comprehensive Kafka Tutorial!",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "3. What role does ZooKeeper play in a cluster of Kafka?",
        "answer": "Apache ZooKeeper acts as a distributed, open-source configuration and synchronization service, along with being a naming registry for distributed applications. It keeps track of the status of the Kafka cluster nodes, as well as of Kafka topics, partitions, etc.\nSince the data is divided across collections of nodes within ZooKeeper, it exhibits high availability and consistency. When a node fails, ZooKeeper performs an instant failover migration.\nZooKeeper is used in Kafka for managing service discovery for Kafka brokers, which form the cluster. ZooKeeper communicates with Kafka when a new broker joins, when a broker dies, when a topic gets removed, or when a topic is added so that each node in the cluster knows about these changes. Thus, it provides an in-sync view of the Kafka cluster configuration.",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "4. What is Kafka?",
        "answer": "Kafka is a message divider project coded in Scala. Kafka was originally developed by LinkedIn as an open-source project in early 2011. The purpose of the project was to achieve the best stand for conducting the real-time statistics nourishment.\nLearn ‘What is Kafka?’ from this insightful blog!",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "5. Why do you think the replications to be dangerous in Kafka?",
        "answer": "Duplication assures that the issued messages available are absorbed in the case of any appliance mistake, plan fault, or recurrent software promotions.\nGet 100% Hike!\nMaster Most in Demand Skills Now !\nBy providing your contact details, you agree to our Terms of Use & Privacy Policy\n\nAdvanced Kafka Interview Questions",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "6. What major role does a Kafka Producer API play?",
        "answer": "It is responsible for covering two producers: kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer. Kafka Producer API mainly provides all producer performance to its clients through a single API.",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "7. Distinguish between Kafka and Flume?",
        "answer": "Flume’s major use case is to gulp down data into Hadoop. Flume is incorporated with Hadoop’s monitoring system, file formats, file system, and utilities such as Morphlines. Along with Flume’s design of sinks, sources, and channels, Flume can help one shift data to other systems lithely. However, the main feature of Hadoop is its Hadoop integration. Flume is the best option to use when we have non-relational data sources or a long file to stream into Hadoop.\nOn the other hand, Kafka’s major use case is a distributed publish–subscribe messaging system. It is not developed specifically for Hadoop, and using Kafka to read and write data to Hadoop is considerably trickier than it is with Flume. Kafka can be used when we particularly need a highly reliable and scalable enterprise messaging system to connect multiple systems like Hadoop.\nFind out how Kafka is used to process real-time JSON Data from this informative blog!",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "8. Describe partitioning key.",
        "answer": "Its role is to specify the target divider of the memo within the producer. Usually, a hash-oriented divider concludes the divider ID according to the given factors. Consumers also use tailored partitions.",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "9. Inside the manufacturer, when does the QueueFullException emerge?",
        "answer": "QueueFullException naturally happens when the manufacturer tries to propel communications at a speed which a broker can’t grip. Consumers need to insert sufficient brokers to collectively grip the amplified load since the producer doesn’t block.",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "10. Can Kafka be utilized without ZooKeeper?",
        "answer": "It is impossible to use Kafka without ZooKeeper because it is not feasible to go around ZooKeeper and attach it in a straight line with the server. If ZooKeeper is down for a number of causes, then we will not be able to serve customers’ demands.",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "11. Elaborate the architecture of Kafka.",
        "answer": "In Kafka, a cluster contains multiple brokers since it is a distributed system. Topic in the system will get divided into multiple partitions, and each broker stores one or more of those partitions so that multiple producers and consumers can publish and retrieve messages at the same time.",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "12. How to start a Kafka server?",
        "answer": "Given that Kafka exercises ZooKeeper, we can start the ZooKeeper’s server. One can use the convince script packaged with Kafka to get a crude but effective single-node ZooKeeper instance:\nbin/zookeeper-server-start.shconfig/zookeeper.properties\nNow the Kafka server can start:\nbin/Kafka-server-start.shconfig/server.properties",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "13. What are consumers or users?",
        "answer": "Kafka provides single-consumer abstractions that discover both queuing and publish–subscribe consumer group. Kafka tags itself with a user group, and every communication available on a topic is distributed to one user case within every promising user group. User instances are in the disconnected process. We can determine the messaging model of the consumer based on the consumer groups.\nIf all consumer instances have the same consumer set, then this works like a conventional queue adjusting load over the consumers.\nIf all customer instances have dissimilar consumer groups, then this works like a publish–subscribe system, and all messages are transmitted to all the consumers.",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "14. Describe an Offset.",
        "answer": "The messages in partitions will be given a sequential ID known as an offset, and the offset will be used to identify each message in the partition uniquely. With the aid of ZooKeeper, Kafka stores the offsets of messages used for a specific topic and partition by a consumer group.",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "15. What do you know about a partition key?",
        "answer": "A partition key is used to point to the aimed division of communication in Kafka producer. Usually, a hash-oriented divider concludes the division ID with the input, and also people use modified divisions.",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "Watch this Kafka Tutorial For Beginners",
        "answer": "",
        "reference": "intellipaat.com",
        "role": "kafka"
    },
    {
        "question": "1. What does it mean if a replica is not an In-Sync Replica for a long time?",
        "answer": "A replica that has been out of ISR for a long period of time indicates that the follower is unable to fetch data at the same rate as the leader.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "2. What are the traditional methods of message transfer? How is Kafka better from them?",
        "answer": "Following are the traditional methods of message transfer:- Message Queuing:- \nA point-to-point technique is used in the message queuing pattern. A message in the queue will be destroyed once it has been consumed, similar to how a message is removed from the server once it has been delivered in the Post Office Protocol. Asynchronous messaging is possible with these queues.\nIf a network problem delays a message's delivery, such as if a consumer is unavailable, the message will be held in the queue until it can be sent. This means that messages aren't always sent in the same order. Instead, they are given on a first-come, first-served basis, which can improve efficiency in some situations. Message Queuing:- \nA point-to-point technique is used in the message queuing pattern. A message in the queue will be destroyed once it has been consumed, similar to how a message is removed from the server once it has been delivered in the Post Office Protocol. Asynchronous messaging is possible with these queues.\nIf a network problem delays a message's delivery, such as if a consumer is unavailable, the message will be held in the queue until it can be sent. This means that messages aren't always sent in the same order. Instead, they are given on a first-come, first-served basis, which can improve efficiency in some situations. Message Queuing:-     Publisher - Subscriber Model:- \nThe publish-subscribe pattern entails publishers producing (\"publishing\") messages in multiple categories and subscribers consuming published messages from the various categories to which they are subscribed. Unlike point-to-point texting, a message is only removed once it has been consumed by all category subscribers.\nKafka caters to a single consumer abstraction that encompasses both of the aforementioned- the consumer group. Following are the benefits of using Kafka over the traditional messaging transfer techniques:\nScalable: A cluster of devices is used to partition and streamline the data thereby, scaling up the storage capacity.\nFaster: Thousands of clients can be served by a single Kafka broker as it can manage megabytes of reads and writes per second.\nDurability and Fault-Tolerant: The data is kept persistent and tolerant to any hardware failures by copying the data in the clusters. Publisher - Subscriber Model:- \nThe publish-subscribe pattern entails publishers producing (\"publishing\") messages in multiple categories and subscribers consuming published messages from the various categories to which they are subscribed. Unlike point-to-point texting, a message is only removed once it has been consumed by all category subscribers.\nKafka caters to a single consumer abstraction that encompasses both of the aforementioned- the consumer group. Following are the benefits of using Kafka over the traditional messaging transfer techniques:\nScalable: A cluster of devices is used to partition and streamline the data thereby, scaling up the storage capacity.\nFaster: Thousands of clients can be served by a single Kafka broker as it can manage megabytes of reads and writes per second.\nDurability and Fault-Tolerant: The data is kept persistent and tolerant to any hardware failures by copying the data in the clusters. Publisher - Subscriber Model:-   Scalable: A cluster of devices is used to partition and streamline the data thereby, scaling up the storage capacity.\nFaster: Thousands of clients can be served by a single Kafka broker as it can manage megabytes of reads and writes per second.\nDurability and Fault-Tolerant: The data is kept persistent and tolerant to any hardware failures by copying the data in the clusters. Scalable: A cluster of devices is used to partition and streamline the data thereby, scaling up the storage capacity. Scalable: Faster: Thousands of clients can be served by a single Kafka broker as it can manage megabytes of reads and writes per second. Faster: Durability and Fault-Tolerant: The data is kept persistent and tolerant to any hardware failures by copying the data in the clusters. Durability and Fault-Tolerant:  ",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "3. What are the major components of Kafka?",
        "answer": "Following are the major components of Kafka:-   Topic:\nA Topic is a category or feed in which records are saved and published.\nTopics are used to organize all of Kafka's records. Consumer apps read data from topics, whereas producer applications write data to them. Records published to the cluster remain in the cluster for the duration of a configurable retention period.\nKafka keeps records in the log, and it's up to the consumers to keep track of where they are in the log (the \"offset\"). As messages are read, a consumer typically advances the offset in a linear fashion. The consumer, on the other hand, is in charge of the position, as he or she can consume messages in any order. When reprocessing records, for example, a consumer can reset to an older offset.\nProducer:\nA Kafka producer is a data source for one or more Kafka topics that optimizes, writes, and publishes messages. Partitioning allows Kafka producers to serialize, compress, and load balance data among brokers.\nConsumer:\nData is read by consumers by reading messages from topics to which they have subscribed. Consumers will be divided into groups. Each consumer in a consumer group will be responsible for reading a subset of the partitions of each subject to which they have subscribed.\nBroker:\nA Kafka broker is a server that works as part of a Kafka cluster (in other words, a Kafka cluster is made up of a number of brokers). Multiple brokers typically work together to build a Kafka cluster, which provides load balancing, reliable redundancy, and failover. The cluster is managed and coordinated by brokers using Apache ZooKeeper. Without sacrificing performance, each broker instance can handle read and write volumes of hundreds of thousands per second (and gigabytes of messages). Each broker has its own ID and can be in charge of one or more topic log divisions.\nZooKeeper is also used by Kafka brokers for leader elections, in which a broker is chosen to lead the handling of client requests for a certain partition of a topic. Connecting to any broker will bring a client up to speed with the entire Kafka cluster. A minimum of three brokers should be used to achieve reliable failover; the higher the number of brokers, the more reliable the failover. Topic:\nA Topic is a category or feed in which records are saved and published.\nTopics are used to organize all of Kafka's records. Consumer apps read data from topics, whereas producer applications write data to them. Records published to the cluster remain in the cluster for the duration of a configurable retention period.\nKafka keeps records in the log, and it's up to the consumers to keep track of where they are in the log (the \"offset\"). As messages are read, a consumer typically advances the offset in a linear fashion. The consumer, on the other hand, is in charge of the position, as he or she can consume messages in any order. When reprocessing records, for example, a consumer can reset to an older offset. Topic: A Topic is a category or feed in which records are saved and published.\nTopics are used to organize all of Kafka's records. Consumer apps read data from topics, whereas producer applications write data to them. Records published to the cluster remain in the cluster for the duration of a configurable retention period.\nKafka keeps records in the log, and it's up to the consumers to keep track of where they are in the log (the \"offset\"). As messages are read, a consumer typically advances the offset in a linear fashion. The consumer, on the other hand, is in charge of the position, as he or she can consume messages in any order. When reprocessing records, for example, a consumer can reset to an older offset. A Topic is a category or feed in which records are saved and published. Topics are used to organize all of Kafka's records. Consumer apps read data from topics, whereas producer applications write data to them. Records published to the cluster remain in the cluster for the duration of a configurable retention period. Kafka keeps records in the log, and it's up to the consumers to keep track of where they are in the log (the \"offset\"). As messages are read, a consumer typically advances the offset in a linear fashion. The consumer, on the other hand, is in charge of the position, as he or she can consume messages in any order. When reprocessing records, for example, a consumer can reset to an older offset. Producer:\nA Kafka producer is a data source for one or more Kafka topics that optimizes, writes, and publishes messages. Partitioning allows Kafka producers to serialize, compress, and load balance data among brokers. Producer: A Kafka producer is a data source for one or more Kafka topics that optimizes, writes, and publishes messages. Partitioning allows Kafka producers to serialize, compress, and load balance data among brokers. A Kafka producer is a data source for one or more Kafka topics that optimizes, writes, and publishes messages. Partitioning allows Kafka producers to serialize, compress, and load balance data among brokers. Consumer:\nData is read by consumers by reading messages from topics to which they have subscribed. Consumers will be divided into groups. Each consumer in a consumer group will be responsible for reading a subset of the partitions of each subject to which they have subscribed. Consumer: Data is read by consumers by reading messages from topics to which they have subscribed. Consumers will be divided into groups. Each consumer in a consumer group will be responsible for reading a subset of the partitions of each subject to which they have subscribed. Data is read by consumers by reading messages from topics to which they have subscribed. Consumers will be divided into groups. Each consumer in a consumer group will be responsible for reading a subset of the partitions of each subject to which they have subscribed. Broker:\nA Kafka broker is a server that works as part of a Kafka cluster (in other words, a Kafka cluster is made up of a number of brokers). Multiple brokers typically work together to build a Kafka cluster, which provides load balancing, reliable redundancy, and failover. The cluster is managed and coordinated by brokers using Apache ZooKeeper. Without sacrificing performance, each broker instance can handle read and write volumes of hundreds of thousands per second (and gigabytes of messages). Each broker has its own ID and can be in charge of one or more topic log divisions.\nZooKeeper is also used by Kafka brokers for leader elections, in which a broker is chosen to lead the handling of client requests for a certain partition of a topic. Connecting to any broker will bring a client up to speed with the entire Kafka cluster. A minimum of three brokers should be used to achieve reliable failover; the higher the number of brokers, the more reliable the failover. Broker: A Kafka broker is a server that works as part of a Kafka cluster (in other words, a Kafka cluster is made up of a number of brokers). Multiple brokers typically work together to build a Kafka cluster, which provides load balancing, reliable redundancy, and failover. The cluster is managed and coordinated by brokers using Apache ZooKeeper. Without sacrificing performance, each broker instance can handle read and write volumes of hundreds of thousands per second (and gigabytes of messages). Each broker has its own ID and can be in charge of one or more topic log divisions.\nZooKeeper is also used by Kafka brokers for leader elections, in which a broker is chosen to lead the handling of client requests for a certain partition of a topic. Connecting to any broker will bring a client up to speed with the entire Kafka cluster. A minimum of three brokers should be used to achieve reliable failover; the higher the number of brokers, the more reliable the failover. A Kafka broker is a server that works as part of a Kafka cluster (in other words, a Kafka cluster is made up of a number of brokers). Multiple brokers typically work together to build a Kafka cluster, which provides load balancing, reliable redundancy, and failover. The cluster is managed and coordinated by brokers using Apache ZooKeeper. Without sacrificing performance, each broker instance can handle read and write volumes of hundreds of thousands per second (and gigabytes of messages). Each broker has its own ID and can be in charge of one or more topic log divisions. ZooKeeper is also used by Kafka brokers for leader elections, in which a broker is chosen to lead the handling of client requests for a certain partition of a topic. Connecting to any broker will bring a client up to speed with the entire Kafka cluster. A minimum of three brokers should be used to achieve reliable failover; the higher the number of brokers, the more reliable the failover.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "4. Explain the four core API architecture that Kafka uses.",
        "answer": "Following are the four core APIs that Kafka uses:   Producer API:\nThe Producer API in Kafka allows an application to publish a stream of records to one or more Kafka topics.\nConsumer API:\nAn application can subscribe to one or more Kafka topics using the Kafka Consumer API. It also enables the application to process streams of records generated in relation to such topics.\nStreams API:\nThe Kafka Streams API allows an application to use a stream processing architecture to process data in Kafka. An application can use this API to take input streams from one or more topics, process them using streams operations, and generate output streams to transmit to one or more topics. The Streams API allows you to convert input streams into output streams in this manner.\nConnect API:\nThe Kafka Connector API connects Kafka topics to applications. This opens up possibilities for constructing and managing the operations of producers and consumers, as well as establishing reusable links between these solutions. A connector, for example, may capture all database updates and ensure that they are made available in a Kafka topic. Producer API:\nThe Producer API in Kafka allows an application to publish a stream of records to one or more Kafka topics. Producer API:  Consumer API:\nAn application can subscribe to one or more Kafka topics using the Kafka Consumer API. It also enables the application to process streams of records generated in relation to such topics. Consumer API:  Streams API:\nThe Kafka Streams API allows an application to use a stream processing architecture to process data in Kafka. An application can use this API to take input streams from one or more topics, process them using streams operations, and generate output streams to transmit to one or more topics. The Streams API allows you to convert input streams into output streams in this manner. Streams API:  Connect API:\nThe Kafka Connector API connects Kafka topics to applications. This opens up possibilities for constructing and managing the operations of producers and consumers, as well as establishing reusable links between these solutions. A connector, for example, may capture all database updates and ensure that they are made available in a Kafka topic. Connect API: ",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "5. What do you mean by a Partition in Kafka?",
        "answer": "Kafka topics are separated into partitions, each of which contains records in a fixed order. A unique offset is assigned and attributed to each record in a partition. Multiple partition logs can be found in a single topic. This allows several users to read from the same topic at the same time. Topics can be parallelized via partitions, which split data into a single topic among numerous brokers. Replication in Kafka is done at the partition level. A replica is the redundant element of a topic partition. Each partition often contains one or more replicas, which means that partitions contain messages that are duplicated across many Kafka brokers in the cluster. One server serves as the leader of each partition (replica), while the others function as followers. The leader replica is in charge of all read-write requests for the partition, while the followers replicate the leader. If the lead server goes down, one of the followers takes over as the leader. To disperse the burden, we should aim for a good balance of leaders, with each broker leading an equal number of partitions.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "6. What do you mean by zookeeper in Kafka and what are its uses?",
        "answer": "Apache ZooKeeper is a naming registry for distributed applications as well as a distributed, open-source configuration and synchronization service. It keeps track of the Kafka cluster nodes' status, as well as Kafka topics, partitions, and so on. ZooKeeper is used by Kafka brokers to maintain and coordinate the Kafka cluster. When the topology of the Kafka cluster changes, such as when brokers and topics are added or removed, ZooKeeper notifies all nodes. When a new broker enters the cluster, for example, ZooKeeper notifies the cluster, as well as when a broker fails. ZooKeeper also allows brokers and topic partition pairs to elect leaders, allowing them to select which broker will be the leader for a given partition (and server read and write operations from producers and consumers), as well as which brokers contain clones of the same data. When the cluster of brokers receives a notification from ZooKeeper, they immediately begin to coordinate with one another and elect any new partition leaders that are required. This safeguards against the unexpected absence of a broker.  ",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "7. Can we use Kafka without Zookeeper?",
        "answer": "Kafka can now be used without ZooKeeper as of version 2.8. The release of Kafka 2.8.0 in April 2021 gave us all the opportunity to try it out without ZooKeeper. However, this version is not yet ready for production and lacks some key features.\nIn the previous versions, bypassing Zookeeper and connecting directly to the Kafka broker was not possible. This is because when the Zookeeper is down, it is unable to fulfill client requests. Kafka can now be used without ZooKeeper as of version 2.8. The release of Kafka 2.8.0 in April 2021 gave us all the opportunity to try it out without ZooKeeper. However, this version is not yet ready for production and lacks some key features. In the previous versions, bypassing Zookeeper and connecting directly to the Kafka broker was not possible. This is because when the Zookeeper is down, it is unable to fulfill client requests.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "8. Explain the concept of Leader and Follower in Kafka.",
        "answer": "In Kafka, each partition has one server that acts as a Leader and one or more servers that operate as Followers. The Leader is in charge of all read and writes requests for the partition, while the Followers are responsible for passively replicating the leader. In the case that the Leader fails, one of the Followers will assume leadership. The server's load is balanced as a result of this.  ",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "9. Why is Topic Replication important in Kafka? What do you mean by ISR in Kafka?",
        "answer": "  Topic replication is critical for constructing Kafka deployments that are both durable and highly available. When one broker fails, topic replicas on other brokers remain available to ensure that data is not lost and that the Kafka deployment is not disrupted. The replication factor specifies the number of copies of a topic that are kept across the Kafka cluster. It takes place at the partition level and is defined at the subject level. A replication factor of two, for example, will keep two copies of a topic for each partition. Each partition has an elected leader, and other brokers store a copy that can be used if necessary. Logically, the replication factor cannot be more than the cluster's total number of brokers. An In-Sync Replica (ISR) is a replica that is up to date with the partition's leader.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "10. What do you understand about a consumer group in Kafka?",
        "answer": "  A consumer group in Kafka is a collection of consumers who work together to ingest data from the same topic or range of topics. The name of an application is essentially represented by a consumer group. Consumers in Kafka often fall into one of several categories. The ‘-group' command must be used to consume messages from a consumer group.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "11. What is the maximum size of a message that Kafka can receive?",
        "answer": "By default, the maximum size of a Kafka message is 1MB (megabyte). The broker settings allow you to modify the size. Kafka, on the other hand, is designed to handle 1KB messages as well. 1MB",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "12. What are some of the features of Kafka?",
        "answer": "Following are the key features of Kafka:-   Kafka is a messaging system built for high throughput and fault tolerance.\nKafka has a built-in patriation system known as a Topic.\nKafka Includes a replication feature as well.\nKafka provides a queue that can handle large amounts of data and move messages from one sender to another.\nKafka can also save the messages to storage and replicate them across the cluster.\nFor coordination and synchronization with other services, Kafka collaborates with Zookeeper.\nApache Spark is well supported by Kafka. Kafka is a messaging system built for high throughput and fault tolerance. Kafka has a built-in patriation system known as a Topic. Kafka Includes a replication feature as well. Kafka provides a queue that can handle large amounts of data and move messages from one sender to another. Kafka can also save the messages to storage and replicate them across the cluster. For coordination and synchronization with other services, Kafka collaborates with Zookeeper. Apache Spark is well supported by Kafka.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "13. How do you start a Kafka server?",
        "answer": "Firstly, we extract Kafka once we have downloaded the most recent version. We must make sure that our local environment has Java 8+ installed in order to run Kafka. The following commands must be done in order to start the Kafka server and ensure that all services are started in the correct order: Start the ZooKeeper service by doing the following: Start the ZooKeeper service by doing the following: $bin/zookeeper-server-start.sh config/zookeeper.properties $bin/zookeeper-server-start.sh config/zookeeper.properties To start the Kafka broker service, open a new terminal and type the following commands: To start the Kafka broker service, open a new terminal and type the following commands: $ bin/kafka-server-start.sh config/server.properties $ bin/kafka-server-start.sh config/server.properties",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "14. What do you mean by geo-replication in Kafka?",
        "answer": "Geo-Replication is a Kafka feature that allows messages in one cluster to be copied across many data centers or cloud regions. Geo-replication entails replicating all of the files and storing them throughout the globe if necessary. Geo-replication can be accomplished with Kafka's MirrorMaker Tool. Geo-replication is a technique for ensuring data backup.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "15. What are some of the disadvantages of Kafka?",
        "answer": "Following are the disadvantages of Kafka : Kafka performance degrades if there is message tweaking. When the message does not need to be updated, Kafka works well.\nWildcard topic selection is not supported by Kafka. It is necessary to match the exact topic name.\nBrokers and consumers reduce Kafka's performance when dealing with huge messages by compressing and decompressing the messages. This has an impact on Kafka's throughput and performance.\nCertain message paradigms, including point-to-point queues and request/reply, are not supported by Kafka.\nKafka does not have a complete set of monitoring tools. Kafka performance degrades if there is message tweaking. When the message does not need to be updated, Kafka works well. Wildcard topic selection is not supported by Kafka. It is necessary to match the exact topic name. Brokers and consumers reduce Kafka's performance when dealing with huge messages by compressing and decompressing the messages. This has an impact on Kafka's throughput and performance. Certain message paradigms, including point-to-point queues and request/reply, are not supported by Kafka. Kafka does not have a complete set of monitoring tools.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "16. Tell me about some of the real-world usages of Apache Kafka.",
        "answer": "Following are some of the real-world usages of Apache Kafka:   As a Message Broker: Due to its high throughput value, Kafka is capable of managing a huge amount of comparable types of messages or data. Kafka can be used as a publish-subscribe messaging system that allows data to be read and published in a convenient manner.\nTo Monitor operational data: Kafka can be used to keep track of metrics related to certain technologies, such as security logs.\nWebsite activity tracking: Kafka can be used to check that data is transferred and received successfully by websites. Kafka can handle the massive amounts of data created by websites for each page and for the activities of users.\nData logging: Kafka's data replication between nodes functionality can be used to restore data on nodes that have failed. Kafka may also be used to collect data from a variety of logs and make it available to consumers.\nStream Processing with Kafka: Kafka may be used to handle streaming data, which is data that is read from one topic, processed, and then written to another. Users and applications will have access to a new topic containing the processed data. As a Message Broker: Due to its high throughput value, Kafka is capable of managing a huge amount of comparable types of messages or data. Kafka can be used as a publish-subscribe messaging system that allows data to be read and published in a convenient manner. As a Message Broker: To Monitor operational data: Kafka can be used to keep track of metrics related to certain technologies, such as security logs. To Monitor operational data: Website activity tracking: Kafka can be used to check that data is transferred and received successfully by websites. Kafka can handle the massive amounts of data created by websites for each page and for the activities of users. Website activity tracking: Data logging: Kafka's data replication between nodes functionality can be used to restore data on nodes that have failed. Kafka may also be used to collect data from a variety of logs and make it available to consumers. Data logging: Stream Processing with Kafka: Kafka may be used to handle streaming data, which is data that is read from one topic, processed, and then written to another. Users and applications will have access to a new topic containing the processed data. Stream Processing with Kafka:",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "17. What are the use cases of Kafka monitoring?",
        "answer": "Following are the use cases of Kafka monitoring : Track System Resource Consumption: It can be used to keep track of system resources such as memory, CPU, and disk utilization over time.\nMonitor threads and JVM usage: Kafka relies on the Java garbage collector to free up memory, ensuring that it runs frequently thereby guaranteeing that the Kafka cluster is more active.\nKeep an eye on the broker, controller, and replication statistics so that the statuses of partitions and replicas can be modified as needed.\nFinding out which applications are causing excessive demand and identifying performance bottlenecks might help solve performance issues rapidly. Track System Resource Consumption: It can be used to keep track of system resources such as memory, CPU, and disk utilization over time. Track System Resource Consumption Monitor threads and JVM usage: Kafka relies on the Java garbage collector to free up memory, ensuring that it runs frequently thereby guaranteeing that the Kafka cluster is more active. Monitor threads and JVM usage Keep an eye on the broker, controller, and replication statistics so that the statuses of partitions and replicas can be modified as needed. Finding out which applications are causing excessive demand and identifying performance bottlenecks might help solve performance issues rapidly.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "18. What do you mean by Kafka schema registry?",
        "answer": "  A Schema Registry is present for both producers and consumers in a Kafka cluster, and it holds Avro schemas. For easy serialization and de-serialization, Avro schemas enable the configuration of compatibility parameters between producers and consumers. The Kafka Schema Registry is used to ensure that the schema used by the consumer and the schema used by the producer are identical. The producers just need to submit the schema ID and not the whole schema when using the Confluent schema registry in Kafka. The consumer looks up the matching schema in the Schema Registry using the schema ID.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "19. What are the benefits of using clusters in Kafka?",
        "answer": "Kafka cluster is basically a group of multiple brokers. They are used to maintain load balance. Because Kafka brokers are stateless, they rely on Zookeeper to keep track of their cluster state. A single Kafka broker instance can manage hundreds of thousands of reads and writes per second, and each broker can handle TBs of messages without compromising performance. Zookeeper can be used to choose the Kafka broker leader. Thus having a cluster of Kafka brokers heavily increases the performance.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "20. Describe partitioning key in Kafka.",
        "answer": "In Kafka terminology, messages are referred to as records. Each record has a key and a value, with the key being optional. For record partitioning, the record's key is used. There will be one or more partitions for each topic. Partitioning is a straightforward data structure. It's the append-only sequence of records, which is arranged chronologically by the time they were attached. Once a record is written to a partition, it is given an offset – a sequential id that reflects the record's position in the partition and uniquely identifies it inside it. Partitioning is done using the record's key. By default, Kafka producer uses the record's key to determine which partition the record should be written to. The producer will always choose the same partition for two records with the same key.   This is important because we may have to deliver records to customers in the same order that they were made. You want these events to come in the order they were created when a consumer purchases an eBook from your webshop and subsequently cancels the transaction. If you receive a cancellation event before a buy event, the cancellation will be rejected as invalid (since the purchase has not yet been registered in the system), and the system will then record the purchase and send the product to the client (and lose you money). You might use a customer id as the key of these Kafka records to solve this problem and assure ordering. This will ensure that all of a customer's purchase events are grouped together in the same partition.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "21. What is the purpose of partitions in Kafka?",
        "answer": "Partitions allow a single topic to be partitioned across numerous servers from the perspective of the Kafka broker. This allows you to store more data in a single topic than a single server can. If you have three brokers and need to store 10TB of data in a topic, one option is to construct a topic with only one partition and store all 10TB on one broker. Another alternative is to build a three-partitioned topic and distribute 10 TB of data among all brokers. A partition is a unit of parallelism from the consumer's perspective.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "1. Tell me about some of the use cases where Kafka is not suitable.",
        "answer": "Following are some of the use cases where Kafka is not suitable : Kafka is designed to manage large amounts of data. Traditional messaging systems would be more appropriate if only a small number of messages need to be processed every day.\nAlthough Kafka includes a streaming API, it is insufficient for executing data transformations. For ETL (extract, transform, load) jobs, Kafka should be avoided.\nThere are superior options, such as RabbitMQ, for scenarios when a simple task queue is required.\nIf long-term storage is necessary, Kafka is not a good choice. It simply allows you to save data for a specific retention period and no longer. Kafka is designed to manage large amounts of data. Traditional messaging systems would be more appropriate if only a small number of messages need to be processed every day. Although Kafka includes a streaming API, it is insufficient for executing data transformations. For ETL (extract, transform, load) jobs, Kafka should be avoided. There are superior options, such as RabbitMQ, for scenarios when a simple task queue is required. If long-term storage is necessary, Kafka is not a good choice. It simply allows you to save data for a specific retention period and no longer.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "2. What is a Replication Tool in Kafka? Explain some of the replication tools available in Kafka.",
        "answer": "The Kafka Replication Tool is used to create a high-level design for the replica maintenance process. The following are some of the replication tools available: Preferred Replica Leader Election Tool: Partitions are spread to many brokers in a cluster, each copy known as a replica, using the Preferred Replica Leader Election Tool. The leader is frequently referred to as the favored replica. The brokers normally spread the leader position equitably across the cluster for various partitions, but owing to failures, planned shutdowns, and other factors, an imbalance can develop over time. This tool can be used to preserve the balance in these situations by reassigning the preferred replicas, and hence the leaders.\nTopics tool: The Kafka topics tool is in charge of all administration operations relating to topics, including:\nListing and describing the topics.\nTopic generation.\nModifying Topics.\nAdding a topic's dividers.\nDisposing of topics.\nTool to reassign partitions: The replicas assigned to a partition can be changed with this tool. This refers to adding or removing followers from a partition.\nStateChangeLogMerger tool: The StateChangeLogMerger tool collects data from brokers in a cluster, formats it into a central log, and aids in the troubleshooting of state change issues. Sometimes there are issues with the election of a leader for a particular partition. This tool can be used to figure out what's causing the issue.\nChange topic configuration tool: used to create new configuration choices, modify current configuration options, and delete configuration options. Preferred Replica Leader Election Tool: Partitions are spread to many brokers in a cluster, each copy known as a replica, using the Preferred Replica Leader Election Tool. The leader is frequently referred to as the favored replica. The brokers normally spread the leader position equitably across the cluster for various partitions, but owing to failures, planned shutdowns, and other factors, an imbalance can develop over time. This tool can be used to preserve the balance in these situations by reassigning the preferred replicas, and hence the leaders. Preferred Replica Leader Election Tool: Topics tool: The Kafka topics tool is in charge of all administration operations relating to topics, including:\nListing and describing the topics.\nTopic generation.\nModifying Topics.\nAdding a topic's dividers.\nDisposing of topics. Topics tool: Listing and describing the topics.\nTopic generation.\nModifying Topics.\nAdding a topic's dividers.\nDisposing of topics. Listing and describing the topics. Topic generation. Modifying Topics. Adding a topic's dividers. Disposing of topics. Tool to reassign partitions: The replicas assigned to a partition can be changed with this tool. This refers to adding or removing followers from a partition. Tool to reassign partitions: StateChangeLogMerger tool: The StateChangeLogMerger tool collects data from brokers in a cluster, formats it into a central log, and aids in the troubleshooting of state change issues. Sometimes there are issues with the election of a leader for a particular partition. This tool can be used to figure out what's causing the issue. StateChangeLogMerger tool: Change topic configuration tool: used to create new configuration choices, modify current configuration options, and delete configuration options. Change topic configuration tool:",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "3. Differentiate between Rabbitmq and Kafka.",
        "answer": "Following are the differences between Kafka and Rabbitmq:   Based on Architecture : Based on Architecture : Based on Architecture : Rabbitmq: Rabbitmq: Rabbitmq is a general-purpose message broker and request/reply, point-to-point, and pub-sub communication patterns are all used by it.\nIt has a smart broker/ dumb consumer model. There is the consistent transmission of messages to consumers at about the same speed as the broker monitors the consumer's status.\nIt is a mature platform and is well supported for Java, client libraries, .NET, Ruby, and Node.js. It offers a variety of plugins as well.\nThe communication can be synchronous or asynchronous. It also provides options for distributed deployment. Rabbitmq is a general-purpose message broker and request/reply, point-to-point, and pub-sub communication patterns are all used by it. It has a smart broker/ dumb consumer model. There is the consistent transmission of messages to consumers at about the same speed as the broker monitors the consumer's status. It is a mature platform and is well supported for Java, client libraries, .NET, Ruby, and Node.js. It offers a variety of plugins as well. The communication can be synchronous or asynchronous. It also provides options for distributed deployment. Kafka: Kafka: Kafka is a message and stream platform for high-volume publish-subscribe messages and streams. It is durable, quick, and scalable.\nIt is a durable message store, similar to a log, and it runs in a server cluster and maintains streams of records in topics (categories).\nIn this, messages are made up of three components: a value, a key, and a timestamp.\nIt has a dumb broker / smart consumer model as it does not track which messages are viewed by customers and only maintains unread messages. Kafka stores all messages for a specific amount of time.\nIn this, external services are required to run, including Apache Zookeeper in some circumstances. Kafka is a message and stream platform for high-volume publish-subscribe messages and streams. It is durable, quick, and scalable. It is a durable message store, similar to a log, and it runs in a server cluster and maintains streams of records in topics (categories). In this, messages are made up of three components: a value, a key, and a timestamp. It has a dumb broker / smart consumer model as it does not track which messages are viewed by customers and only maintains unread messages. Kafka stores all messages for a specific amount of time. In this, external services are required to run, including Apache Zookeeper in some circumstances. Manner of Handling Messages : Manner of Handling Messages : Manner of Handling Messages : Basis Rabbitmq Kafka\nOrdering of messages The ordering of messages is not supported here.  Partitions in Kafka enable message ordering. Message keys are used while sending the messages to the topic.\nLifetime of messages Since Rabbitmq is a message queue, messages are done away with once consumed and the acknowledgement is sent. Since Kafka is a log, the messages are always present there. We can have a message retention policy for the same.\nPrioritizing the messages  In this, priorities can be specified for the messages and the messages can be consumed according to their priority. Prioritising the messages is not possible in  Kafka.\nGuarantee of delivering the messages  Atomicity is not guaranteed in this case, even when the transaction involves a single queue. In Kafka, it is guaranteed that the whole batch of messages in a partition is either sent successfully or failed. Basis Rabbitmq Kafka\nOrdering of messages The ordering of messages is not supported here.  Partitions in Kafka enable message ordering. Message keys are used while sending the messages to the topic.\nLifetime of messages Since Rabbitmq is a message queue, messages are done away with once consumed and the acknowledgement is sent. Since Kafka is a log, the messages are always present there. We can have a message retention policy for the same.\nPrioritizing the messages  In this, priorities can be specified for the messages and the messages can be consumed according to their priority. Prioritising the messages is not possible in  Kafka.\nGuarantee of delivering the messages  Atomicity is not guaranteed in this case, even when the transaction involves a single queue. In Kafka, it is guaranteed that the whole batch of messages in a partition is either sent successfully or failed. Basis Rabbitmq Kafka Basis Rabbitmq Kafka Basis Rabbitmq Kafka Ordering of messages The ordering of messages is not supported here.  Partitions in Kafka enable message ordering. Message keys are used while sending the messages to the topic.\nLifetime of messages Since Rabbitmq is a message queue, messages are done away with once consumed and the acknowledgement is sent. Since Kafka is a log, the messages are always present there. We can have a message retention policy for the same.\nPrioritizing the messages  In this, priorities can be specified for the messages and the messages can be consumed according to their priority. Prioritising the messages is not possible in  Kafka.\nGuarantee of delivering the messages  Atomicity is not guaranteed in this case, even when the transaction involves a single queue. In Kafka, it is guaranteed that the whole batch of messages in a partition is either sent successfully or failed. Ordering of messages The ordering of messages is not supported here.  Partitions in Kafka enable message ordering. Message keys are used while sending the messages to the topic. Ordering of messages Ordering of messages The ordering of messages is not supported here. Partitions in Kafka enable message ordering. Message keys are used while sending the messages to the topic. Lifetime of messages Since Rabbitmq is a message queue, messages are done away with once consumed and the acknowledgement is sent. Since Kafka is a log, the messages are always present there. We can have a message retention policy for the same. Lifetime of messages Lifetime of messages Since Rabbitmq is a message queue, messages are done away with once consumed and the acknowledgement is sent. Since Kafka is a log, the messages are always present there. We can have a message retention policy for the same. Prioritizing the messages  In this, priorities can be specified for the messages and the messages can be consumed according to their priority. Prioritising the messages is not possible in  Kafka. Prioritizing the messages Prioritizing the messages In this, priorities can be specified for the messages and the messages can be consumed according to their priority. Prioritising the messages is not possible in  Kafka. Guarantee of delivering the messages  Atomicity is not guaranteed in this case, even when the transaction involves a single queue. In Kafka, it is guaranteed that the whole batch of messages in a partition is either sent successfully or failed. Guarantee of delivering the messages Guarantee of delivering the messages Atomicity is not guaranteed in this case, even when the transaction involves a single queue. In Kafka, it is guaranteed that the whole batch of messages in a partition is either sent successfully or failed. Based on Approach : Based on Approach : Based on Approach : Kafka: The pull model is used by Kafka. Batches of messages from a given offset are requested by consumers. When there are no messages past the offset, Kafka allows for long-pooling, which eliminates tight loops.\nBecause of Kafka's partitions, a pull model makes sense. In a partition with no competing customers, Kafka provides message orders. This allows users to take advantage of message batching for more efficient message delivery and higher throughput.\nRabbitmq: RabbitMQ operates on a push paradigm, which prevents users from becoming overwhelmed by imposing a prefetch limit on them. This can be used for messaging with low latency. The push model's goal is to distribute messages individually and promptly, ensuring that work is parallelized equitably and messages are handled roughly in the order they came in the queue. Kafka: The pull model is used by Kafka. Batches of messages from a given offset are requested by consumers. When there are no messages past the offset, Kafka allows for long-pooling, which eliminates tight loops.\nBecause of Kafka's partitions, a pull model makes sense. In a partition with no competing customers, Kafka provides message orders. This allows users to take advantage of message batching for more efficient message delivery and higher throughput. Kafka  Rabbitmq: RabbitMQ operates on a push paradigm, which prevents users from becoming overwhelmed by imposing a prefetch limit on them. This can be used for messaging with low latency. The push model's goal is to distribute messages individually and promptly, ensuring that work is parallelized equitably and messages are handled roughly in the order they came in the queue. Rabbitmq Based on Performance: Based on Performance: Based on Performance: Kafka: Compared to message brokers like RabbitMQ, Kafka provides significantly better performance. It boosts performance by using sequential disc I/O, making it a good choice for queue implementation. With limited resources, it can achieve high throughput (millions of messages per second), which is essential for large data use cases.\nRabbitmq: RabbitMQ can also handle a million messages per second, but it does so at the expense of more resources (around 30 nodes). RabbitMQ can be used for many of the same applications as Kafka, however, it must be used in conjunction with other technologies such as Apache Cassandra. Kafka: Compared to message brokers like RabbitMQ, Kafka provides significantly better performance. It boosts performance by using sequential disc I/O, making it a good choice for queue implementation. With limited resources, it can achieve high throughput (millions of messages per second), which is essential for large data use cases. Kafka Rabbitmq: RabbitMQ can also handle a million messages per second, but it does so at the expense of more resources (around 30 nodes). RabbitMQ can be used for many of the same applications as Kafka, however, it must be used in conjunction with other technologies such as Apache Cassandra. Rabbitmq",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "4. What are the parameters that you should look for while optimising kafka for optimal performance?",
        "answer": "Two major measurements are taken into account while tuning for optimal performance: latency measures, which relate to the amount of time it takes to process one event, and throughput measures, which refer to the number of events that can be processed in a given length of time. Most systems are tuned for one of two things: delay or throughput, whereas Kafka can do both. The following stages are involved in optimizing Kafka's performance:   Kafka producer tuning: Data that producers must provide to brokers is kept in a batch. The producer transmits the batch to the broker when it's ready. To adjust the producers for latency and throughput, two parameters must be considered: batch size and linger time. The batch size must be chosen with great care. If the producer is constantly delivering messages, a bigger batch size is recommended to maximize throughput. However, if the batch size is set to a huge value, it may never fill up or take a long time to do so, affecting the latency. The batch size must be selected based on the nature of the volume of messages transmitted by the producer. The linger duration is included to create a delay while more records are added to the batch, allowing for larger records to be transmitted. More messages can be transmitted in one batch with a longer linger period, but latency may suffer as a result. A shorter linger time, on the other hand, will result in fewer messages being transmitted faster, resulting in lower latency but also lower throughput.\nTuning the Kafka broker: Each partition in a topic has a leader, and each leader has 0 or more followers. It's critical that the leaders are appropriately balanced, and that some nodes aren't overworked in comparison to others.\nTuning Kafka Consumers: To ensure that consumers keep up with producers, the number of partitions for a topic should be equal to the number of consumers. The divisions are divided among the consumers in the same consumer group. Kafka producer tuning: Data that producers must provide to brokers is kept in a batch. The producer transmits the batch to the broker when it's ready. To adjust the producers for latency and throughput, two parameters must be considered: batch size and linger time. The batch size must be chosen with great care. If the producer is constantly delivering messages, a bigger batch size is recommended to maximize throughput. However, if the batch size is set to a huge value, it may never fill up or take a long time to do so, affecting the latency. The batch size must be selected based on the nature of the volume of messages transmitted by the producer. The linger duration is included to create a delay while more records are added to the batch, allowing for larger records to be transmitted. More messages can be transmitted in one batch with a longer linger period, but latency may suffer as a result. A shorter linger time, on the other hand, will result in fewer messages being transmitted faster, resulting in lower latency but also lower throughput. Kafka producer tuning: Tuning the Kafka broker: Each partition in a topic has a leader, and each leader has 0 or more followers. It's critical that the leaders are appropriately balanced, and that some nodes aren't overworked in comparison to others. Tuning the Kafka broker: Tuning Kafka Consumers: To ensure that consumers keep up with producers, the number of partitions for a topic should be equal to the number of consumers. The divisions are divided among the consumers in the same consumer group. Tuning Kafka Consumers:",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "5. Differentiate between Redis and Kafka.",
        "answer": "The following table illustrates the differences between Redis and Kafka:   Redis Kafka\nPush-based message delivery is supported by Redis. This means that messages published to Redis will be distributed to consumers automatically. Pull-based message delivery is supported by Kafka. The messages published to the Kafka broker are not automatically sent to the consumers; instead, consumers must pull the messages when they are ready.\nMessage retention is not supported by Redis. The communications are destroyed once they have been delivered to the recipients. In its log, Kafka allows for message preservation.\nParallel processing is not supported by Redis. Multiple consumers in a consumer group can consume partitions of the topic concurrently because of the Kafka's partitioning feature.\nRedis can not manage vast amounts of data because it's an in-memory database. Kafka can handle massive amounts of data since it uses disc space as its primary storage.\nBecause Redis is an in-memory store, it is much faster than Kafka. Because Kafka stores data on disc, it is slower than Redis. Redis Kafka\nPush-based message delivery is supported by Redis. This means that messages published to Redis will be distributed to consumers automatically. Pull-based message delivery is supported by Kafka. The messages published to the Kafka broker are not automatically sent to the consumers; instead, consumers must pull the messages when they are ready.\nMessage retention is not supported by Redis. The communications are destroyed once they have been delivered to the recipients. In its log, Kafka allows for message preservation.\nParallel processing is not supported by Redis. Multiple consumers in a consumer group can consume partitions of the topic concurrently because of the Kafka's partitioning feature.\nRedis can not manage vast amounts of data because it's an in-memory database. Kafka can handle massive amounts of data since it uses disc space as its primary storage.\nBecause Redis is an in-memory store, it is much faster than Kafka. Because Kafka stores data on disc, it is slower than Redis. Redis Kafka Redis Kafka Redis Kafka Push-based message delivery is supported by Redis. This means that messages published to Redis will be distributed to consumers automatically. Pull-based message delivery is supported by Kafka. The messages published to the Kafka broker are not automatically sent to the consumers; instead, consumers must pull the messages when they are ready.\nMessage retention is not supported by Redis. The communications are destroyed once they have been delivered to the recipients. In its log, Kafka allows for message preservation.\nParallel processing is not supported by Redis. Multiple consumers in a consumer group can consume partitions of the topic concurrently because of the Kafka's partitioning feature.\nRedis can not manage vast amounts of data because it's an in-memory database. Kafka can handle massive amounts of data since it uses disc space as its primary storage.\nBecause Redis is an in-memory store, it is much faster than Kafka. Because Kafka stores data on disc, it is slower than Redis. Push-based message delivery is supported by Redis. This means that messages published to Redis will be distributed to consumers automatically. Pull-based message delivery is supported by Kafka. The messages published to the Kafka broker are not automatically sent to the consumers; instead, consumers must pull the messages when they are ready. Push-based message delivery is supported by Redis. This means that messages published to Redis will be distributed to consumers automatically. Pull-based message delivery is supported by Kafka. The messages published to the Kafka broker are not automatically sent to the consumers; instead, consumers must pull the messages when they are ready. Message retention is not supported by Redis. The communications are destroyed once they have been delivered to the recipients. In its log, Kafka allows for message preservation. Message retention is not supported by Redis. The communications are destroyed once they have been delivered to the recipients. In its log, Kafka allows for message preservation. Parallel processing is not supported by Redis. Multiple consumers in a consumer group can consume partitions of the topic concurrently because of the Kafka's partitioning feature. Parallel processing is not supported by Redis. Multiple consumers in a consumer group can consume partitions of the topic concurrently because of the Kafka's partitioning feature. Redis can not manage vast amounts of data because it's an in-memory database. Kafka can handle massive amounts of data since it uses disc space as its primary storage. Redis can not manage vast amounts of data because it's an in-memory database. Kafka can handle massive amounts of data since it uses disc space as its primary storage. Because Redis is an in-memory store, it is much faster than Kafka. Because Kafka stores data on disc, it is slower than Redis. Because Redis is an in-memory store, it is much faster than Kafka. Because Kafka stores data on disc, it is slower than Redis.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "6. Describe in what ways Kafka enforces security.",
        "answer": "The security given by Kafka is made up of three parts:   Encryption: All communications sent between the Kafka broker and its many clients are encrypted. This prevents data from being intercepted by other clients. All messages are shared in an encrypted format between the components.\nAuthentication: Before being able to connect to Kafka, apps that use the Kafka broker must be authenticated. Only approved applications will be able to send or receive messages. To identify themselves, authorized applications will have unique ids and passwords.\nAfter authentication, authorization is carried out. It is possible for a client to publish or consume messages once it has been validated. The permission ensures that write access to apps can be restricted to prevent data contamination. Encryption: All communications sent between the Kafka broker and its many clients are encrypted. This prevents data from being intercepted by other clients. All messages are shared in an encrypted format between the components. Encryption: Authentication: Before being able to connect to Kafka, apps that use the Kafka broker must be authenticated. Only approved applications will be able to send or receive messages. To identify themselves, authorized applications will have unique ids and passwords. Authentication: After authentication, authorization is carried out. It is possible for a client to publish or consume messages once it has been validated. The permission ensures that write access to apps can be restricted to prevent data contamination.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "7. Differentiate between Kafka and Java Messaging Service(JMS).",
        "answer": "The following table illustrates the differences between Kafka and Java Messaging Service:   Java Messaging Service(JMS) Kafka\nThe push model is used to deliver the messages. Consumers receive messages on a regular basis. A pull mechanism is used in the delivery method. When consumers are ready to receive the messages, they pull them.\nWhen the JMS queue receives confirmation from the consumer that the message has been received, it is permanently destroyed. Even after the consumer has viewed the communications, they are maintained for a specified length of time.\nJMS is better suited to multi-node clusters in very complicated systems. Kafka is better suited to handling big amounts of data.\nJMS is a FIFO queue that does not support any other type of ordering. Kafka ensures that partitions are sent in the order in which they appeared in the message. Java Messaging Service(JMS) Kafka\nThe push model is used to deliver the messages. Consumers receive messages on a regular basis. A pull mechanism is used in the delivery method. When consumers are ready to receive the messages, they pull them.\nWhen the JMS queue receives confirmation from the consumer that the message has been received, it is permanently destroyed. Even after the consumer has viewed the communications, they are maintained for a specified length of time.\nJMS is better suited to multi-node clusters in very complicated systems. Kafka is better suited to handling big amounts of data.\nJMS is a FIFO queue that does not support any other type of ordering. Kafka ensures that partitions are sent in the order in which they appeared in the message. Java Messaging Service(JMS) Kafka Java Messaging Service(JMS) Kafka Java Messaging Service(JMS) Kafka The push model is used to deliver the messages. Consumers receive messages on a regular basis. A pull mechanism is used in the delivery method. When consumers are ready to receive the messages, they pull them.\nWhen the JMS queue receives confirmation from the consumer that the message has been received, it is permanently destroyed. Even after the consumer has viewed the communications, they are maintained for a specified length of time.\nJMS is better suited to multi-node clusters in very complicated systems. Kafka is better suited to handling big amounts of data.\nJMS is a FIFO queue that does not support any other type of ordering. Kafka ensures that partitions are sent in the order in which they appeared in the message. The push model is used to deliver the messages. Consumers receive messages on a regular basis. A pull mechanism is used in the delivery method. When consumers are ready to receive the messages, they pull them. The push model is used to deliver the messages. Consumers receive messages on a regular basis. A pull mechanism is used in the delivery method. When consumers are ready to receive the messages, they pull them. When the JMS queue receives confirmation from the consumer that the message has been received, it is permanently destroyed. Even after the consumer has viewed the communications, they are maintained for a specified length of time. When the JMS queue receives confirmation from the consumer that the message has been received, it is permanently destroyed. Even after the consumer has viewed the communications, they are maintained for a specified length of time. JMS is better suited to multi-node clusters in very complicated systems. Kafka is better suited to handling big amounts of data. JMS is better suited to multi-node clusters in very complicated systems. Kafka is better suited to handling big amounts of data. JMS is a FIFO queue that does not support any other type of ordering. Kafka ensures that partitions are sent in the order in which they appeared in the message. JMS is a FIFO queue that does not support any other type of ordering. Kafka ensures that partitions are sent in the order in which they appeared in the message.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "8. What do you understand about Kafka MirrorMaker?",
        "answer": "The MirrorMaker is a standalone utility for copying data from one Apache Kafka cluster to another. The MirrorMaker reads data from original cluster topics and writes it to a destination cluster with the same topic name. The source and destination clusters are separate entities that can have various partition counts and offset values.  ",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "9. Differentiate between Kafka and Flume.",
        "answer": "Apache Flume is a dependable, distributed, and available software for aggregating, collecting, and transporting massive amounts of log data quickly and efficiently. Its architecture is versatile and simple, based on streaming data flows. It's written in the Java programming language. It features its own query processing engine, allowing it to alter each fresh batch of data before sending it to its intended sink. It is designed to be adaptable.   The following table illustrates the differences between Kafka and Flume : Kafka Flume\nKafka is a distributed data system. Apache Flume is a system that is available, dependable, and distributed.\nIt essentially functions as a pull model. It essentially functions as a push model.\nIt is made for absorbing and analysing real-time streaming data. It collects, aggregates, and moves massive amounts of log data from a variety of sources to a centralised data repository in an efficient manner.\nIf it is resilient to node failure, it facilitates automatic recovery. If the flume-agent fails, you will lose events in the channel.\nKafka operates as a cluster that manages incoming high-volume data streams in real-time. Flume is a tool for collecting log data from web servers that are spread.\nIt is a messaging system that is fault-tolerant, efficient, and scalable. It is made specifically for Hadoop.\nIt's simple to scale. In comparison to Kafka, it is not scalable. Kafka Flume\nKafka is a distributed data system. Apache Flume is a system that is available, dependable, and distributed.\nIt essentially functions as a pull model. It essentially functions as a push model.\nIt is made for absorbing and analysing real-time streaming data. It collects, aggregates, and moves massive amounts of log data from a variety of sources to a centralised data repository in an efficient manner.\nIf it is resilient to node failure, it facilitates automatic recovery. If the flume-agent fails, you will lose events in the channel.\nKafka operates as a cluster that manages incoming high-volume data streams in real-time. Flume is a tool for collecting log data from web servers that are spread.\nIt is a messaging system that is fault-tolerant, efficient, and scalable. It is made specifically for Hadoop.\nIt's simple to scale. In comparison to Kafka, it is not scalable. Kafka Flume Kafka Flume Kafka Flume Kafka is a distributed data system. Apache Flume is a system that is available, dependable, and distributed.\nIt essentially functions as a pull model. It essentially functions as a push model.\nIt is made for absorbing and analysing real-time streaming data. It collects, aggregates, and moves massive amounts of log data from a variety of sources to a centralised data repository in an efficient manner.\nIf it is resilient to node failure, it facilitates automatic recovery. If the flume-agent fails, you will lose events in the channel.\nKafka operates as a cluster that manages incoming high-volume data streams in real-time. Flume is a tool for collecting log data from web servers that are spread.\nIt is a messaging system that is fault-tolerant, efficient, and scalable. It is made specifically for Hadoop.\nIt's simple to scale. In comparison to Kafka, it is not scalable. Kafka is a distributed data system. Apache Flume is a system that is available, dependable, and distributed. Kafka is a distributed data system. Apache Flume is a system that is available, dependable, and distributed. It essentially functions as a pull model. It essentially functions as a push model. It essentially functions as a pull model. It essentially functions as a push model. It is made for absorbing and analysing real-time streaming data. It collects, aggregates, and moves massive amounts of log data from a variety of sources to a centralised data repository in an efficient manner. It is made for absorbing and analysing real-time streaming data. It collects, aggregates, and moves massive amounts of log data from a variety of sources to a centralised data repository in an efficient manner. If it is resilient to node failure, it facilitates automatic recovery. If the flume-agent fails, you will lose events in the channel. If it is resilient to node failure, it facilitates automatic recovery. If the flume-agent fails, you will lose events in the channel. Kafka operates as a cluster that manages incoming high-volume data streams in real-time. Flume is a tool for collecting log data from web servers that are spread. Kafka operates as a cluster that manages incoming high-volume data streams in real-time. Flume is a tool for collecting log data from web servers that are spread. It is a messaging system that is fault-tolerant, efficient, and scalable. It is made specifically for Hadoop. It is a messaging system that is fault-tolerant, efficient, and scalable. It is made specifically for Hadoop. It's simple to scale. In comparison to Kafka, it is not scalable. It's simple to scale. In comparison to Kafka, it is not scalable.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "10. What do you mean by confluent kafka? What are its advantages?",
        "answer": "Confluent is an Apache Kafka-based data streaming platform: a full-scale streaming platform capable of not just publish-and-subscribe but also data storage and processing within the stream. Confluent Kafka is a more comprehensive Apache Kafka distribution. It enhances Kafka's integration capabilities by including tools for optimizing and managing Kafka clusters, as well as ways for ensuring the streams' security. Kafka is easy to construct and operate because of the Confluent Platform. Confluent's software comes in three varieties: A free, open-source streaming platform that makes it simple to get started with real-time data streams;\nAn enterprise-grade version with more administration, operations, and monitoring tools;\nA premium cloud-based version. A free, open-source streaming platform that makes it simple to get started with real-time data streams; An enterprise-grade version with more administration, operations, and monitoring tools; A premium cloud-based version. Following are the advantages of Confluent Kafka : It features practically all of Kafka's characteristics, as well as a few extras.\nIt greatly simplifies the administrative operations procedures.\nIt relieves data managers of the burden of thinking about data relaying. It features practically all of Kafka's characteristics, as well as a few extras. It greatly simplifies the administrative operations procedures. It relieves data managers of the burden of thinking about data relaying.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "11. Describe message compression in Kafka. What is the need of message compression in Kafka? Also mention if there are any disadvantages of it.",
        "answer": "Producers transmit data to brokers in JSON format in Kafka. The JSON format stores data in string form, which can result in several duplicate records being stored in the Kafka topic. As a result, the amount of disc space used increases. As a result, before delivering messages to Kafka, compression or delaying of data is performed to save disk space. Because message compression is performed on the producer side, no changes to the consumer or broker setup are required.   It is advantageous because of the following factors: It decreases the latency of messages transmitted to Kafka by reducing their size.\nProducers can send more net messages to the broker with less bandwidth.\nWhen data is saved in Kafka using cloud platforms, it can save money in circumstances where cloud services are paid.\nMessage compression reduces the amount of data stored on disk, allowing for faster read and write operations. It decreases the latency of messages transmitted to Kafka by reducing their size. Producers can send more net messages to the broker with less bandwidth. When data is saved in Kafka using cloud platforms, it can save money in circumstances where cloud services are paid. Message compression reduces the amount of data stored on disk, allowing for faster read and write operations. Message Compression has the following disadvantages : Producers must use some CPU cycles to compress their work.\nDecompression takes up several CPU cycles for consumers.\nCompression and decompression place a higher burden on the CPU. Producers must use some CPU cycles to compress their work. Decompression takes up several CPU cycles for consumers. Compression and decompression place a higher burden on the CPU.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "12. What do you mean by multi-tenancy in Kafka?",
        "answer": "  Multi-tenancy is a software operation mode in which many instances of one or more programs operate in a shared environment independently of one another. The instances are considered to be physically separate yet logically connected. The level of logical isolation in a system that supports multi-tenancy must be comprehensive, but the level of physical integration can vary. Kafka is multi-tenant because it allows for the configuration of many topics for data consumption and production on the same cluster.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "13. What do you understand about log compaction and quotas in Kafka?",
        "answer": "Log compaction is a way through which Kafka assures that for each topic partition, at least the last known value for each message key within the log of data is kept. This allows for the restoration of state following an application crash or a system failure. During any operational maintenance, it allows refreshing caches after an application restarts. Any consumer processing the log from the beginning will be able to see at least the final state of all records in the order in which they were written, because of the log compaction.   A Kafka cluster can apply quotas on producers and fetch requests as of Kafka 0.9. Quotas are byte-rate limits that are set for each client-id. A client-id is a logical identifier for a request-making application. A single client-id can therefore link to numerous producers and client instances. The quota will be applied to them all as a single unit. Quotas prevent a single application from monopolizing broker resources and causing network saturation by consuming extremely large amounts of data.  ",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "14. What are the guarantees that Kafka provides?",
        "answer": "Following are the guarantees that Kafka assures : The messages are displayed in the same order as they were published by the producers. The order of the messages is maintained.\nThe replication factor determines the number of replicas. If the replication factor is n, the Kafka cluster has fault tolerance for up to n-1 servers.\nPer partition, Kafka can provide \"at least one\" delivery semantics. This means that if a partition is given numerous times, Kafka assures that it will reach a customer at least once. The messages are displayed in the same order as they were published by the producers. The order of the messages is maintained. The replication factor determines the number of replicas. If the replication factor is n, the Kafka cluster has fault tolerance for up to n-1 servers. Per partition, Kafka can provide \"at least one\" delivery semantics. This means that if a partition is given numerous times, Kafka assures that it will reach a customer at least once.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "15. What do you mean by an unbalanced cluster in Kafka? How can you balance it?",
        "answer": "It's as simple as assigning a unique broker id, listeners, and log directory to the server.properties file to add new brokers to an existing Kafka cluster. However, these brokers will not be allocated any data partitions from the cluster's existing topics, so they won't be performing much work unless the partitions are moved or new topics are formed. A cluster is referred to as unbalanced if it has any of the following problems : Leader Skew: Leader Skew: Leader Skew:  Consider the following scenario: a topic with three partitions and a replication factor of three across three brokers.   The leader receives all reads and writes on a partition. Followers send fetch requests to the leaders in order to receive their most recent messages. Followers exist solely for redundancy and fail-over purposes. Consider the case of a broker who has failed. It's possible that the failed broker was a collection of numerous leader partitions. Each unsuccessful broker's leader partition is promoted as the leader by its followers on the other brokers. Because fail-over to an out-of-sync replica is not allowed, the follower must be in sync with the leader in order to be promoted as the leader.   If another broker goes down, all of the leaders are on the same broker, therefore there is no redundancy.   When both brokers 1 and 3 go live, the partitions gain some redundancy, but the leaders stay focused on broker 2.   As a result, the Kafka brokers have a leader imbalance. When a node is a leader for more partitions than the number of partitions/number of brokers, the cluster is in a leader skewed condition. Solving the leader skew problem: Solving the leader skew problem: Kafka offers the ability to reassign leaders to the desired replicas in order to tackle this problem. This can be accomplished in one of two ways: The auto.leader.rebalance.enable=true broker option allows the controller node to transfer leadership to the preferred replica leaders, restoring the even distribution.\nWhen Kafka-preferred-replica-election.sh is run, the preferred replica is selected for all partitions: The utility requires a JSON file containing a mandatory list of zookeeper hosts and an optional list of topic partitions. If no list is provided, the utility uses a zookeeper to retrieve all of the cluster's topic partitions. The Kafka-preferred-replica-election.sh utility can be time-consuming to use. Custom scripts can render only the topics and partitions that are required, automating the process across the cluster. The auto.leader.rebalance.enable=true broker option allows the controller node to transfer leadership to the preferred replica leaders, restoring the even distribution. When Kafka-preferred-replica-election.sh is run, the preferred replica is selected for all partitions: The utility requires a JSON file containing a mandatory list of zookeeper hosts and an optional list of topic partitions. If no list is provided, the utility uses a zookeeper to retrieve all of the cluster's topic partitions. The Kafka-preferred-replica-election.sh utility can be time-consuming to use. Custom scripts can render only the topics and partitions that are required, automating the process across the cluster. Broker Skew: Broker Skew: Broker Skew: Let us consider a Kafka cluster with nine brokers. Let the topic name be \"sample_topic.\" The following is how the brokers are assigned to the topic in our example: Broker Id Number of Partitions Partitions Is Skewed?\n0\n3\n(0, 7, 8)\nNo\n1\n4\n(0, 1, 8, 9)\nNo\n2\n5\n(0, 1, 2 , 9, 10)\nNo\n3\n6\n(1, 2, 3, 9, 19, 11)\nYes\n4\n6\n(2, 3, 4, 10, 11, 12)\nYes\n5\n6\n(3, 4, 5, 11, 12, 13)\nYes\n6\n5\n(4, 5, 6, 12, 13)\nNo\n7\n4\n(5, 6, 7, 13)\nNo\n8\n3\n(6, 7, 8)\nNo Broker Id Number of Partitions Partitions Is Skewed?\n0\n3\n(0, 7, 8)\nNo\n1\n4\n(0, 1, 8, 9)\nNo\n2\n5\n(0, 1, 2 , 9, 10)\nNo\n3\n6\n(1, 2, 3, 9, 19, 11)\nYes\n4\n6\n(2, 3, 4, 10, 11, 12)\nYes\n5\n6\n(3, 4, 5, 11, 12, 13)\nYes\n6\n5\n(4, 5, 6, 12, 13)\nNo\n7\n4\n(5, 6, 7, 13)\nNo\n8\n3\n(6, 7, 8)\nNo Broker Id Number of Partitions Partitions Is Skewed? Broker Id Number of Partitions Partitions Is Skewed? Broker Id Number of Partitions Partitions Is Skewed? 0\n3\n(0, 7, 8)\nNo\n1\n4\n(0, 1, 8, 9)\nNo\n2\n5\n(0, 1, 2 , 9, 10)\nNo\n3\n6\n(1, 2, 3, 9, 19, 11)\nYes\n4\n6\n(2, 3, 4, 10, 11, 12)\nYes\n5\n6\n(3, 4, 5, 11, 12, 13)\nYes\n6\n5\n(4, 5, 6, 12, 13)\nNo\n7\n4\n(5, 6, 7, 13)\nNo\n8\n3\n(6, 7, 8)\nNo 0\n3\n(0, 7, 8)\nNo 0 0 3 3 (0, 7, 8) (0, 7, 8) No No 1\n4\n(0, 1, 8, 9)\nNo 1 1 4 4 (0, 1, 8, 9) (0, 1, 8, 9) No No 2\n5\n(0, 1, 2 , 9, 10)\nNo 2 2 5 5 (0, 1, 2 , 9, 10) (0, 1, 2 , 9, 10) No No 3\n6\n(1, 2, 3, 9, 19, 11)\nYes 3 3 6 6 (1, 2, 3, 9, 19, 11) (1, 2, 3, 9, 19, 11) Yes Yes 4\n6\n(2, 3, 4, 10, 11, 12)\nYes 4 4 6 6 (2, 3, 4, 10, 11, 12) (2, 3, 4, 10, 11, 12) Yes Yes 5\n6\n(3, 4, 5, 11, 12, 13)\nYes 5 5 6 6 (3, 4, 5, 11, 12, 13) (3, 4, 5, 11, 12, 13) Yes Yes 6\n5\n(4, 5, 6, 12, 13)\nNo 6 6 5 5 (4, 5, 6, 12, 13) (4, 5, 6, 12, 13) No No 7\n4\n(5, 6, 7, 13)\nNo 7 7 4 4 (5, 6, 7, 13) (5, 6, 7, 13) No No 8\n3\n(6, 7, 8)\nNo 8 8 3 3 (6, 7, 8) (6, 7, 8) No No On brokers 3,4 and 5, the topic “sample_topic” is skewed. This is because if the number of partitions per broker on a given issue is more than the average, the broker is considered to be skewed. Solving the broker skew problem : Solving the broker skew problem : The following steps can be used to solve it: Generate the candidate assignment configuration using the partition reassignment tool (Kafka-reassign-partition.sh) with the –generate option. The current and intended replica allocations are shown here.\nCreate a JSON file with the suggested assignment.\nTo update the metadata for balancing, run the partition reassignment tool.\nRun the “Kafka-preferred-replica-election.sh” tool to complete the balancing after the partition reassignment is complete. Generate the candidate assignment configuration using the partition reassignment tool (Kafka-reassign-partition.sh) with the –generate option. The current and intended replica allocations are shown here. Create a JSON file with the suggested assignment. To update the metadata for balancing, run the partition reassignment tool. Run the “Kafka-preferred-replica-election.sh” tool to complete the balancing after the partition reassignment is complete.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "16. How will you expand a cluster in Kafka?",
        "answer": "To add a server to a Kafka cluster, it only needs to be given a unique broker id and Kafka must be started on that server. However, until a new topic is created, a new server will not be given any of the data partitions. As a result, when a new machine is introduced to the cluster, some existing data must be migrated to these new machines. To relocate some partitions to the new broker, we use the partition reassignment tool. Kafka will make the new server a follower of the partition it is migrating to, allowing it to replicate the data on that partition completely. When all of the data has been duplicated, the new server can join the ISR, and one of the current replicas will erase the data it has for that partition.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "17. What do you mean by graceful shutdown in Kafka?",
        "answer": "The Apache cluster will automatically identify any broker shutdown or failure. In this instance, new leaders for partitions previously handled by that device will be chosen. This can happen as a result of a server failure or even if it is shut down for maintenance or configuration changes. When a server is taken down on purpose, Kafka provides a graceful method for terminating the server rather than killing it. When a server is switched off: To prevent having to undertake any log recovery when Kafka is restarted, it ensures that all of its logs are synced onto a disk. Because log recovery takes time, purposeful restarts can be sped up.\nPrior to shutting down, all partitions for which the server is the leader will be moved to the replicas. The leadership transfer will be faster as a result, and the period each partition is inaccessible will be decreased to a few milliseconds. To prevent having to undertake any log recovery when Kafka is restarted, it ensures that all of its logs are synced onto a disk. Because log recovery takes time, purposeful restarts can be sped up. Prior to shutting down, all partitions for which the server is the leader will be moved to the replicas. The leadership transfer will be faster as a result, and the period each partition is inaccessible will be decreased to a few milliseconds.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "18. Can the number of partitions for a topic be changed in Kafka?",
        "answer": "Currently, Kafka does not allow you to reduce the number of partitions for a topic. The partitions can be expanded but not shrunk. The alter command in Apache Kafka allows you to change the behavior of a topic and its associated configurations. To add extra partitions, use the alter command. To increase the number of partitions to five, use the following command: ./bin/kafka-topics.sh --alter --zookeeper localhost:2181 --topic sample-topic --partitions 5 ./bin/kafka-topics.sh --alter --zookeeper localhost:2181 --topic sample-topic --partitions 5",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "19. What do you mean by BufferExhaustedException and OutOfMemoryException in Kafka?",
        "answer": "When the producer can't assign memory to a record because the buffer is full, a BufferExhaustedException is thrown. If the producer is in non-blocking mode, and the rate of production exceeds the rate at which data is transferred from the buffer for long enough, the allocated buffer will be depleted, the exception will be thrown. BufferExhaustedException If the consumers are sending huge messages or if there is a spike in the number of messages sent at a rate quicker than the rate of downstream processing, an OutOfMemoryException may arise. As a result, the message queue fills up, consuming memory space. OutOfMemoryException",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "20. How will you change the retention time in Kafka at runtime?",
        "answer": "A topic's retention time can be configured in Kafka. A topic's default retention time is seven days. While creating a new subject, we can set the retention time. When a topic is generated, the broker's property log.retention.hours are used to set the retention time. When configurations for a currently operating topic need to be modified, kafka-topic.sh must be used. The right command is determined on the Kafka version in use. The command to use up to 0.8.2 is kafka-topics.sh --alter.\nUse kafka-configs.sh --alter starting with version 0.9.0. The command to use up to 0.8.2 is kafka-topics.sh --alter. Use kafka-configs.sh --alter starting with version 0.9.0.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "21. Differentiate between Kafka streams and Spark Streaming.",
        "answer": "  Kafka Streams Spark Streaming\nKafka is fault-tolerant because of partitions and their replicas. Using Cache and RDD (Resilient Distributed Dataset), Spark can restore partitions.\nIt is only capable of handling real-time streams It is capable of handling both real-time and batch tasks.\nMessages in the Kafka log are persistent. To keep the data durable, you'll need to utilize a dataframe or another data structure.\nThere are no interactive modes in Kafka. The data from the producer is simply consumed by the broker, who then waits for the client to read it. Interactive modes are available. Kafka Streams Spark Streaming\nKafka is fault-tolerant because of partitions and their replicas. Using Cache and RDD (Resilient Distributed Dataset), Spark can restore partitions.\nIt is only capable of handling real-time streams It is capable of handling both real-time and batch tasks.\nMessages in the Kafka log are persistent. To keep the data durable, you'll need to utilize a dataframe or another data structure.\nThere are no interactive modes in Kafka. The data from the producer is simply consumed by the broker, who then waits for the client to read it. Interactive modes are available. Kafka Streams Spark Streaming Kafka Streams Spark Streaming Kafka Streams Spark Streaming Kafka is fault-tolerant because of partitions and their replicas. Using Cache and RDD (Resilient Distributed Dataset), Spark can restore partitions.\nIt is only capable of handling real-time streams It is capable of handling both real-time and batch tasks.\nMessages in the Kafka log are persistent. To keep the data durable, you'll need to utilize a dataframe or another data structure.\nThere are no interactive modes in Kafka. The data from the producer is simply consumed by the broker, who then waits for the client to read it. Interactive modes are available. Kafka is fault-tolerant because of partitions and their replicas. Using Cache and RDD (Resilient Distributed Dataset), Spark can restore partitions. Kafka is fault-tolerant because of partitions and their replicas. Using Cache and RDD (Resilient Distributed Dataset), Spark can restore partitions. It is only capable of handling real-time streams It is capable of handling both real-time and batch tasks. It is only capable of handling real-time streams It is capable of handling both real-time and batch tasks. Messages in the Kafka log are persistent. To keep the data durable, you'll need to utilize a dataframe or another data structure. Messages in the Kafka log are persistent. To keep the data durable, you'll need to utilize a dataframe or another data structure. There are no interactive modes in Kafka. The data from the producer is simply consumed by the broker, who then waits for the client to read it. Interactive modes are available. There are no interactive modes in Kafka. The data from the producer is simply consumed by the broker, who then waits for the client to read it. Interactive modes are available.",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "22. What are Znodes in Kafka Zookeeper? How many types of Znodes are there?",
        "answer": "The nodes in a ZooKeeper tree are called znodes. Version numbers for data modifications, ACL changes, and timestamps are kept by Znodes in a structure. ZooKeeper uses the version number and timestamp to verify the cache and guarantee that updates are coordinated. Each time the data on Znode changes, the version number connected with it grows. There are three different types of Znodes:   Persistence Znode: These are znodes that continue to function even after the client who created them has been disconnected. Unless otherwise specified, all znodes are persistent by default.\nEphemeral Znode: Ephemeral znodes are only active while the client is still alive. When the client who produced them disconnects from the ZooKeeper ensemble, the ephemeral Znodes are automatically removed. They have a significant part in the election of the leader.\nSequential Znode: When znodes are constructed, the ZooKeeper can be asked to append an increasing counter to the path's end. The parent znode's counter is unique. Sequential nodes can be either persistent or ephemeral. Persistence Znode: These are znodes that continue to function even after the client who created them has been disconnected. Unless otherwise specified, all znodes are persistent by default. Persistence Znode: Ephemeral Znode: Ephemeral znodes are only active while the client is still alive. When the client who produced them disconnects from the ZooKeeper ensemble, the ephemeral Znodes are automatically removed. They have a significant part in the election of the leader. Ephemeral Znode: Sequential Znode: When znodes are constructed, the ZooKeeper can be asked to append an increasing counter to the path's end. The parent znode's counter is unique. Sequential nodes can be either persistent or ephemeral. Sequential Znode: Conclusion: In this article, we discussed the most frequently asked interview questions on Kafka. It should be clear why Kafka is such an effective streaming platform. Kafka is a useful solution for scenarios that require real-time data processing, application activity tracking, and monitoring. At the same time, Kafka should not be utilized for on-the-fly data conversions, data storage, or when a simple task queue is all that is required. References and Resources: References and Resources: Kafka Documentation Kafka Documentation Spark Interview Spark Interview Java Interview Java Interview Kafka Vs RabbitMQ Kafka Vs RabbitMQ",
        "reference": "interviewbit.com",
        "role": "kafka"
    },
    {
        "question": "1) What is Apache Kafka?",
        "answer": "Apache Kafka is a publish-subscribe messaging application developed by Apache and written in Scala programming language. It is an open-source distributed, partitioned and replicated log service and a message broker application. The design pattern of Kafka is mainly based on the design of the transactional log.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "2) What are some key features of Apache Kafka?",
        "answer": "Following is the list of some of the key features of Apache Kafka:\n\nADVERTISEMENT\nKafka was started by the Apache software and written in Scala programming language.\nKafka is a publish-subscribe messaging system built for high throughput and fault tolerance.\nKafka has a built-in partition system known as a Topic.\nKafka provides the feature of replication.\nKafka provides a queue that can handle large amounts of data and move messages from one sender to another.\nKafka can also save the messages to storage and replicate them across the cluster.\nKafka collaborates with Zookeeper to coordinate and synchronize with other services.\nApache Spark is well supported by Kafka.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "3) What are the different elements or components available in Apache Kafka?",
        "answer": "Following are some important elements or components available in Apache Kafka:\n\nTopic: In Kafka, a topic is a collection or a stream of messages that belong to the same type.\nProducer: In Kafka, Producers are used to issuing communications and publishing messages to a specific Kafka topic.\nConsumer: Kafka Consumers are used to subscribing a topic and also read and process messages from the topic. These are also responsible for subscribing to various topics and pull the data from different brokers.\nBrokers: Brokers are a set of servers that has the capability of storing publisher messages. They are used to manage the storage of messages in the topic.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "4) What do you understand by a consumer group in Apache Kafka?",
        "answer": "A consumer group is an exclusive concept of Kafka, which specifies that we will have one or more consumers who consume subscribed topics within each Kafka consumer group.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "5) What is the role of the ZooKeeper in Kafka?",
        "answer": "Apache Kafka is a distributed system. Within the Kafka environment, the ZooKeeper stores offset-related information, which is used to consume a specific topic and by a specific consumer group. The main role of Zookeeper is to build coordination between different nodes in a cluster, but it can also be used to recover from previously committed offset if any node fails as it works as periodically commit offset.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "6) Can we use Apache Kafka without ZooKeeper? / Is it possible to use Kafka without ZooKeeper?",
        "answer": "It is impossible to sideline Zookeeper and connect directly to the Kafka server. So, we cannot use Apache Kafka without ZooKeeper. If ZooKeeper is down, we cannot serve any client request in Kafka.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "7) What is the traditional method of message transfer in Kafka?",
        "answer": "In Apache Kafka, the traditional method of message transfer has two ways:\nQueuing: In the queuing method, a pool of consumers may read messages from the server, and each message goes to one of them.\nPublish-Subscribe: In the Publish-Subscribe model, messages are broadcasted to all consumers.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "8) What is the role of offset in Apache Kafka?",
        "answer": "Offset is a sequential ID number or a unique id assigned to the messages in the partitions. Offsets are used to identify each message in the partition uniquely with the id available within the partition.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "9) What do you understand by a Consumer Group in Kafka?",
        "answer": "Consumer Group in Kafka is nothing but an exclusive concept of Kafka. Every Kafka consumer group consists of one or more consumers who consume a set of subscribed topics.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "10) What are the key benefits of Apache Kafka over the other traditional techniques?",
        "answer": "Following is a list of key benefits of Apache Kafka above other traditional messaging techniques:\nADVERTISEMENT\nKafka is Fast: Kafka is extremely fast because a single Kafka broker can serve thousands of clients by handling megabytes of reads and writes per second.\nKafka is Scalable: In Kafka, we can partition data and streamline over a cluster of machines to enable larger data.\nKafka is Durable: In Kafka, messages are persistent and are replicated within the cluster to prevent data loss. That's why Kafka is durable.\nKafka is Distributed by Design: Kafka provides fault tolerance features, and its distributed design also guarantees durability.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "11) What are the four core API architectures that Kafka uses?",
        "answer": "Following are the four core APIs that Kafka uses:\n\nProducer API: In Apache Kafka, the Producer API allows an application to publish a stream of records to one or more Kafka topics.\nConsumer API: In Apache Kafka, the consumer API allows an application to subscribe to one or more Kafka topics. It also enables the application to process streams of records generated about such topics.\nStreams API: In Apache Kafka, the Kafka Streams API allows an application to use a stream processing architecture to process data in Kafka. We can also use this application API to take input streams from one or more topics, process those using stream operations, and generate output streams to transmit to more topics. We can also use the Streams API to convert input streams into output streams.\nConnect API: In Apache Kafka, the Kafka Connect API (also called Connector API) connects Kafka topics to applications. This API constructs and manages the operations of producers and consumers and establishing reusable links between these solutions. For example, A Connect API may capture all database updates and ensure they are made available in a Kafka topic.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "12) What do you understand by the terms leader and follower in the Kafka environment?",
        "answer": "The terms leader and follower are used in the Apache Kafka environment to maintain the overall system and ensure the load balancing on the servers. Following is a list of some important features of leader and follower in Kafka:\nFor every partition in the Kafka environment, one server plays the role of leader, and the remaining servers act as followers.\nThe leader level is responsible for executing the all data read and write commands, and the rest of the followers have to replicate the process.\nSuppose any time any fault occurs and the leader is not able to function appropriately. In that case, one of the followers takes the place and responsibility of the leaders and makes the system stable and helps in the server's load balancing.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "13) What do you understand by the partition in Kafka?",
        "answer": "In every Kafka broker, some partitions are available, either a leader or a replica of a topic.\nEvery Kafka topic separated into partitions contains records in a fixed order in each of them.\nEach record in a partition is assigned and attributed with a unique offset. Multiple partition logs are possible in a single topic. Because of this facility, several users can read from the same topic at the same time.\nTopics can be parallelized via partitions, which split data into a single topic among numerous brokers.\nIn Kafka, replication is done at the partition level, and a replica is the redundant element of a topic partition.\nEach partition can contain one or more replicas, and it means the partitions can contain messages that are duplicated across many Kafka brokers in the cluster.\nOne server acts as the leader of each partition or replica, while the others act as followers.\nIf the leader goes down in any circumstances, one of the followers takes over as the leader.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "14) Why is Kafka technology significant to use? / What are some key advantages of using Kafka?",
        "answer": "Following are some key advantages of Kafka, which makes it significant to use:\nMinimum Input High-throughput: Apache Kafka doesn't require any large hardware to handle a huge amount of data. It can handle high-velocity and high-volume data by itself and support a message throughput of thousands of messages per second.\nFault-Tolerant: Kafka is fault-tolerant, and it is resistant to any node or machine failure within a cluster.\nScalability: Kafka is fully scalable. It can be scaled-out, without facing any downtime in its execution by adding some additional nodes.\nLow Latency: Low latency is one of the biggest advantages of Kafka, and it can easily handle many messages with the very low latency of milliseconds demanded by most new use cases.\nDurability: Kafka is a great example of durability. It supports messages replication to ensure that any messages are never lost, which is why its durability.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "15) What is the importance of Topic Replication in Kafka? What do you understand by ISR in Kafka?",
        "answer": "Topic replication is very important in Kafka. It is used to construct Kafka deployments to ensure durability and high availability. When one broker fails, topic replicas on other brokers remain available to ensure that data is not lost and Kafka deployment is not disrupted in any case. The replication ensures that the messages published are not lost.\nThe replication factor specifies the number of copies of a topic kept across the Kafka cluster. It takes place at the partition level and is defined at the subject level. For example, taking a replication factor of two will keep two copies of a topic for each partition. The replication factor cannot be more than the cluster's total number of brokers.\nISR stands for In-Sync Replica, and it is a replica that is up to date with the partition's leader.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "16) What would be if a replica stays out of the ISR for a very long time?",
        "answer": "If a replica stays out of the ISR for a very long time, or if a replica is not in sync with the ISR, then it means that the follower server cannot receive and execute data as fast as possible the leader is doing. So, it specifies that the follower is not able to come up with the leader activities.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "17) What is the process of starting a Kafka server?",
        "answer": "When you start to run the Kafka environment on a zookeeper, you must ensure to run the zookeeper server first and then start the Kafka server. This is the correct way to start the Kafka server. Follow the steps given below:\nFirst, download the most recent version of Kafka and extract it.\nEnsure that the local environment has Java 8+ installed to run Kafka.\nUse the following commands to start the Kafka server and ensure that all services are started in the correct order:\nStart the ZooKeeper service by doing the following:\n$bin/zookeeper-server-start.sh config/zookeeper.properties  \nTo start the Kafka broker service, open a new terminal and type the following command:\n$ bin/kafka-server-start.sh config/server.properties",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "18) What do you understand by a consumer group in Kafka?",
        "answer": "In Apache Kafka, a consumer group is a collection of consumers who work together to ingest data from the same topic or range of topics.\n\n\nIn Apache Kafka, a consumer group is a collection of consumers who work together to ingest data from the same topic or range of topics. The consumer group essentially represents the name of an application. There are several categories of consumers in Kafka. The '-group' command must be used to consume messages from a consumer group.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "19) What is the role of Kafka producer API?",
        "answer": "The Kafka procedure API does the producer functionality through one API call to the client. Especially, the Kafka producer API combines the efforts of Kafka.producer.SyncProducer and the Kafka.producer.async.Async Producer.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "20) What is the maximum size of a message that Kafka can receive?",
        "answer": "By default, the maximum size of a Kafka message is 1MB (megabyte), but we can modify it accordingly. The broker settings facilitate us to modify the size.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "21) What are the key differences between Apache Kafka and Apache Flume?",
        "answer": "A list of key differences between Apache Kafka and Apache Flume:\nApache Kafka Apache Flume\nApache Kafka is a distributed data store or a data system. Apache Flume is a distributed, available, and reliable system.\nApache Kafka is optimized for ingesting and processing streaming data in real-time. Apache Flume can efficiently collect, aggregate and move a large amount of log data from many different sources to a centralized data store.\nApache Kafka is easy to scale. Apache Flume is not scalable as Kafka. It is not easy to scale.\nIt is working as a pull model. It is working as a push model.\nIt is a highly available, fault-tolerant, efficient and scalable messaging system. It also supports automatic recovery. It is specially designed for Hadoop. In case of flume-agent failure, it is possible to lose events in the channel.\nApache Kafka runs as a cluster and easily handles the incoming high volume data streams in real-time. Apache Flume is a tool to collect log data from distributed web servers.\nApache Kafka treats each topic partition as an ordered set of messages. Apache Flume takes in streaming data from multiple sources for storage and analysis, which is used in Hadoop.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "22) What do you understand by geo-replication in Kafka?",
        "answer": "In Kafka, geo-replication is a feature that facilitates you to copy messages form one cluster to many other data centers or cloud regions. Using geo-replication, you can replicate all of the files and store them throughout the globe if required. We can accomplish geo-replication by using Kafka's MirrorMaker Tool. By using the geo-replication technique, we can ensure data backup without any failure.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "23) Is Apache Kafka a distributed streaming platform? What can you do with it?",
        "answer": "Yes. Apache Kafka is a distributed streaming platform. A streaming platform contains the following three important capabilities:\nA distributed streaming platform helps us to push records easily.\nIt provides huge storage space and also helps us to store a lot of records without any problem.\nIt helps us to process the records as they come in.\nKafka technology facilitates us to do the following things:\nWith Apache Kafka, we can build a real-time stream of data pipelines to transmit data between two systems.\nWe can also build a real-time streaming platform that can react to the data.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "24) What are the types of the traditional method of message transfer?",
        "answer": "There are mainly two types of the traditional message transfer method. These types are:\nQueuing: In Queuing method, a pool of consumers can read a message from the server, and each message goes to one of them.\nPublish-Subscribe: In the Publish-Subscribe method, messages are broadcasted to all consumers.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "25) What are the biggest disadvantages of Kafka?",
        "answer": "Following is the list of most critical disadvantages of Kafka:\nWhen the messages are continuously updated or changed, Kafka performance degrades. Kafka works well when the message does not need to be updated.\nBrokers and consumers reduce Kafka's performance when they get huge messages because they have to deal with the data by compressing and decompressing the messages. This can reduce the overall Kafka's throughput and performance.\nKafka doesn't support wildcard topic selection. It is necessary to match the exact topic name.\nKafka doesn't support certain message paradigms such as point-to-point queues and request/reply.\nKafka does not have a complete set of monitoring tools.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "26) What is the purpose of the retention period in the Kafka cluster?",
        "answer": "Within the Kafka cluster, the retention period is used to retain all the published records without checking whether they have been consumed or not. Using a configuration setting for the retention period, we can easily discard the records. The main purpose of discarding the records from the Kafka cluster is to free up some space.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "27) What do you understand by load balancing? What ensures load balancing of the server in Kafka?",
        "answer": "In Apache Kafka, load balancing is a straightforward process that the Kafka producers by default handle. The load balancing process spreads out the message load between partitions while preserving message ordering. Kafka enables users to specify the exact partition for a message.\nIn Kafka, leaders perform the task of all read and write requests for the partition. On the other hand, followers passively replicate the leader. At the time of leader failure, one of the followers takes over the role of the leader, and this entire process ensures load balancing of the servers.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "28) When does the broker leave the ISR?",
        "answer": "ISR is a set of message replicas that are completely synced up with the leaders. It means ISR contains all the committed messages, and ISR always includes all the replicas until it gets a real failure. An ISR can drop a replica if it deviates from the leader.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "29) How can you get exactly-once messaging from Kafka during data production?",
        "answer": "To get exactly-once messaging during data production from Kafka, we must follow the two things avoiding duplicates during data consumption and avoiding duplication during data production.\nFollowing are the two ways to get exactly one semantics while data production:\nAvail a single writer per partition. Whenever you get a network error, you should check the last message in that partition to see if your last write succeeded.\nIn the message, include a primary key (UUID or something) and de-duplicate on the consumer.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "30) What is the use of Apache Kafka Cluster?",
        "answer": "Apache Kafka Cluster is a messaging system used to overcome the challenges of collecting a large volume of data and analyzing the collected data. Following are the main benefits of Apache Kafka Cluster:\nUsing Apache Kafka Cluster, we can track web activities by storing/sending the events for real-time processes.\nBy using this, we can alert as well as report the operational metrics.\nApache Kafka Cluster also facilitates us to transform data into the standard format.\nIt allows continuous processing of streaming data to the topics.\nBecause of its awesome features, it is ruling over some of the most popular applications such as ActiveMQ, RabbitMQ, AWS etc.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "31) What are some of the real-world usages of Apache Kafka?",
        "answer": "Following are some of the real-world usages of Apache Kafka:\nApache Kafka as a Message Broker: Apache Kafka has a great throughput value, so; it can manage a huge amount of comparable types of messages or data. Apache Kafka can be used as a publish-subscribe messaging system that allows data to be read and published conveniently.\nTo track website activities: Apache Kafka can check if the data is transferred and received successfully by websites. Apache Kafka can handle the massive amounts of data created by websites for each page and the activities of users.\nTo monitor operational data: We can use Apache Kafka to keep track of metrics related to certain technologies, such as security logs.\nData logging: Apache Kafka provides data replication between nodes functionality that can be used to restore data on failed nodes. It can also be used to collect data from various logs and make it available to consumers.\nStream Processing with Kafka: Apache Kafka can also handle streaming data, the data that is read from one topic, processed, and then written to another. Users and applications will have access to a new topic containing the processed data.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "32) What do you understand by the term \"Log Anatomy\" in Apache Kafka?",
        "answer": "Log Anatomy is a way to view a partition. We view the log as the partitions, and a data source writes messages to the log. It facilitates that one or more consumers read that data from the log at any time they want. It specifies that the data source can write a log, and the log is being read by consumers at different offsets simultaneously.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "33) What are the ways to tune Kafka for optimal performance?",
        "answer": "There are mainly three ways to tune Kafka for optimal performance:\nTuning Kafka Producers\nKafka Brokers Tuning\nTuning Kafka Consumers",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "34) What are the use cases of Kafka monitoring?",
        "answer": "Following are the use cases of Apache Kafka monitoring:\nApache Kafka monitoring can keep track of system resources consumption such as memory, CPU, and disk utilization over time.\nApache Kafka monitoring is used to monitor threads and JVM usage. It relies on the Java garbage collector to free up memory, ensuring that it frequently runs, thereby guaranteeing that the Kafka cluster is more active.\nIt can be used to determine which applications are causing excessive demand, and identifying performance bottlenecks might help rapidly solve performance issues.\nIt always checks the broker, controller, and replication statistics to modify the partitions and replicas status if required.",
        "reference": "javatpoint.com",
        "role": "kafka"
    },
    {
        "question": "35) What is the difference between Apache Kafka and RabbitMQ?",
        "answer": "RabbitMQ is one of Apache Kafka's alternatives. Let's see the key differences between Apache Kafka and RabbitMQ:\nDifferences between Apache Kafka and RabbitMQ:\nApache Kafka RabbitMQ\nApache Kafka provides message ordering because of its partitions. Here, messages are sent to topics by message key. RabbitMQ doesn't support message ordering.\nApache Kafka is distributed, durable and highly available. Here, data is shared as well as replicated. There are no such features in RabbitMQ.\nApache Kafka is a log, and it supports message logging that means messages are always there. We can manage this by specifying a message retention policy. Rabbit MQ is a queue. Here, messages are destroyed once consumed, and acknowledgement is provided.\nIt retains order only inside a partition and guarantees that the whole batch of messages either fails or passes. It doesn't provide a guarantee, even in relation to transactions involving a single queue.\nIn Apache Kafka, the performance rate is around 100,000 messages/second. In the case of RabbitMQ, the performance rate is around 20,000 messages/second.",
        "reference": "javatpoint.com",
        "role": "kafka"
    }
]