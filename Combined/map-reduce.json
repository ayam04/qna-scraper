[
    {
        "question": "Here is a Mapreduce Tutorial Video by Intellipaat",
        "answer": "Basic Interview Questions",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "1. Compare MapReduce and Spark",
        "answer": "Criteria MapReduce Spark\nProcessing Speeds Good Exceptional\nStandalone mode Needs Hadoop Can work independently\nEase of use Needs extensive Java program APIs for Python, Java, & Scala\nVersatility Real-time & machine learning applications Not optimized for real-time & machine learning applications",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "2. What is MapReduce?",
        "answer": "Referred as the core of Hadoop, MapReduce is a programming framework to process large sets of data or big data across thousands of servers in a Hadoop Cluster. The concept of MapReduce is similar to the cluster scale-out data processing systems. The term MapReduce refers to two important processes of Hadoop program operates.\nFirst is the map() job, which converts a set of data into another breaking down individual elements into key/value pairs (tuples). Then comes reduce() job into play, wherein the output from the map, i.e. the tuples serve as the input and are combined into smaller set of tuples. As the name suggests, the map job every time occurs before the reduce one.\nLearn more about MapReduce in this insightful article on: Hadoop MapReduce – What it Refers To?",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "3. Illustrate a simple example of the working of MapReduce.",
        "answer": "Let’s take a simple example to understand the functioning of MapReduce. However, in real-time projects and applications, this is going to be elaborate and complex as the data we deal with Hadoop and MapReduce is extensive and massive.\nAssume you have five files and each file consists of two key/value pairs as in two columns in each file – a city name and its temperature recorded. Here, name of city is the key and the temperature is value.\nSan Francisco, 22\nLos Angeles, 15\nVancouver, 30\nLondon, 25\nLos Angeles, 16\nVancouver, 28\nLondon,12\nIt is important to note that each file may consist of the data for same city multiple times. Now, out of this data, we need to calculate the maximum temperature for each city across these five files. As explained, the MapReduce framework will divide it into five map tasks and each map task will perform data functions on one of the five files and returns maxim temperature for each city.\n(San Francisco, 22)(Los Angeles, 16)(Vancouver, 30)(London, 25)\nSimilarly each mapper performs it for the other four files and produce intermediate results, for instance like below.\n(San Francisco, 32)(Los Angeles, 2)(Vancouver, 8)(London, 27)\n(San Francisco, 29)(Los Angeles, 19)(Vancouver, 28)(London, 12)\n(San Francisco, 18)(Los Angeles, 24)(Vancouver, 36)(London, 10)\n(San Francisco, 30)(Los Angeles, 11)(Vancouver, 12)(London, 5)\nThese tasks are then passed to the reduce job, where the input from all files are combined to output a single value. The final results here would be:\n(San Francisco, 32)(Los Angeles, 24)(Vancouver, 36)(London, 27)\nThese calculations are performed instantly and are extremely efficient to calculate outputs on a large dataset.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "4. What are the main components of MapReduce Job?",
        "answer": "Main Driver Class: providing job configuration parameters\nMapper Class: must extend org.apache.hadoop.mapreduce.Mapper class and performs execution of\nmap() method\nReducer Class: must extend org.apache.hadoop.mapreduce.Reducer class\nGet 100% Hike!\nMaster Most in Demand Skills Now !\nBy providing your contact details, you agree to our Terms of Use & Privacy Policy",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "5. What is Shuffling and Sorting in MapReduce?",
        "answer": "Shuffling and Sorting are two major processes operating simultaneously during the working of mapper and reducer.\nThe process of transferring data from Mapper to reducer is Shuffling. It is a mandatory operation for reducers to proceed their jobs further as the shuffling process serves as input for the reduce tasks.\nIn MapReduce, the output key-value pairs between the map and reduce phases (after the mapper) are automatically sorted before moving to the Reducer. This feature is helpful in programs where you need sorting at some stages. It also saves the programmer’s overall time.\nLearn all about shuffling and sorting in this comprehensive MapReduce Tutorial.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "6. What is Partitioner and its usage?",
        "answer": "Partitioner is yet another important phase that controls the partitioning of the intermediate map-reduce output keys using a hash function. The process of partitioning determines in what reducer, a key-value pair (of the map output) is sent. The number of partitions is equal to the total number of reduce jobs for the process.\nHash Partitioner is the default class available in Hadoop , which implements the following function.int getPartition(K key, V value, int numReduceTasks)\nThe function returns the partition number using the numReduceTasks is the number of fixed reducers.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "7. What is Identity Mapper and Chain Mapper?",
        "answer": "Identity Mapper is the default Mapper class provided by Hadoop. when no other Mapper class is defined, Identify will be executed. It only writes the input data into output and do not perform and computations and calculations on the input data.\nThe class name is org.apache.hadoop.mapred.lib.IdentityMapper.\nChain Mapper is the implementation of simple Mapper class through chain operations across a set of Mapper classes, within a single map task. In this, the output from the first mapper becomes the input for second mapper and second mapper’s output the input for third mapper and so on until the last mapper.\nThe class name is org.apache.hadoop.mapreduce.lib.ChainMapper.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "8. What main configuration parameters are specified in MapReduce?",
        "answer": "The MapReduce programmers need to specify following configuration parameters to perform the map and reduce jobs:\nThe input location of the job in HDFs.\nThe output location of the job in HDFS.\nThe input’s and output’s format.\nThe classes containing map and reduce functions, respectively.\nThe .jar file for mapper, reducer and driver classes",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "9. Name Job control options specified by MapReduce.",
        "answer": "Since this framework supports chained operations wherein an input of one map job serves as the output for other, there is a need for job controls to govern these complex operations.\nThe various job control options are:\nJob.submit() : to submit the job to the cluster and immediately return\nJob.waitforCompletion(boolean) : to submit the job to the cluster and wait for its completion",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "10. What is InputFormat in Hadoop?",
        "answer": "Another important feature in MapReduce programming, InputFormat defines the input specifications for a job. It performs the following functions:\nValidates the input-specification of job.\nSplit the input file(s) into logical instances called InputSplit. Each of these split files are then assigned to individual Mapper.\nProvides implementation of RecordReader to extract input records from the above instances for further Mapper processing\nIntermediate Interview Questions",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "11. What is the difference between HDFS block and InputSplit?",
        "answer": "An HDFS block splits data into physical divisions while InputSplit in MapReduce splits input files logically.\nWhile InputSplit is used to control number of mappers, the size of splits is user defined. On the contrary, the HDFS block size is fixed to 64 MB, i.e. for 1GB data , it will be 1GB/64MB = 16 splits/blocks. However, if input split size is not defined by user, it takes the HDFS default block size.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "12. What is Text Input Format?",
        "answer": "It is the default InputFormat for plain text files in a given job having input files with .gz extension. In TextInputFormat, files are broken into lines, wherein key is position in the file and value refers to the line of text. Programmers can write their own InputFormat.\nThe hierarchy is:\njava.lang.Object\norg.apache.hadoop.mapreduce.InputFormat<K,V>;\norg.apache.hadoop.mapreduce.lib.input.FileInputFormat <LongWritable,Text>\norg.apache.hadoop.mapreduce.lib.input.TextInputFormat",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "13. What is JobTracker?",
        "answer": "JobTracker is a Hadoop service used for the processing of MapReduce jobs  in the cluster. It submits and tracks the jobs to specific nodes having data. Only one JobTracker runs on single Hadoop cluster on its own JVM process. if JobTracker goes down, all the jobs halt.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "14. Explain job scheduling through JobTracker.",
        "answer": "JobTracker communicates with NameNode to identify data location and submits the work to TaskTracker node. The TaskTracker plays a major role as it notifies the JobTracker for any job failure. It actually is referred to the heartbeat reporter reassuring the JobTracker that it is still alive. Later, the JobTracker is responsible for the actions as in it may either resubmit the job or mark a specific record as unreliable or blacklist it.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "15. What is SequenceFileInputFormat?",
        "answer": "A compressed binary output file format to read in sequence files and extends the FileInputFormat.It passes data between output-input (between output of one MapReduce job to input of another MapReduce job)phases of MapReduce jobs.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "16. How to set mappers and reducers for Hadoop jobs?",
        "answer": "Users can configure JobConf variable to set number of mappers and reducers.\njob.setNumMaptasks()\njob.setNumreduceTasks()",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "17. Explain JobConf in MapReduce.",
        "answer": "It is a primary interface to define a map-reduce job in the Hadoop for job execution. JobConf specifies mapper, Combiner, partitioner, Reducer,InputFormat , OutputFormat implementations and other advanced job faets liek Comparators.\nThese are described in Mapreduce’s online reference guide and on Mapreduce community.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "18. What is a MapReduce Combiner?",
        "answer": "Also known as semi-reducer, Combiner is an optional class to combine the map out records using the same key. The main function of a combiner is to accept inputs from Map Class and pass those key-value pairs to Reducer class",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "19. What is RecordReader in a Map Reduce?",
        "answer": "RecordReader is used to read key/value pairs form the InputSplit by converting the byte-oriented view  and presenting record-oriented view to Mapper.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "20. Define Writable data types in MapReduce.",
        "answer": "Hadoop reads and writes data in a serialized form in writable interface. The Writable interface has several classes like Text (storing String data), IntWritable, LongWriatble, FloatWritable, BooleanWritable. users are free to define their personal Writable classes as well.\nAdvanced Interview Questions",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "21. What is OutputCommitter?",
        "answer": "OutPutCommitter describes the commit of MapReduce task. FileOutputCommitter is the default available class available for OutputCommitter in MapReduce. It performs the following operations:\nCreate temporary output directory for the job during initialization.\nThen, it cleans the job as in removes temporary output directory post job completion.\nSets up the task temporary output.\nIdentifies whether a task needs commit. The commit is applied if required.\nJobSetup, JobCleanup and TaskCleanup are important tasks during output commit.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "22. What is a “map” in Hadoop?",
        "answer": "In Hadoop, a map is a phase in HDFS query solving. A map reads data from an input location, and outputs a key value pair according to the input type.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "23. What is a “reducer” in Hadoop?",
        "answer": "In Hadoop, a reducer collects the output generated by the mapper, processes it, and creates a final output of its own.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "24. What are the parameters of mappers and reducers?",
        "answer": "The four parameters for mappers are:\nLongWritable (input)\ntext (input)\ntext (intermediate output)\nIntWritable (intermediate output)\nThe four parameters for reducers are:\nText (intermediate output)\nIntWritable (intermediate output)\nText (final output)\nIntWritable (final output)",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "25. What are the key differences between Pig vs MapReduce?",
        "answer": "PIG is a data flow language, the key focus of Pig is manage the flow of data from input source to output store. As part of managing this data flow it moves data feeding it to\nprocess 1. taking output and feeding it to\nprocess2. The core features are preventing execution of subsequent stages if previous stage fails, manages temporary storage of data and most importantly compresses and rearranges processing steps for faster processing. While this can be done for any kind of processing tasks Pig is written specifically for managing data flow of Map reduce type of jobs. Most if not all jobs in a Pig are map reduce jobs or data movement jobs. Pig allows for custom functions to be added which can be used for processing in Pig, some default ones are like ordering, grouping, distinct, count etc.\nMapreduce on the other hand is a data processing paradigm, it is a framework for application developers to write code in so that its easily scaled to PB of tasks, this creates a separation between the developer that writes the application vs the developer that scales the application. Not all applications can be migrated to Map reduce but good few can be including complex ones like k-means to simple ones like counting uniques in a dataset.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "26. What is partitioning?",
        "answer": "Partitioning is a process to identify the reducer instance which would be used to supply the mappers output. Before mapper emits the data (Key Value) pair to reducer, mapper identify the reducer as an recipient of mapper output. All the key, no matter which mapper has generated this, must lie with same reducer.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "27. How to set which framework would be used to run mapreduce program?",
        "answer": "mapreduce.framework.name. it can be\nLocal\nclassic\nYarn",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    },
    {
        "question": "28. What platform and Java version is required to run Hadoop?",
        "answer": "Java 1.6.x or higher version are good for Hadoop, preferably from Sun. Linux and Windows are the supported operating system for Hadoop, but BSD, Mac OS/X and Solaris are more famous to work.",
        "reference": "intellipaat.com",
        "role": "map-reduce"
    }
]