[
    {
        "question": "1. What do you understand by Natural Language Processing?",
        "answer": "Natural Language Processing is a field of computer science that deals with communication between computer systems and humans. It is a technique used in Artificial Intelligence and Machine Learning. It is used to create automated software that helps understand human-spoken languages to extract useful information from the data. Techniques in NLP allow computer systems to process and interpret data in the form of natural languages.",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "2. List any two real-life applications of Natural Language Processing.",
        "answer": "Two real-life applications of Natural Language Processing are as follows:\nGoogle Translate: Google Translate is one of the famous applications of Natural Language Processing. It helps convert written or spoken sentences into any language. Also, we can find the correct pronunciation and meaning of a word by using Google Translate. It uses advanced techniques of Natural Language Processing to achieve success in translating sentences into various languages.\n\nChatbots: To provide a better customer support service, companies have started using chatbots for 24/7 service. AI Chatbots help resolve the basic queries of customers. If a chatbot is not able to resolve any query, then it forwards it to the support team, while still engaging the customer. It helps make customers feel that the customer support team is quickly attending to them. With the help of chatbots, companies have become capable of building cordial relations with customers. It is only possible with the help of Natural Language Processing.",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "3. What are stop words?",
        "answer": "Stop words are said to be useless data for a search engine. Words such as articles, prepositions, etc. are considered stop words. There are stop words such as was, were, is, am, the, a, an, how, why, and many more. In Natural Language Processing, we eliminate the stop words to understand and analyze the meaning of a sentence. The removal of stop words is one of the most important tasks for search engines. Engineers design the algorithms of search engines in such a way that they ignore the use of stop words. This helps show the relevant search result for a query.\nBecome an expert in Natural Language Processing (NLP). Enroll now in NLP training in New York.",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "4. What is NLTK?",
        "answer": "NLTK is a Python library, which stands for Natural Language Toolkit. We use NLTK to process data in human-spoken languages. NLTK allows us to apply techniques such as parsing, tokenization, lemmatization, stemming, and more to understand natural languages. It helps in categorizing text, parsing linguistic structure, analyzing documents, etc.\nA few of the libraries of the NLTK package that we often use in NLP are:\nSequentialBackoffTagger\nDefaultTagger\nUnigramTagger\ntreebank\nwordnet\nFreqDist\npatterns\nRegexpTagger\nbackoff_tagger\nUnigramTagger, BigramTagger, and TrigramTagger\nTo know how to use NLP properly with real-world experience projects. Enroll in NLP Training in Chennai now.\nGet 100% Hike!\nMaster Most in Demand Skills Now !\nBy providing your contact details, you agree to our Terms of Use & Privacy Policy",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "5. What is Syntactic Analysis?",
        "answer": "Syntactic analysis is a technique of analyzing sentences to extract meaning from them. Using syntactic analysis, a machine can analyze and understand the order of words arranged in a sentence. NLP employs grammar rules of a language that helps in the syntactic analysis of the combination and order of words in documents.\nThe techniques used for syntactic analysis are as follows:\n\nParsing: It helps in deciding the structure of a sentence or text in a document. It helps analyze the words in the text based on the grammar of the language.\nWord segmentation: The segmentation of words segregates the text into small significant units.\nMorphological segmentation: The purpose of morphological segmentation is to break words into their base form.\nStemming: It is the process of removing the suffix from a word to obtain its root word.\nLemmatization: It helps combine words using suffixes, without altering the meaning of the word.",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "6. What is Semantic Analysis?",
        "answer": "Semantic analysis helps make a machine understand the meaning of a text. It uses various algorithms for the interpretation of words in sentences. It also helps understand the structure of a sentence.\nTechniques used for semantic analysis are as given below:\n\nNamed entity recognition: This is the process of information retrieval that helps identify entities such as the name of a person, organization, place, time, emotion, etc.\nWord sense disambiguation: It helps identify the sense of a word used in different sentences.\nNatural language generation: It is a process used by the software to convert structured data into human-spoken languages. By using NLG, organizations can automate content for custom reports.\nIf you want to learn Artificial Intelligence then enroll in Artificial Intelligence Training now!",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "7. List the components of Natural Language Processing.",
        "answer": "The major components of NLP are as follows:\n\nEntity extraction: Entity extraction refers to the retrieval of information such as place, person, organization, etc. by the segmentation of a sentence. It helps in the recognition of an entity in a text.\nSyntactic analysis: Syntactic analysis helps draw the specific meaning of a text.\nPragmatic analysis: To find useful information from a text, we implement pragmatic analysis techniques.\nMorphological and lexical analysis: It helps in explaining the structure of words by analyzing them through parsing.",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "8. What is Latent Semantic Indexing (LSI)?",
        "answer": "Latent semantic indexing is a mathematical technique used to improve the accuracy of the information retrieval process. The design of LSI algorithms allows machines to detect the hidden (latent) correlation between semantics (words). To enhance information understanding, machines generate various concepts that associate with the words of a sentence.\nThe technique used for information understanding is called singular value decomposition. It is generally used to handle static and unstructured data. The matrix obtained for singular value decomposition contains rows for words and columns for documents. This method is best suited to identify components and group them according to their types.\nThe main principle behind LSI is that words carry a similar meaning when used in a similar context. Computational LSI models are slow in comparison to other models. However, they are good at contextual awareness which helps improve the analysis and understanding of a text or a document.",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "9. What are Regular Expressions?",
        "answer": "A regular expression is used to match and tag words. It consists of a series of characters for matching strings.\nSuppose, if A and B are regular expressions, then the following are true for them:\nIf {ɛ} is a regular language, then ɛ is a regular expression for it.\nIf A and B are regular expressions, then A + B is also a regular expression within the language {A, B}.\nIf A and B are regular expressions, then the concatenation of A and B (A.B) is a regular expression.\nIf A is a regular expression, then A* (A occurring multiple times) is also a regular expression.",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "10. What is Regular Grammar?",
        "answer": "Regular grammar is used to represent a regular language.\nRegular grammar comprises rules in the form of A -> a, A -> aB, and many more. The rules help detect and analyze strings by automated computation.\nRegular grammar consists of four tuples:\n‘N’ is used to represent the non-terminal set.\n‘∑’ represents the set of terminals.\n‘P’ stands for the set of productions.\n‘S € N’ denotes the start of non-terminal.\nRegular grammar is of 2 types:\n(a) Left Linear Grammar(LLG)\n(b) Right Linear Grammar(RLG)",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "Watch this video on Natural Language Processing Interview Questions for Beginners:",
        "answer": "",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "11. What is Parsing in the context of NLP?",
        "answer": "Parsing in NLP refers to the understanding of a sentence and its grammatical structure by a machine. Parsing allows the machine to understand the meaning of a word in a sentence and the grouping of words, phrases, nouns, subjects, and objects in a sentence. Parsing helps analyze the text or the document to extract useful insights from it. To understand parsing, refer to the below diagram:\n\nIn this, ‘Jonas ate an orange’ is parsed to understand the structure of the sentence.\nGet job assistance and get trained in NLP by enrolling in NLP Training in Bangalore.\nIntermediate NLP Interview Questions",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "12. What is TF-IDF?",
        "answer": "TFIDF or Term Frequency-Inverse Document Frequency indicates the importance of a word in a set. It helps in information retrieval with numerical statistics. For a specific document, TF-IDF shows a frequency that helps identify the keywords in a document. The major use of TF-IDF in NLP is the extraction of useful information from crucial documents by statistical data. It is ideally used to classify and summarize the text in documents and filter out stop words.\nTF helps calculate the ratio of the frequency of a term in a document and the total number of terms. Whereas, IDF denotes the importance of the term in a document.\nThe formula for calculating TF-IDF:\nTF(W) = (Frequency of W in a document)/(The total number of terms in the document)\nIDF(W) = log_e(The total number of documents/The number of documents having the term W)\nWhen TF*IDF is high, the frequency of the term is less and vice versa.\nGoogle uses TF-IDF to decide the index of search results according to the relevancy of pages. The design of the TF-IDF algorithm helps optimize the search results in Google. It helps quality content rank up in search results.\nIf you want to know more about ‘What is Natural Language Processing?’ you can go through this Natural Language Processing Using Python course!",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "13. Define the terminology in NLP.",
        "answer": "This is one of the most often asked NLP interview questions.\nThe interpretation of Natural Language Processing depends on various factors, and they are:\n\nWeights and Vectors\nUse of TF-IDF for information retrieval\nLength (TF-IDF and doc)\nGoogle Word Vectors\nWord Vectors\nStructure of the Text\nPOS tagging\nHead of the sentence\nNamed Entity Recognition (NER)\nSentiment Analysis\nKnowledge of the characteristics of sentiment\nKnowledge about entities and the common dictionary available for sentiment analysis\nClassification of Text\nSupervised learning algorithm\nTraining set\nValidation set\nTest set\nFeatures of the text\nLDA\nMachine Reading\nRemoval of possible entities\nJoining with other entities\nDBpedia\nFRED (lib) Pikes",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "14. Explain Dependency Parsing in NLP.",
        "answer": "Dependency parsing helps assign a syntactic structure to a sentence. Therefore, it is also called syntactic parsing. Dependency parsing is one of the critical tasks in NLP. It allows the analysis of a sentence using parsing algorithms. Also, by using the parse tree in dependency parsing, we can check the grammar and analyze the semantic structure of a sentence.\nFor implementing dependency parsing, we use the spaCy package. It implements token properties to operate the dependency parse tree.\nThe below diagram shows the dependency parse tree:",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "15. What is the difference between NLP and NLU?",
        "answer": "The below table shows the difference between NLP and NLU:",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "16. What is the difference between NLP and CI?",
        "answer": "The below table shows the difference between NLP and CI:",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "17. What is Pragmatic Analysis?",
        "answer": "Pragmatic analysis is an important task in NLP for interpreting knowledge that is lying outside a given document. The aim of implementing pragmatic analysis is to focus on exploring a different aspect of the document or text in a language. This requires a comprehensive knowledge of the real world. The pragmatic analysis allows software applications for the critical interpretation of the real-world data to know the actual meaning of sentences and words.\nExample:\nConsider this sentence: ‘Do you know what time it is?’\nThis sentence can either be asked for knowing the time or for yelling at someone to make them note the time. This depends on the context in which we use the sentence.",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "18. What is Pragmatic Ambiguity?",
        "answer": "Pragmatic ambiguity refers to the multiple descriptions of a word or a sentence. An ambiguity arises when the meaning of the sentence is not clear. The words of the sentence may have different meanings. Therefore, in practical situations, it becomes a challenging task for a machine to understand the meaning of a sentence. This leads to pragmatic ambiguity.\nExample:\nCheck out the below sentence.\n‘Are you feeling hungry?’\nThe given sentence could be either a question or a formal way of offering food.",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "19. What are unigrams, bigrams, trigrams, and n-grams in NLP?",
        "answer": "When we parse a sentence one word at a time, then it is called a unigram. The sentence parsed two words at a time is a bigram.\nWhen the sentence is parsed three words at a time, then it is a trigram. Similarly, n-gram refers to the parsing of n words at a time.\nExample: To understand unigrams, bigrams, and trigrams, you can refer to the below diagram:\n\nTherefore, parsing allows machines to understand the individual meaning of a word in a sentence. Also, this type of parsing helps predict the next word and correct spelling errors.\nAre you interested in learning Artificial Intelligence from experts? Enroll in our AI Course in Bangalore now!",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "20. What are the steps involved in solving an NLP problem?",
        "answer": "Below are the steps involved in solving an NLP problem:\nGather the text from the available dataset or by web scraping\nApply stemming and lemmatization for text cleaning\nApply feature engineering techniques\nEmbed using word2vec\nTrain the built model using neural networks or other Machine Learning techniques\nEvaluate the model’s performance\nMake appropriate changes in the model\nDeploy the model",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "21. What is Feature Extraction in NLP?",
        "answer": "Features or characteristics of a word help in text or document analysis. They also help in sentiment analysis of a text. Feature extraction is one of the techniques that are used by recommendation systems. Reviews such as ‘excellent,’ ‘good,’ or ‘great’ for a movie are positive reviews, recognized by a recommender system. The recommender system also tries to identify the features of the text that help in describing the context of a word or a sentence. Then, it makes a group or category of the words that have some common characteristics. Now, whenever a new word arrives, the system categorizes it as per the labels of such groups.",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "22. What are precision and recall?",
        "answer": "The metrics used to test an NLP model are precision, recall, and F1. Also, we use accuracy for evaluating the model’s performance. The ratio of prediction and the desired output yields the accuracy of the model.\nPrecision is the ratio of true positive instances and the total number of positively predicted instances.\n\nRecall is the ratio of true positive instances and the total actual positive instances.",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "23. What is F1 score in NLP?",
        "answer": "F1 score evaluates the weighted average of recall and precision. It considers both false negative and false positive instances while evaluating the model. F1 score is more accountable than accuracy for an NLP model when there is an uneven distribution of class. Let us look at the formula for calculating F1 score:\n\nIf you are preparing for an Artificial Intelligence job interview, go through this top Artificial Intelligence Interview Questions and Answers!\n\nAdvanced NLP Interview Questions",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "24. How to tokenize a sentence using the nltk package?",
        "answer": "Tokenization is a process used in NLP to split a sentence into tokens. Sentence tokenization refers to splitting a text or paragraph into sentences.\nFor tokenizing, we will import sent_tokenize from the nltk package:\n  from nltk.tokenize import sent_tokenize<>\nWe will use the below paragraph for sentence tokenization:\nPara = “Hi Guys. Welcome to Intellipaat. This is a blog on the NLP interview questions and answers.”\n  sent_tokenize(Para)\nOutput:\n  [ 'Hi Guys.' ,\n  'Welcome to Intellipaat. ',\n  'This is a blog on the NLP interview questions and answers. ' ] \nTokenizing a word refers to splitting a sentence into words.\nNow, to tokenize a word, we will import word_tokenize from the nltk package.\n  from nltk.tokenize import word_tokenize\nPara = “Hi Guys. Welcome to Intellipaat. This is a blog on the NLP interview questions and answers.”\n  word_tokenize(Para)\nOutput:\n  [ 'Hi' , 'Guys' , ' . ' , 'Welcome' , 'to' , 'Intellipaat' , ' . ' , 'This' , 'is' ,   'a', 'blog' , 'on' , 'the' , 'NLP' , 'interview' , 'questions' , 'and' , 'answers' , ' . ' ]",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "25. Explain how we can do parsing.",
        "answer": "Parsing is the method to identify and understand the syntactic structure of a text. It is done by analyzing the individual elements of the text. The machine parses the text one word at a time, then two at a time, further three, and so on.\nWhen the machine parses the text one word at a time, then it is a unigram.\nWhen the text is parsed two words at a time, it is a bigram.\nThe set of words is a trigram when the machine parses three words at a time.\nLook at the below diagram to understand unigram, bigram, and trigram.\n\nNow, let’s implement parsing with the help of the nltk package.\n  import nltk\n  text = ”Top 30 NLP interview questions and answers”\nWe will now tokenize the text using word_tokenize.\n  text_token= word_tokenize(text)\nNow, we will use the function for extracting unigrams, bigrams, and trigrams.\n  list(nltk.unigrams(text))\nOutput:\n  [ \"Top 30 NLP interview questions and answer\"]\n \n  list(nltk.bigrams(text))\nOutput:\n  [\"Top 30\", \"30 NLP\", \"NLP interview\", \"interview questions\",   \"questions and\", \"and answer\"]\n \n  list(nltk.trigrams(text))\nOutput:\n  [\"Top 30 NLP\", \"NLP interview questions\", \"questions and answers\"]\nFor extracting n-grams, we can use the function nltk.ngrams and give the argument n for the number of parsers.\n  list(nltk.ngrams(text,n))",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "26. Explain Stemming with the help of an example.",
        "answer": "In Natural Language Processing, stemming is the method to extract the root word by removing suffixes and prefixes from a word.\nFor example, we can reduce ‘stemming’ to ‘stem’ by removing ‘m’ and ‘ing.’\nWe use various algorithms for implementing stemming, and one of them is PorterStemmer.\nFirst, we will import PorterStemmer from the nltk package.\n  from nltk.stem import PorterStemmer\nCreating an object for PorterStemmer\n  pst=PorterStemmer()\n  pst.stem(“running”), pst.stem(“cookies”), pst.stem(“flying”)\nOutput:\n  (‘run’, ‘cooki', ‘fly’ )",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "27. Explain Lemmatization with the help of an example.",
        "answer": "We use stemming and lemmatization to extract root words. However, stemming may not give the actual word, whereas lemmatization generates a meaningful word.\nIn lemmatization, rather than just removing the suffix and the prefix, the process tries to find out the root word with its proper meaning.\nExample: ‘Bricks’ becomes ‘brick,’ ‘corpora’ becomes ‘corpus,’ etc.\nLet’s implement lemmatization with the help of some nltk packages.\nFirst, we will import the required packages.\n  from nltk.stem import wordnet\n  from nltk.stem import WordnetLemmatizer\nCreating an object for WordnetLemmatizer()\n  lemma= WordnetLemmatizer()\n  list = [“Dogs”, “Corpora”, “Studies”]\n  for n in list:\n  print(n + “:” + lemma.lemmatize(n))\nOutput:\n  Dogs: Dog\n  Corpora: Corpus\n  Studies: Study\nInterested in learning Artificial Intelligence? Go through this Artificial Intelligence Tutorial!",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "28. What is Parts-of-speech Tagging?",
        "answer": "The parts-of-speech (POS) tagging is used to assign tags to words such as nouns, adjectives, verbs, and more. The software uses the POS tagging to first read the text and then differentiate the words by tagging. The software uses algorithms for the parts-of-speech tagging. POS tagging is one of the most essential tools in Natural Language Processing. It helps in making the machine understand the meaning of a sentence.\nWe will look at the implementation of the POS tagging using stop words.\nLet’s import the required nltk packages.\n  import nltk\n  from nltk.corpus import stopwords\n  from nltk.tokenize import word_tokenize, sent_tokenize\n  stop_words = set(stopwords.words('english'))\n  txt = \"Sourav, Pratyush, and Abhinav are good friends.\"\nTokenizing using sent_tokenize\n  tokenized_text = sent_tokenize(txt)\nTo find punctuation and words in a string, we will use word_tokenizer and then remove the stop words.\n  for n in tokenized_text:\n  wordsList = nltk.word_tokenize(i)\n  wordsList = [w for w in wordsList if not w instop_words]\nNow, we will use the POS tagger.\n  tagged_words = nltk.pos_tag(wordsList)\n  print(tagged_words)\nOutput:\n  [('Sourav', 'NNP'), ('Pratyush', 'NNP'), ('Abhinav', 'NNP'), ('good',  'JJ'), ('friends', 'NNS')]",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "29. Explain Named Entity Recognition by implementing it.",
        "answer": "Named Entity Recognition (NER) is an information retrieval process. NER helps classify named entities such as monetary figures, location, things, people, time, and more. It allows the software to analyze and understand the meaning of the text. NER is mostly used in NLP, Artificial Intelligence, and Machine Learning. One of the real-life applications of NER is chatbots used for customer support.\nLet’s implement NER using the spaCy package.\nImporting the spaCy package:\n  import spacy\n  nlp = spacy.load('en_core_web_sm')\n  Text = \"The head office of Google is in California\"\n  document = nlp(text)for ent in document.ents:\n  print(ent.text, ent.start_char, ent.end_char, ent.label_)\nOutput:\n  Office 9 15 Place\n  Google 19 25 ORG\n  California 32 41 GPE\nNote: Office 9 15 Place means word starts at 9th position when tokenized and ends at 15, this is inclusive of spaces.",
        "reference": "intellipaat.com",
        "role": "nlp"
    },
    {
        "question": "1. What are the stages in the lifecycle of a natural language processing (NLP) project?",
        "answer": "Following are the stages in the lifecycle of a natural language processing (NLP) project:   Data Collection: The procedure of collecting, measuring, and evaluating correct insights for research using established approved procedures is referred to as data collection.\nData Cleaning: The practice of correcting or deleting incorrect, corrupted, improperly formatted, duplicate, or incomplete data from a dataset is known as data cleaning.\nData Pre-Processing: The process of converting raw data into a comprehensible format is known as data preparation.\nFeature Engineering: Feature engineering is the process of extracting features (characteristics, qualities, and attributes) from raw data using domain expertise.\nData Modeling: The practice of examining data objects and their relationships with other things is known as data modelling. It's utilised to look into the data requirements for various business activities.\nModel Evaluation: Model evaluation is an important step in the creation of a model. It aids in the selection of the best model to represent our data and the prediction of how well the chosen model will perform in the future.\nModel Deployment: The technical task of exposing an ML model to real-world use is known as model deployment.\nMonitoring and Updating: The activity of measuring and analysing production model performance to ensure acceptable quality as defined by the use case is known as machine learning monitoring. It delivers alerts about performance difficulties and assists in diagnosing and resolving the core cause. Data Collection: The procedure of collecting, measuring, and evaluating correct insights for research using established approved procedures is referred to as data collection. Data Collection: Data Cleaning: The practice of correcting or deleting incorrect, corrupted, improperly formatted, duplicate, or incomplete data from a dataset is known as data cleaning. Data Cleaning: Data Pre-Processing: The process of converting raw data into a comprehensible format is known as data preparation. Data Pre-Processing: Feature Engineering: Feature engineering is the process of extracting features (characteristics, qualities, and attributes) from raw data using domain expertise. Feature Engineering: Data Modeling: The practice of examining data objects and their relationships with other things is known as data modelling. It's utilised to look into the data requirements for various business activities. Data Modeling: Model Evaluation: Model evaluation is an important step in the creation of a model. It aids in the selection of the best model to represent our data and the prediction of how well the chosen model will perform in the future. Model Evaluation: Model Deployment: The technical task of exposing an ML model to real-world use is known as model deployment. Model Deployment: Monitoring and Updating: The activity of measuring and analysing production model performance to ensure acceptable quality as defined by the use case is known as machine learning monitoring. It delivers alerts about performance difficulties and assists in diagnosing and resolving the core cause. Monitoring and Updating:",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "2. What do you mean by Lemmatization in NLP?",
        "answer": "The method of mapping all the various forms of a word to its base word (also called “lemma”) is known as Lemmatization. Although this may appear close to the definition of stemming, these are actually different. For instance, the word “better,” after stemming, remains the same. However, upon lemmatization, this should become “good,”. Lemmatization needs greater linguistic knowledge. Modelling and developing efficient lemmatizers still remains an open problem in NLP research. The application of a lemmatizer based on WordNet from NLTK is shown in the code snippet below: from nltk.stem import WordNetLemmatizer\nlemmatizer = WordnetLemmatizer()\nprint(lemmatizer.lemmatize(\"better\", pos=\"a\")) #a is for adjective from nltk.stem import WordNetLemmatizer\nlemmatizer = WordnetLemmatizer()\nprint(lemmatizer.lemmatize(\"better\", pos=\"a\")) #a is for adjective",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "3. What do you mean by Stemming in NLP?",
        "answer": "When we remove the suffixes from a word so that the word is reduced to its base form, this process is called stemming. When the word is reduced to its base form, all the different variants of that word can be represented by the same form (e.g., “bird” and “birds” are both reduced to “bird”). We can do this by using a fixed set of rules. For instance:  if a word ends in “-es,” we can remove the “-es”). Even though these rules might not really make sense as a linguistically correct base form, stemming is usually carried out to match user queries in search engines to relevant documents. And in text classification, is done to reduce the feature space to train our machine learning (ML) models. The code snippet given below depicts the way to use a well known NLP algorithm for stemming called Porter Stemmer using NLTK: from nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\nword1, word2 = \"bikes\", \"revolution\" \nprint(stemmer.stem(word1), stemmer.stem(word2)) from nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\nword1, word2 = \"bikes\", \"revolution\" \nprint(stemmer.stem(word1), stemmer.stem(word2)) This gives “bike” as the stemmed version for “bikes,” but “revolut” as the stemmed form of “revolution,” even though the latter is not linguistically correct. Even if this might not affect the performance of the search engine, a derivation of the correct linguistic form becomes useful in some other cases. This can be done by another process that is closer to stemming, known as lemmatization.",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "4. What are the steps involved in preprocessing data for NLP?",
        "answer": "Here are some common pre-processing steps used in NLP software: Preliminaries: This includes word tokenization and sentence segmentation.\nCommon Steps: Stop word removal, stemming and lemmatization, removing digits/punctuation, lowercasing, etc.\nProcessing Steps: Code mixing, normalization, language detection, transliteration, etc.\nAdvanced Processing: Parts of Speech (POS) tagging, coreference resolution, parsing, etc. Preliminaries: This includes word tokenization and sentence segmentation. Preliminaries: Common Steps: Stop word removal, stemming and lemmatization, removing digits/punctuation, lowercasing, etc. Common Steps: Processing Steps: Code mixing, normalization, language detection, transliteration, etc. Processing Steps: Advanced Processing: Parts of Speech (POS) tagging, coreference resolution, parsing, etc. Advanced Processing:  ",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "5. What do you mean by Text Extraction and Cleanup?",
        "answer": "The process of extracting raw text from the input data by getting rid of all the other non-textual information, such as markup, metadata, etc., and converting the text to the required encoding format is called text extraction and cleanup. Usually, this depends on the format of available data for the required project. text extraction and cleanup Following are the common ways used for Text Extraction in NLP: Named Entity Recognition\nSentiment Analysis\nText Summarization\nAspect Mining\nTopic Modeling Named Entity Recognition Sentiment Analysis Text Summarization Aspect Mining Topic Modeling",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "6. How can data be obtained for NLP projects?",
        "answer": "There are multiple ways in which data can be obtained for NLP projects. Some of them are as follows: Using publicly available datasets: Datasets for NLP purposes are available on websites like Kaggle as well as Google Datasets.\nBy using data augmentation: These are used to create additional datasets from existing datasets.\nScraping data from the web: Using coding in Python or other languages once can scrape data from websites that are usually not readily available in a structured form. Using publicly available datasets: Datasets for NLP purposes are available on websites like Kaggle as well as Google Datasets. Using publicly available datasets: By using data augmentation: These are used to create additional datasets from existing datasets. By using data augmentation: Scraping data from the web: Using coding in Python or other languages once can scrape data from websites that are usually not readily available in a structured form. Scraping data from the web:",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "7. What is meant by data augmentation? What are some of the ways in which data augmentation can be done in NLP projects?",
        "answer": "NLP has some methods through which we can take a small dataset and use that in order to create more data. This is called data augmentation. In this, we use language properties to create text that is syntactically similar to the source text data. Some of the ways in which data augmentation can be done in NLP projects are as follows: Replacing entities\nTF-IDF–based word replacement\nAdding noise to data\nBack translation\nSynonym replacement\nBigram flipping Replacing entities TF-IDF–based word replacement Adding noise to data Back translation Synonym replacement Bigram flipping",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "8. How do Conversational Agents work?",
        "answer": "The following NLP components are used in Conversational Agents: Speech Recognition and Synthesis: In the first stage, speech recognition helps convert speech signals to their phonemes, and are then transcribed as words.\nNatural Language Understanding (NLU): Here, the transcribed text from stage one is further analysed through AI techniques within the natural language understanding system. Certain NLP tasks such as Named Entity Recognition, Text Classification, Language modelling, etc. come into play here.\nDialog Management: Once the needed information from text is extracted, we move on to the stage of understanding the user’s intent. The user’s response can then be classified by using a text classification system as a pre-defined intent. This helps the conversational agent in figuring out what is actually being asked.\nGenerating Response: Based on the above stages, the agent generates an appropriate response that is based on a semantic interpretation of the user’s intent. Speech Recognition and Synthesis: In the first stage, speech recognition helps convert speech signals to their phonemes, and are then transcribed as words. Speech Recognition and Synthesis: Natural Language Understanding (NLU): Here, the transcribed text from stage one is further analysed through AI techniques within the natural language understanding system. Certain NLP tasks such as Named Entity Recognition, Text Classification, Language modelling, etc. come into play here. Natural Language Understanding (NLU): Dialog Management: Once the needed information from text is extracted, we move on to the stage of understanding the user’s intent. The user’s response can then be classified by using a text classification system as a pre-defined intent. This helps the conversational agent in figuring out what is actually being asked. Dialog Management: Generating Response: Based on the above stages, the agent generates an appropriate response that is based on a semantic interpretation of the user’s intent. Generating Response:  ",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "9. What are the different approaches used to solve NLP problems?",
        "answer": "There are multiple approaches to solving NLP problems. These usually come in 3 categories: Heuristics\nMachine learning\nDeep Learning Heuristics Machine learning Deep Learning",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "10. What are some of the common NLP tasks?",
        "answer": "Some of the common tasks of NLP include: Machine Translation: This helps in translating a given piece of text from one language to another.\nText Summarization: Based on a large corpus, this is used to give a short summary that gives an idea of the entire text in the document.\nLanguage Modeling: Based on the history of previous words, this helps uncover what the further sentence will look like. A good example of this is the auto-complete sentences feature in Gmail.\nTopic Modelling: This helps uncover the topical structure of a large collection of documents. This indicates what topic a piece of text is actually about.\nQuestion Answering: This helps prepare answers automatically based on a corpus of text, and on a question that is posed.\nConversational Agent: These are basically voice assistants that we commonly see such as Alexa, Siri, Google Assistant, Cortana, etc.\nInformation Retrieval: This helps in fetching relevant documents based on a user’s search query.\nInformation Extraction: This is the task of extracting relevant pieces of information from a given text, such as calendar events from emails.\nText Classification: This is used to create a bucket of categories of a given text, based on its content. This is used in a wide variety of AI-based applications such as sentiment analysis and spam detection. Machine Translation: This helps in translating a given piece of text from one language to another. Machine Translation: Text Summarization: Based on a large corpus, this is used to give a short summary that gives an idea of the entire text in the document. Text Summarization: Language Modeling: Based on the history of previous words, this helps uncover what the further sentence will look like. A good example of this is the auto-complete sentences feature in Gmail. Language Modeling: Topic Modelling: This helps uncover the topical structure of a large collection of documents. This indicates what topic a piece of text is actually about. Topic Modelling: Question Answering: This helps prepare answers automatically based on a corpus of text, and on a question that is posed. Question Answering: Conversational Agent: These are basically voice assistants that we commonly see such as Alexa, Siri, Google Assistant, Cortana, etc. Conversational Agent: Information Retrieval: This helps in fetching relevant documents based on a user’s search query. Information Retrieval: Information Extraction: This is the task of extracting relevant pieces of information from a given text, such as calendar events from emails. Information Extraction: Text Classification: This is used to create a bucket of categories of a given text, based on its content. This is used in a wide variety of AI-based applications such as sentiment analysis and spam detection. Text Classification:   Common NLP Tasks in order of Difficulty",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "1. What do you mean by TF-IDF in Natural language Processing?",
        "answer": "TF-IDF also called Term Frequency-Inverse Document Frequency helps us get the importance of a particular word relative to other words in the corpus. It's a common scoring metric in information retrieval (IR) and summarization. TF-IDF converts words into vectors and adds semantic information, resulting in weighted unusual words that may be utilised in a variety of NLP applications. Term Frequency-Inverse Document Frequency",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "2. What do you mean by perplexity in NLP?",
        "answer": "It's a statistic for evaluating the effectiveness of language models. It is described mathematically as a function of the likelihood that the language model describes a test sample. The perplexity of a test sample X = x1, x2, x3,....,xn is given by, PP(X)=P(x1,x2,…,xN)-1N The total number of word tokens is N. The more perplexing the situation, the less information the language model conveys. Conclusion One of the most important reasons for NLP is that it allows computers to converse with people in natural language. Other language-related activities are also scaled. Computers can now hear, analyse, quantify, and identify which parts of speech are significant thanks to Natural Language Processing (NLP). NLP has a wide range of applications, including chatbots, sentiment analysis, and market intelligence. Since its introduction, NLP has grown in popularity. Today, devices like Amazon's Alexa are extensively used all over the world. And, for businesses, business intelligence and consumer monitoring are quickly gaining traction and will soon rule the industry. References and Resources: References and Resources: Natural Language Processing with Python – Book by Edward Loper, Ewan Klein, and Steven Bird (Published by: O'Reilly Media, Inc.)\nPractical Natural Language Processing – By Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, Harshit Surana (Published by: O'Reilly Media, Inc.)\nNatural Language Processing in Action: Understanding, Analyzing, and Generating Text with Python – Book by Cole Howard, Hannes Hapke, and Hobson Lane Natural Language Processing with Python – Book by Edward Loper, Ewan Klein, and Steven Bird (Published by: O'Reilly Media, Inc.) Practical Natural Language Processing – By Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, Harshit Surana (Published by: O'Reilly Media, Inc.) Natural Language Processing in Action: Understanding, Analyzing, and Generating Text with Python – Book by Cole Howard, Hannes Hapke, and Hobson Lane",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "3. What is the meaning of N-gram in NLP?",
        "answer": "Text N-grams are commonly used in text mining and natural language processing. They're essentially a collection of co-occurring words within a specific frame, and when computing the n-grams, you usually advance one word (although you can move X words forward in more advanced scenarios).",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "4. What is the meaning of Pragmatic Analysis in NLP?",
        "answer": "Pragmatic Analysis is concerned with outside word knowledge, which refers to information that is not contained in the documents and/or questions. The many parts of the language that require real-world knowledge are derived from a pragmatics analysis that focuses on what was described and reinterpreted by what it truly meant.",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "5. What do you mean by Masked language modelling?",
        "answer": "Masked language modelling is an NLP technique for extracting the output from a contaminated input. Learners can use this approach to master deep representations in downstream tasks. Using this NLP technique, you may predict a word based on the other words in the sentence. The following is the process for Masked language modelling: Our text is tokenized. We start with text tokenization, just as we would with transformers.\nMake a tensor of labels. We're using a labels tensor to calculate loss against — and optimise towards — as we train our model.\nTokens in input ids are masked. We can mask a random selection of tokens now that we've produced a duplicate of input ids for labels.\nMake a loss calculation. We use our model to process the input ids and labels tensors and determine the loss between them. Our text is tokenized. We start with text tokenization, just as we would with transformers. Make a tensor of labels. We're using a labels tensor to calculate loss against — and optimise towards — as we train our model. Tokens in input ids are masked. We can mask a random selection of tokens now that we've produced a duplicate of input ids for labels. Make a loss calculation. We use our model to process the input ids and labels tensors and determine the loss between them.",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "6. What do you mean by Autoencoders?",
        "answer": "A network that is used for learning a vector representation of the input in a compressed form, is called an autoencoder. It is a type of unsupervised learning since labels aren’t needed for the process. This is mainly used to learn the mapping function from the input. In order to make the mapping useful, the input is reconstructed from the vector representation. After training is complete, the vector representation that we get helps encode the input text as a dense vector. Autoencoders are generally used to make feature representations. In the figure below, the hidden layer depicts a compressed representation of the source data that captures its essence. The input representation is reconstructed by the output layer called the decoder.  ",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "7. Explain the pipeline for Information extraction (IE) in NLP.",
        "answer": "In comparison to text classification, the typical pipeline for IE necessitates more fine-grained NLP processing. For example, we'd need to know the part-of-speech tags of words to identify named entities (people, organisations, etc.). We would require coreference resolution to connect various references to the same entity (e.g., Albert Einstein, Einstein, the scientist, he, etc.). It's worth noting that none of these stages are required for creating a text classification system. As a result, IE is a more NLP-intensive operation than text categorization. Not all steps in the pipeline are required for all IE jobs, as shown in the diagram, and the figure shows which IE tasks necessitate which degrees of analysis. Other than named entity recognition, all other IE tasks require deeper NLP pre-processing followed by models developed for those specific tasks. Key phrase extraction is the task that requires the least amount of NLP processing (some algorithms also do POS tagging before extracting key phrases), whereas all other IE tasks require deeper NLP pre-processing followed by models developed for those specific tasks. Standard evaluation sets are often used to assess IE tasks in terms of precision, recall, and F1 scores. Because of the various levels of NLP pre-processing required, the accuracy of these processing steps has an impact on IE jobs. All of these factors should be considered when collecting relevant training data and, if necessary, training our own models for IE.  ",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "8. What are some metrics on which NLP models are evaluated?",
        "answer": "The following are some metrics on which NLP models are evaluated: Accuracy: When the output variable is categorical or discrete, accuracy is used. It is the percentage of correct predictions made by the model compared to the total number of predictions made.\nPrecision: Indicates how precise or exact the model's predictions are, i.e., how many positive (the class we care about) examples can the model correctly identify given all of them?\nRecall: Precision and recall are complementary. It measures how effectively the model can recall the positive class, i.e., how many of the positive predictions it generates are correct.\nF1 score: This metric combines precision and recall into a single metric that also represents the trade-off between accuracy and recall, i.e., completeness and exactness.\n(2 Precision Recall) / (Precision + Recall) is the formula for F1.\nAUC: As the prediction threshold is changed, the AUC captures the number of correct positive predictions versus the number of incorrect positive predictions. Accuracy: When the output variable is categorical or discrete, accuracy is used. It is the percentage of correct predictions made by the model compared to the total number of predictions made. Accuracy: Precision: Indicates how precise or exact the model's predictions are, i.e., how many positive (the class we care about) examples can the model correctly identify given all of them? Precision: Recall: Precision and recall are complementary. It measures how effectively the model can recall the positive class, i.e., how many of the positive predictions it generates are correct. Recall: F1 score: This metric combines precision and recall into a single metric that also represents the trade-off between accuracy and recall, i.e., completeness and exactness.\n(2 Precision Recall) / (Precision + Recall) is the formula for F1. F1 score:  AUC: As the prediction threshold is changed, the AUC captures the number of correct positive predictions versus the number of incorrect positive predictions. AUC:",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "9. What is the difference between NLP and NLU?",
        "answer": "Natural Language Processing (NLP) Natural Language Understanding (NLU)\nNLP is a system that manages end-to-end conversations between computers and people at the same time. NLU aids in the solving of Artificial Intelligence's complex problems.\nHumans and machines are both involved in NLP. NLU allows machines to interpret unstructured inputs by transforming them into structured text.\nNLP focuses on interpreting language in its most literal sense, such as what was said. NLU, on the other hand, concentrates on extracting context and meaning, or what was meant.\nNLP can parse text-based on grammar, structure, typography, and point of view. It'll be NLU that helps the machine deduce the meaning behind the language content. Natural Language Processing (NLP) Natural Language Understanding (NLU)\nNLP is a system that manages end-to-end conversations between computers and people at the same time. NLU aids in the solving of Artificial Intelligence's complex problems.\nHumans and machines are both involved in NLP. NLU allows machines to interpret unstructured inputs by transforming them into structured text.\nNLP focuses on interpreting language in its most literal sense, such as what was said. NLU, on the other hand, concentrates on extracting context and meaning, or what was meant.\nNLP can parse text-based on grammar, structure, typography, and point of view. It'll be NLU that helps the machine deduce the meaning behind the language content. Natural Language Processing (NLP) Natural Language Understanding (NLU) Natural Language Processing (NLP) Natural Language Understanding (NLU) Natural Language Processing (NLP) Natural Language Understanding (NLU) NLP is a system that manages end-to-end conversations between computers and people at the same time. NLU aids in the solving of Artificial Intelligence's complex problems.\nHumans and machines are both involved in NLP. NLU allows machines to interpret unstructured inputs by transforming them into structured text.\nNLP focuses on interpreting language in its most literal sense, such as what was said. NLU, on the other hand, concentrates on extracting context and meaning, or what was meant.\nNLP can parse text-based on grammar, structure, typography, and point of view. It'll be NLU that helps the machine deduce the meaning behind the language content. NLP is a system that manages end-to-end conversations between computers and people at the same time. NLU aids in the solving of Artificial Intelligence's complex problems. NLP is a system that manages end-to-end conversations between computers and people at the same time. NLU aids in the solving of Artificial Intelligence's complex problems. Humans and machines are both involved in NLP. NLU allows machines to interpret unstructured inputs by transforming them into structured text. Humans and machines are both involved in NLP. NLU allows machines to interpret unstructured inputs by transforming them into structured text. NLP focuses on interpreting language in its most literal sense, such as what was said. NLU, on the other hand, concentrates on extracting context and meaning, or what was meant. NLP focuses on interpreting language in its most literal sense, such as what was said. NLU, on the other hand, concentrates on extracting context and meaning, or what was meant. NLP can parse text-based on grammar, structure, typography, and point of view. It'll be NLU that helps the machine deduce the meaning behind the language content. NLP can parse text-based on grammar, structure, typography, and point of view. It'll be NLU that helps the machine deduce the meaning behind the language content.",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "10. What is Latent Semantic Indexing (LSI) in NLP?",
        "answer": "Latent Semantic Indexing (LSI), also known as Latent Semantic Analysis, is a mathematical method for improving the accuracy of information retrieval. It aids in the discovery of hidden(latent) relationships between words (semantics) by generating a set of various concepts associated with the terms of a phrase in order to increase information comprehension. Singular value decomposition is the NLP technique utilised for this aim. It's best for working with small groups of static documents. Latent Semantic Indexing",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "11. What do you mean by Parts of Speech (POS) tagging in NLP?",
        "answer": "A Part-Of-Speech Tagger (POS Tagger) reads the text in a language and assigns parts of speech to each word (and other tokens), such as noun, verb, adjective, and so on. To label terms in text bodies, PoS taggers employ an algorithm. With tags like \"noun-plural\" or even more complicated labels, these taggers create more complex categories than those stated as basic PoS.",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "12. What do you mean by a Bag of Words (BOW)?",
        "answer": "The Bag of Words model is a popular one that uses word frequency or occurrences to train a classifier. This methodology generates a matrix of occurrences for documents or phrases, regardless of their grammatical structure or word order. Bag of Words A bag-of-words is a text representation that describes the frequency with which words appear in a document. It entails two steps: A list of terms that are well-known.\nA metric for determining the existence of well-known terms. A list of terms that are well-known. A metric for determining the existence of well-known terms. Because any information about the sequence or structure of words in the document is deleted, it is referred to as a \"bag\" of words. The model simply cares about whether or not recognised terms appear in the document, not where they appear.",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "13. Explain how parsing is done in NLP.",
        "answer": "Parsing is the process of identifying and understanding a text's syntactic structure. It is accomplished by examining the text's constituent pieces. The machine parses each word one by one, then two by two, three by three, and so on. It's a unigram when the system parses the text one word at a time. A bigram is a text that is parsed two words at a time. When the machine parses three words at a time, the set of words is called a trigram. trigram The following points will help us comprehend the importance of parsing in NLP: Any syntax errors are reported by the parser.\nIt aids in the recovery of often occurring errors so that the remainder of the programme can be processed.\nA parser is used to generate the parse tree.\nThe parser is used to construct a symbol table, which is crucial in NLP.\nIn addition, a Parser is utilised to generate intermediate representations (IR). Any syntax errors are reported by the parser. It aids in the recovery of often occurring errors so that the remainder of the programme can be processed. A parser is used to generate the parse tree. The parser is used to construct a symbol table, which is crucial in NLP. In addition, a Parser is utilised to generate intermediate representations (IR).",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "14. What are the steps to follow when building a text classification system?",
        "answer": "When creating a text classification system, the following steps are usually followed: Gather or develop a labelled dataset that is appropriate for the purpose.\nDecide on an evaluation metric after splitting the dataset into two (training and test) or three parts: training, validation (i.e., development), and test sets (s).\nConvert unprocessed text into feature vectors.\nUtilize the feature vectors and labels from the training set to train a classifier.\nBenchmark the model's performance on the test set using the evaluation metric(s) from Step 2.\nDeploy the model and track its performance to serve a real-world use case. Gather or develop a labelled dataset that is appropriate for the purpose. Decide on an evaluation metric after splitting the dataset into two (training and test) or three parts: training, validation (i.e., development), and test sets (s). Convert unprocessed text into feature vectors. Utilize the feature vectors and labels from the training set to train a classifier. Benchmark the model's performance on the test set using the evaluation metric(s) from Step 2. Deploy the model and track its performance to serve a real-world use case.  ",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "15. What is an ensemble method in NLP?",
        "answer": "An ensemble approach is a methodology that derives an output or makes predictions by combining numerous independent similar or distinct models/weak learners. An ensemble can also be created by combining various models such as random forest, SVM, and logistic regression. Bias, variance, and noise, as we all know, have a negative impact on the mistakes and predictions of any machine learning model. Ensemble approaches are employed to overcome these drawbacks.",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "16. Explain the concept of Feature Engineering.",
        "answer": "After a variety of pre-processing procedures and their applications, we need a way to input the pre-processed text into an NLP algorithm later when we employ ML methods to complete our modelling step. The set of strategies that will achieve this goal is referred to as feature engineering. Feature extraction is another name for it. The purpose of feature engineering is to convert the text's qualities into a numeric vector that NLP algorithms can understand. This stage is called \"text representation\".",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "17. What is the meaning of Text Normalization in NLP?",
        "answer": "Consider a situation in which we’re operating with a set of social media posts to find information events. Social media textual content may be very exceptional from the language we’d see in, say, newspapers. A phrase may be spelt in multiple ways, such as in shortened forms, (for instance, with and without hyphens), names are usually in lowercase, and so on. When we're developing NLP tools to work with such kinds of data, it’s beneficial to attain a canonical representation of textual content that captures these kinds of variations into one representation. This is referred to as text normalization. Converting all text to lowercase or uppercase, converting digits to text (e.g., 7 to seven), expanding abbreviations, and so on are some frequent text normalisation stages.",
        "reference": "interviewbit.com",
        "role": "nlp"
    },
    {
        "question": "1) What is the full form of NLP? / What is Natural Language Processing?",
        "answer": "NLP stands for \"Natural Language Processing\". NLP is a field of computer science that deals with communication between computer systems and humans. This technique uses Artificial Intelligence and Machine Learning to create automated software that helps understand the human spoken languages and extract useful information from the data gathered from the audio.\nThe techniques used in NLP allow computer systems to process and interpret data in the form of natural languages. It designs algorithms that can extract meaning from large datasets in audio or text format by applying machine learning algorithms. In other words, we can say that NLP is software that uses artificial intelligence and machine learning algorithms to understand natural languages or the way human beings read and write in a language and extracts required information from such data.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "2) What are some real-life applications / real-world examples of Natural Language Processing (NLP)?",
        "answer": "Some real-life applications of NLP or Natural Language Processing are as follows:\nSpelling/Grammar Checking Apps: Spelling and grammar checking applications are real-life examples of Natural Language Processing. These apps are mainly used in mobile applications and websites that facilitate users to correct grammar mistakes in the entered text rely on NLP algorithms. They also recommend the best possible substitutes that the users might type. This is possible because of specific NLP models being used in the backend.\nGoogle Translate: Google Translate is the most famous application of Natural Language Processing. Using this, you can convert your written or spoken sentences into any language. You can also get the correct pronunciation and meaning of a word by using Google Translate. The Google Translate application uses some advanced techniques of Natural Language Processing to provide translation of sentences into various languages.\nChatbots apps: Chatbots applications provide a better customer support service. Many websites and companies use this to offer customer support through these virtual bots that chat with the user and resolve their problems. Many companies use chatbots for 24/7 service to resolve the basic queries of customers. Generally, it filters the basic issues that do not require an interaction with the companies' customer executives. It makes the customers feel that the customer support team quickly attends them. If a chatbot cannot resolve any user's query, it forwards it to the support team while still engaging the customer. Chatbots also make companies capable of building cordial relations with customers. These all are only possible because of Natural Language Processing.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "3) What are the most used NLP (Natural Language Processing) Terminologies?",
        "answer": "Following is the list of most used NLP (Natural Language Processing) Terminologies:\nADVERTISEMENT\nPreprocessing: This is a method used to remove unwanted text or noise from the given text and make it \"clean.\" It is the first step of any NLP task. s\nDocuments: Documents are the body of text and are collectively used to form a corpus.\nCorpus, or Corpora (Plural): It is a collection of text of similar type, for example, movie reviews, social media posts, etc.\nVocabulary: It is a group of terms used in a text or speech.\nOut of Vocabulary: It specifies the terms not included in the vocabulary. We put the terms created during the model's training in this category.\nTokenization: It is used in NLP to break down large sets of text into small parts for easy readability and understanding. Here, the small parts are referred to as 'text' and provide a piece of meaningful information.\nN-grams: It specifies the continuous sequence (similar to the power set in number theory) of n-tokens of a given text.\nParts of Speech (POS): It specifies the word's functions, such as a noun, verb, etc.\nParts of Speech Tagging: It is the process of tagging words in the sentences into different parts of speech.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "4) What are the most used NLP (Natural Language Processing) Terminologies?",
        "answer": "Following is the list of most used NLP (Natural Language Processing) Terminologies:\nEmbeddings (Word): This process is used to embed each token as a vector and then pass it into a machine learning model. We can apply embeddings also on phrases and characters, apart from words.\nStop Words: These are used to remove the unwanted text from further text processing, for example, a, to, can, etc.\nTransformers: Transformers are deep learning architectures that can parallelize computations. They are used to learn long-term dependencies.\nNormalization: This is a process of mapping similar terms to a canonical form, i.e., a single entity.\nLemmatization: Lemmatization is a type of normalization used to group similar terms to their base form according to their parts of speech. For example, talking and talking can be mapped to a single term, talk.\nStemming: Stemming is also a type of normalization similar to lemmatization. But, it is different in the term that it segregates the words without the parts of speech tags. It is faster than lemmatization and also be more precise in some cases.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "5) What are some of the major components of Natural Language Processing?",
        "answer": "Following is a list of some of the major components of Natural Language Processing:\nEntity extraction: It is used for segmenting a sentence to identify and extract entities, such as a person (real or fictional), organization, geographies, events, etc. 85\nPragmatic analysis: Pragmatic analysis extracts information from the input text. It is part of the process of data extraction.\nSyntactic analysis: Syntactic analysis is used for the proper ordering of words.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "6) What do you understand by Dependency Parsing in NLP or Natural Language Processing?",
        "answer": "In Natural Language Processing, Dependency Parsing is a process of assigning syntactic structure to a sentence and identifying its dependency parses. This is an important process to understand the correlations between the \"head\" words in the syntactic structure. That's why it is also known as syntactic parsing.\nThe process of dependency parsing becomes a little complex if there are more sentences that have more than one dependency parses. Multiple parse trees are known as ambiguities. The main task of dependency parsing is to resolve these ambiguities to assign a syntactic structure to a sentence effectively. It is also used in semantic analysis apart from syntactic structuring.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "7) What are some most common areas of usage of Natural Language Processing?",
        "answer": "Following is a list of some most common areas of usage of Natural Language Processing:\nSemantic Analysis\nText classification\nAutomatic summarization\nQuestioning Answering\nSome real-life examples of Natural Language Processing are chatbots, IOS Siri, Google Assistant, Amazon echo, Spelling, grammar checking apps, and Google translate.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "8) What do you understand by NLTK in Natural Language Processing?",
        "answer": "In Natural Language Processing, NLTK stands for Natural Language Toolkit. It is a Python library used to process data in human spoken languages. NLTK facilitates developers to apply parsing, tokenization, lemmatization, stemming techniques, and more to understand natural languages. It is also used for categorizing text, parsing linguistic structure, analyzing documents, etc.\nFollowing is the list of some libraries of the NLTK package that are often used in NLP:\nDefaultTagger\nUnigramTagger\nRegexpTagger\nbackoff_tagger\nSequentialBackoffTagger\nUnigramTagger\nBigramTagger\nTrigramTagger\ntreebank\nwordnet\nFreqDist\nPatterns etc.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "9) What is the use of TF-IDF? Why is it used in Natural language Processing?",
        "answer": "In Natural language Processing, tf-idf, TF-IDF, or TFIDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic used to specify how important a word is to a document in a collection or the collection of a set.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "10) What is the difference between formal and natural languages?",
        "answer": "The main difference between a formal language and a natural language is that a formal language is a collection of strings. Each string contains symbols from a finite set called alphabets. On the other hand, a natural language is a language that humans use to speak. This is completely different from a formal language as it contains fragments of words and pause words like uh, um, etc.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "11) What are the tools used for training NLP models?",
        "answer": "The most common tools used for training NLP models are NLTK, spaCY, PyTorch-NLP, openNLP, etc.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "12) What do you understand by information extraction? What are the various models of information extraction?",
        "answer": "In Natural Language Processing, information extraction is a technique of automatically extracting structured information from unstructured sources to get useful information. It extracts information such as attributes of entities, the relationship between different entities, and more.\n\nFollowing is a list of various models of information extraction in Natural Language Processing:\nFact Extraction Module\nEntity Extraction Module\nSentiment Analysis Module\nTagger Module\nRelation Extraction Module\nNetwork Graph Module\nDocument Classification and Language Modeling Module",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "13) What are the stop words in Natural Language Processing?",
        "answer": "In Natural Language Processing, stop words are regarded as useless data for a search engine. It includes the words like articles, prepositions, was, were, is, am, the, a, an, how, why, and many more. The algorithm used in Natural Language Processing eliminates the stop words to understand and analyze the meaning of the sentences. Eliminating the stop words is one of the most important tasks for search engines to process data.\nSoftware developers design the algorithms of search engines so that they ignore the use of stop words and only show the relevant search result for a query.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "14) What is Bag of Words in Natural Language Processing?",
        "answer": "Bag of Words is a commonly used model in Natural Language Processing that depends on word frequencies or occurrences to train a classifier. This model creates an occurrence matrix for documents or sentences without depending on their grammatical structure or word order.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "15) What do you understand by semantic analysis? What are the techniques used for semantic analysis?",
        "answer": "Semantic analysis is a process that makes a machine understand the meaning of a text. It uses several algorithms to interpret the words in sentences. It is also used to understand the structure of a sentence.\nFollowing are the techniques used for semantic analysis:\n\nNamed entity recognition: This technique is used to specify the process of information retrieval that helps identify the entities like the name of a person, organization, place, time, emotion, etc.\nNatural language generation: This technique specifies a process used by the software to convert the structured data into human spoken languages. By using natural language generation, organizations can automate content for custom reports.\nWord sense disambiguation: It technique is used to identify the sense of a word used in different sentences.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "16) What is pragmatic ambiguity in NLP?",
        "answer": "Pragmatic ambiguity is used to specify words with more than one meaning, and they can be used in any sentence depending on the context. In pragmatic ambiguity, words have multiple interpretations.\nPragmatic ambiguity occurs when the meaning of the words is not specific. For example, if a word gives different meanings. Due to pragmatic ambiguity, a sentence can have multiple interpretations. Sometimes, we come across sentences that have words with multiple meanings, making the sentence open to interpretation.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "17) What is Latent Semantic Indexing (LSI)? What is the use of this technique?",
        "answer": "LSI or Latent Semantic Indexing is a mathematical technique used in Natural Language Processing. This technique is used to improve the accuracy of the information retrieval process. The LSI algorithm is designed to allow machines to detect the latent correlation between semantics.\nThe machines generate various concepts to enhance information understanding. The technique used for information understanding is called singular value decomposition. It is mainly used to handle static and unstructured data. This is one of the best-suited models to identify components and group them according to their types.\nLatent Semantic Indexing or LSI is based on a principle that specifies that words carry a similar meaning when used in a similar context. The computational LSI models are slow compared to other models, but they can improve a text or document's analysis and understanding.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "18) What do you understand by MLM in Natural Language Processing?",
        "answer": "In Natural Language Processing, MLM is a term that stands for Masked Language Model. It helps learners understand deep representations in downstream tasks by taking the output from the corrupt input.\nThis model is mainly used to predict the words used in a sentence.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "19) What are the most commonly used models to reduce data dimensionality in NLP?",
        "answer": "The most commonly used models to reduce the dimensionality of data in NLP are TF-IDF, Word2vec/Glove, LSI, Topic Modelling, Elmo Embeddings, etc.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "20) What is Lemmatization in Natural Language Processing?",
        "answer": "Lemmatization is a process of doing things properly using a vocabulary and morphological analysis of words. It is mainly used to remove the inflectional endings only and return the base or dictionary form of a word, known as the lemma. It is just like cutting down your beard or shaving to get the original shape of your face.\nFor example: girl's = girl, bikes= bike, leaders= leader etc.\nSo, the main task of Lemmatization is to identify and return the root or original words of the sentence to explore various additional information.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "21) What is Stemming in Natural Language Processing?",
        "answer": "Stemming is a process of extracting the base form of a word by removing the affixes from them. It is just like cutting down the branches of a tree to its stems.\nFor example: After stemming, the words go, goes, and going would be 'go'.\nSearch engines use stemming for indexing the words. It facilitates them to store only the stems rather than storing all forms of a word. By using stemming, the search engines reduce the size of the index and increase the retrieval accuracy.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "22) What is the difference between Stemming and Lemmatization in NLP?",
        "answer": "Stemming and Lemmatization are both the text normalization techniques used in Natural language Processing. Both are used to prepare text, words, and documents for further processing. They seem very similar techniques, but there are quite differences between them. Let's see the main differences between them:\nStemming Lemmatization\nStemming is the process of extracting the base form of a word by removing the affixes from them. It produces the morphological variants of a root/base word. Stemming programs are commonly known as stemming algorithms or stemmers. Lemmatization is a more advanced process and looks beyond word reduction, just like stemming. It considers a full vocabulary of a language and applies a morphological analysis to words.\nFor example, the lemma of 'went' is 'go', and the lemma of 'mice' is 'mouse'.\nStemming is not as much informative as Lemmatization. It is a somewhat crude method for cataloging related words. It essentially cuts letters from the end until the stem is reached. Lemmatization is much more informative than simple Stemming; that is why Spacy has opted to only have Lemmatization available instead of Stemming.\nStemming is not as efficient as Lemmatization. This method works fairly well in most cases, but unfortunately, English has many exceptions requiring a more sophisticated process. Lemmatization is more efficient than Stemming as it works well in exceptional words.\nFollowing are some examples of Stemming:\nrun: run\nrunner: runner\nrunning: run\nran: ran\nruns: run\neasily: easili\nfairly: fair etc. Following are some examples of Lemmatization:\nrun: run\nrunner: run\nrunning: run\nran: run\nruns: run\ngoes: go\ngo: go\nwent: go\nsaw: see\nmice: mouse",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "23) Which NLP techniques use a lexical knowledge base to obtain the correct base form of the words?",
        "answer": "The NLP techniques that use a lexical knowledge base to obtain the correct base form of the words are Lemmatization and stemming.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "24) What is tokenization in Natural Language Processing?",
        "answer": "In Natural Language Processing, tokenization is a method of dividing the text into various tokens. These tokens are the form of the words, just like a word forms into a sentence. In NLP, the program computers process large amounts of natural language data. These large amounts of natural language data have to be cut into shorter forms. So, tokenization is an important step in NLP that cuts the text into minimal units for further processing.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "25) What are some open-source libraries used in NLP?",
        "answer": "Some popular open-source libraries used in NLP are NLTK (Natural Language ToolKit), SciKit Learn, Textblob, CoreNLP, spaCY, Gensim, etc.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "26) What are the key differences between NLP and NLU?",
        "answer": "Following is the list of key differences between NLP and NLU:\nNLP NLU\nNLP is a short form of Natural Language Processing. NLU is a short form of Natural Language Understanding.\nNLP or Language Processing is used to create a system that can make and establish communication between humans and computers. NLU or Natural Language Understanding provides techniques that can solve complicated problems related to machine understanding.\nIt includes all the techniques required for the interaction between computers and humans. It converts the uncategorized input data into a structured format and allows the computers to understand the data.\nIt includes the techniques focused on analyzing \"what is said?\" It includes the techniques to understand \"what is meant?\"",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "27) What are the key differences between NLP (Natural Language Processing) and CI (Conversational Interface)?",
        "answer": "Following is the list of key differences between NLP (Natural Language Processing) and CI (Conversational Interface):\nNatural Language Processing (NLP) Conversational Interface (CI)\nThe full form of NLP is Natural Language Processing. The full form of CI is Conversational Interface.\nThe main focus of NLP is to make computers understand and learn how the normal human being languages' concepts work. The main and only focus of CI is to provide users with an interface to interact with.\nNatural Language Processing uses AI technology to identify, understand, and interpret users' requests through languages. CI or Conversational Interface uses voice, chat, videos, images, and other conversational aid to create the user interface for communication.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "28) What do you understand by Pragmatic Analysis?",
        "answer": "Pragmatic analysis is an important task used in Natural Language Processing for interpreting knowledge lying outside a given document. It is mainly implemented to focus on exploring a different aspect of the document or text in a language. It requires a comprehensive knowledge of the real world to make software applications capable of critical interpretation of the real-world data to know the actual meaning of sentences and words.\nFor example, see the following sentence:\n'Do you know what time it is?'\nThis sentence can be used to ask for knowing the time or for yelling at someone to make them note the time. It completely depends on the context in which this sentence is used.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "29) What are the best open sources of NLP Tools available in the market?",
        "answer": "Some of the best open sources NLP tools available in the market are:\nSpaCy\nTextBlob\nTextacy\nNatural language Toolkit (NLTK)\nRetext\nNLP.js\nStanford NLP\nCogcompNLP etc.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "30) How can you differentiate Artificial Intelligence, Machine Learning, and Natural Language Processing?",
        "answer": "Following are the key differences between Artificial Intelligence, Machine Learning, and, Natural Language Processing:\nArtificial Intelligence Machine Learning Natural Language Processing\nArtificial Intelligence is a technique used to create smarter machines and computers. Machine Learning is a term used for systems that learn from experience. Natural Language Processing or NLP is the set of systems that can understand the languages used by humans and process these languages to make them understood by computers.\nArtificial Intelligence requires human intervention. Without human intervention, it is not possible to create intelligent machines. Machine Learning doesn't require human intervention. It purely involves the working of computers and machines. Natural Language Processing uses both computer and human languages to work properly.\nArtificial Intelligence is a broader concept than Machine Learning. It includes a lot of working fields. Machine Learning is a narrow concept and is a subset of Artificial Intelligence. Natural Language Processing uses the concept of both Artificial Intelligence and Machine Learning to make the tools that can process human language and make it understandable by machines.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "31) What do you understand by POS tagging?",
        "answer": "The full form of POS tagging is Parts of speech tagging. It is most commonly known as POS tagging. According to their context, it specifies a process of identifying specific words in a document and groups them as part of speech.\nPOS tagging is also known as grammatical tagging because it involves understanding grammatical structures and identifying the respective component. It is a very complicated process because the same word can be different parts of speech depending on the situation and the structure of the sentence.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "32) What is NES in Natural Language Processing? Why is it used?",
        "answer": "NES is an acronym that stands for Name Entity Recognition. It is used in Natural Language Processing and is most commonly known as NER. It is the process of identifying specific entities in a text document that is more informative and have a unique context. It includes places, people, organizations, and more. After identification, it extracts these entities and categorizes them under different predefined classes. Later, this step helps in extracting information.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "33) What is parsing in Natural Language Processing? What are the different types of parsing used in NLP?",
        "answer": "Parsing is a technique or a method of analyzing the sentences automatically according to their syntactic structure.\nFollowing is a list of different types of parsing used in Natural Language Processing:\nDependency parsing / Syntactic parsing: Dependency parsing is also known as syntactic parsing. It recognizes a dependency parse of a sentence and assigns a syntax structure to the sentence. It mainly focuses on the relationship between different words.\nSemantic parsing: Semantic parsing is a method of converting the natural language into machine language that a computer can understand and process.\nConstituency parsing: Constituency parsing is a specific parsing method where a division of sentences is divided into sub-parts or constituencies. It is mainly used to extract a constituency-based parse tree from the constituencies of the sentences.\nShallow parsing / Light parsing: Shallow parsing is also known as light parsing and chunking. It identifies constituents of sentences and then links them to different groups of grammatical meanings.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "34) What is language modeling in NLP?",
        "answer": "In Natural Language Processing, language modeling creates a probability distribution of a sequence of words. It provides probability to all the words present in that sequence.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "35) What is topic modeling in NLP?",
        "answer": "In NLP, topic modeling is finding abstract topics in a document or set of documents to find hidden semantic structures.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "36) What is the key difference between dependency parsing and shallow parsing?",
        "answer": "The key difference between dependency parsing and shallow parsing is that dependency parsing is the process of finding a relation between all the different words. On the other hand, shallow parsing is the parsing of a selected limited part of the information.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "37) What do you understand by Pragmatic Ambiguity in NLP?",
        "answer": "In Natural Language Processing, pragmatic ambiguity specifies multiple descriptions of a word or a sentence. It occurs when the words of the sentence may have different meanings, and the correct meaning of the sentence is not clear. In this case, it becomes very difficult for a machine to understand a sentence's meaning, which causes pragmatic ambiguity.\nFor example, see the following sentence:\n\"Are you feeling hungry?\"\nADVERTISEMENT\nThe above sentence could be either a generally asked question or a formal way of offering food.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "38) What are the steps used to solve an NLP problem?",
        "answer": "Following is a list of steps used to solve an NLP problem:\nIn the first step, get the text from the available dataset.\nNow, apply stemming and lemmatization to clean the text.\nNow, apply feature engineering techniques to the received text.\nEmbed using word2vec.\nNow, train the built model using neural networks or other Machine Learning techniques.\nNow it turns to evaluate the model's performance.\nMake the appropriate changes in the model.\nNow, the model is complete. Deploy the model.",
        "reference": "javatpoint.com",
        "role": "nlp"
    },
    {
        "question": "39) What is noise removal in NLP? Why is it used?",
        "answer": "Noise removal is one of the NLP techniques. As the name specifies, it is used to remove unnecessary pieces of text from the sentences.",
        "reference": "javatpoint.com",
        "role": "nlp"
    }
]