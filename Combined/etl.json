[
    {
        "question": "1. Compare between ETL and ELT.",
        "answer": "Criteria ETL ELT\nWorking methodology Data from the source system to the data warehouse Leverages the target system to transform data\nPrivacy Pre-loading transformation has the potential to eliminate PII, which provides assistance in complying with HIPAA regulations. More privacy safeguards are necessary for directly loading data.\nCosts Cost issues can arise from the use of separate servers. The simplified data stack has a lower cost.\nPerformance Average Good\nFlexibility High Low",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "2. What is an ETL process?",
        "answer": "ETL is the process of Extraction, Transformation, and Loading.\nEnroll now in Informatica training to learn more about its concepts.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "3. How many steps are there in an ETL process?",
        "answer": "In its fundamental form, the ETL process involves the extraction, transformation, and loading of data. Although the acronym suggests a concise and orderly three-step procedure – extract, transform, load.\nGo through this insightful blog to get a detailed understanding of the ETL process!",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "4. What are the steps involved in an ETL process?",
        "answer": "The steps involved are defining the source and the target, creating the mapping, creating the session, and creating the workflow.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "5. Can there be sub-steps for each of the ETL steps?",
        "answer": "Yes, each step in the ETL (Extract, Transform, Load) process can have sub-steps that further refine and enhance the data. Here are some examples of sub-steps for each of the ETL steps:\nExtract:\nConnect to the data source\nRetrieve data based on specific criteria or filters\nPerform data sampling or profiling to understand the structure and quality of the data\nHandle data extraction errors or exceptions\nTransform:\nFiltering\nCleansing\nDe-duplicating\nValidating\nStandardizing\nNormalizing\nAggregating\nEnriching\nDeriving\nLoad:\nCreate or configure the destination database or data warehouse\nDefine the schema or structure of the target data tables\nMap and transform the transformed data to match the target schema\nHandle data loading errors or conflicts\nOptimize the loading process for performance and efficiency\nEnsure data consistency and referential integrity during the loading process\nThese sub-steps provide a more detailed breakdown of the tasks and operations that can be performed within each step of the ETL process. They help in ensuring data quality, consistency, and alignment with the desired data model or requirements.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "6. What are initial load and full load?",
        "answer": "In ETL, the initial load is the process for populating all data warehousing tables for the very first time. In full load, when the data is loaded for the first time, all set records are loaded at a stretch depending on its volume. It would erase all contents from the table and would reload the fresh data.\n\nWatch this ETL Pipeline video:",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "7. What is meant by incremental load?",
        "answer": "Incremental load refers to applying dynamic changes as and when required in a specific period and predefined schedules.\nGet 100% Hike!\nMaster Most in Demand Skills Now !\nBy providing your contact details, you agree to our Terms of Use & Privacy Policy",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "8. What are the names of the layers in the three-tier system of ETL and how does it function?",
        "answer": "The data warehouse is considered to be the 3-tier system in ETL.\nIt is known as a 3-tier system because data warehouses generally have a three-level (tier) architecture:\nBottom Tier (Data Warehouse Server)\nMiddle Tier (OLAP Server)\nTop Tier (Front end Tools)\n\nThe middle tier in ETL provides end users the data that is usable in a secure way. the other two layers are on either side of the middle tier, the end users and the back-end data storage.\nThe first layer in ETL is the source layer, and it is the layer where data lands. The second layer is the integration layer where the data is stored after transformation. The third layer is the dimension layer where the actual presentation layer is present.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "9. What are the characteristics of snapshots and what is their meaning?",
        "answer": "Snapshots are copies of the read-only data that is stored in the master table.\nSnapshots are located on remote nodes and refreshed periodically so that the changes in the master table can be recorded. They are also replicas of tables.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "10. What are views?",
        "answer": "Views are built using the attributes of one or more tables. View with a single table can be updated, but those with multiple tables cannot be updated.\n\nETL Interview Questions for Experienced (2 to 5 Years)",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "11. What is the meaning of a materialized view log, and how would you define a materialized view?",
        "answer": "A materialized view log is a table that stores changes to the base tables used in a materialized view. A materialized view is a pre-computed aggregate table that contains summarized or joined data from fact and dimension tables.\nLearn more about Informatica from this in-depth Informatica Tutorial!",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "12. What is the difference between PowerCenter and PowerMart?",
        "answer": "PowerCenter processes large volumes of data, whereas Power Mart processes small volumes of data.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "13. Mention a few ETL flaws.",
        "answer": "Following are a few common ETL bugs:\nUser Interface Bug: GUI bugs encompass issues with color selection, font style, navigation, spelling check, etc., in the user interface.\nInput/Output Bug: This type of bug causes the application to accept invalid values instead of valid ones.\nBoundary Value Analysis Bug: Bugs in this section verify both the minimum and maximum values.\nCalculation bugs: Calculation bugs usually result in mathematical errors that lead to incorrect results.\nLoad Condition Bugs: This software defect prevents the inclusion of multiple users and prohibits the utilization of user-accepted data.\nRace Condition Bugs: This type of bug disrupts the proper functioning of your system, causing it to crash or hang.\nECP (Equivalence Class Partitioning) Bug: A bug of this type leads to the presence of invalid types.\nVersion Control Bugs: Regression testing is where you commonly encounter these types of bugs, and it lacks version details.\nHardware Bugs: This type of bug hinders the device from responding to an application as expected.\nHelp Source Bugs: Due to this bug, the help documentation becomes incorrect.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "14. With which apps can PowerCenter be connected?",
        "answer": "PowerCenter can be connected with ERP sources such as SAP, Oracle Apps, PeopleSoft, etc.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "15. Which partition is used to improve the performance of ETL transactions?",
        "answer": "To improve the performance of ETL (Extract, Transform, Load) transactions, partitioning is often employed. Partitioning involves dividing a large dataset into smaller, more manageable sections based on a specified criterion. \nThe partitioning strategy used in ETL depends on the specific needs and characteristics of the data. However, a common approach is to utilize horizontal partitioning. In horizontal partitioning, the dataset is divided into distinct partitions based on a specific attribute or range of values.\nBy partitioning the data, several performance benefits can be achieved:\nParallel processing: Partitioning allows different partitions to be processed simultaneously by multiple threads or processes. This parallelism enhances overall ETL performance by utilizing the available computing resources efficiently.\nScalability: As the dataset grows in size, partitioning enables the ETL process to scale effectively. Each partition can be processed independently, enabling horizontal scalability by distributing the workload across multiple processing units or nodes.\nReduced I/O operations: By isolating data within partitions, I/O operations can be minimized. During extraction and transformation, only the relevant partitions need to be accessed, reducing the amount of data read or written. \nEnhanced query performance: Partitioning can improve query performance when accessing or querying the data. By partitioning based on attributes commonly used in queries, the database optimizer can efficiently navigate to the relevant partition, resulting in faster query execution times.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "16. Does PowerMart provide connections to ERP sources?",
        "answer": "No! PowerMart does not provide connections to any of the ERP sources.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "17. What is meant by partitioning in ETL?",
        "answer": "Partitioning in ETL refers to the sub-division of the transactions in order to improve their performance.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "18. What is the benefit of increasing the number of partitions in ETL?",
        "answer": "An increase in the number of partitions enables the Informatica server to create multiple connections to a host of sources.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "19. What is the difference between ETL and ELT?",
        "answer": "Extract, Transform, Load, or ETL, is the process of first removing data from several sources, then transforming it in accordance with needs or business rules, and then loading it into a database or data warehouse. In contrast, ELT stands for Extract, Load, and Transform. In this method, data is first extracted, then loaded into a target system (often a data lake or warehouse), and last, using tools like SQL or other processing engines, transformed inside the target system.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "20. Explain the concept of data skewness in ETL processes.",
        "answer": "An imbalance in the distribution of data among processing nodes or partitions is referred to as data skewness, and it can cause problems with ETL process performance. When some keys or values occur significantly more frequently than others, an imbalance arises that results in some nodes being overwhelmed with data while others are left underutilized. Techniques like data splitting, data shuffling, or the use of sophisticated processing frameworks like Apache Spark can be used to solve data skewness.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "21. How would you handle incremental data loading in ETL pipelines?",
        "answer": "Instead of reloading the complete dataset, incremental data loading updates the new or modified data since the last ETL run. This can be accomplished by tracking the most recent successful ETL run and determining the delta changes that have occurred since using metadata or timestamps. To effectively recognize incremental changes and load them into the target system, methods such as CDC (Change Data Capture), database triggers, and timestamp comparisons can be employed.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "22. Discuss the significance of data quality checks in ETL processes.",
        "answer": "To make sure that the data being extracted, processed, and loaded satisfies certain requirements or expectations in terms of validity, accuracy, consistency, and completeness, data quality checks are essential in ETL procedures. These checks assist in the early detection and correction of data mistakes or inconsistencies in the ETL pipeline, preventing problems with reporting or analytics later on. In order to enforce data quality in ETL pipelines, methods including data profiling, data cleansing, and validation rule implementation are frequently employed.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "23. How would you handle errors and exceptions in ETL workflows?",
        "answer": "To guarantee data dependability and integrity, ETL operations must handle mistakes and exceptions. Strong error-handling techniques, such as retry methods for temporary problems, logging error details for troubleshooting, putting in place checkpoints to resume from the point of failure, and adding alerting mechanisms to inform stakeholders about significant failures, can be used to achieve this. ETL workflow management can also be aided by the use of fault-tolerant processing frameworks such as Apache Airflow or Apache NiFi.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "24. What are some common data integration challenges in ETL processes, and how would you address them?",
        "answer": "Managing schema evolution, handling varied data formats, rapidly processing huge volumes of data, and guaranteeing data consistency across multiple sources are common issues in data integration. Techniques like data normalization, data cleansing, applying data validation criteria, using scalable processing frameworks like Apache Spark, and using schema evolution strategies like versioning or schema-on-read can all be used to address these issues.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "25. Explain the concept of data deduplication in ETL pipelines and its importance.",
        "answer": "To guarantee data accuracy and consistency, duplicate records are found in datasets and removed. This process is known as data deduplication. This is crucial for ETL pipelines to preserve data integrity, save redundant work in downstream processing, and boost system speed. Effective data deduplication can be achieved by utilizing strategies like hashing algorithms, unique identifiers, and database features like DISTINCT clauses.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "26. How would you design a scalable and fault-tolerant ETL architecture?",
        "answer": "Several factors must be taken into account while developing a scalable and fault-tolerant ETL architecture, including putting distributed processing into practice, making use of fault-tolerant storage systems, planning for horizontal scalability, and adding redundancy and failover mechanisms. Developing a solid ETL architecture can be aided by methods such as utilizing microservices architecture, cloud-based ETL services, containerization technologies like Docker and Kubernetes, and data replication and backup plans.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "27. Discuss the concept of data lineage in ETL processes and its significance.",
        "answer": "Data lineage refers to the documentation of the end-to-end journey of data from its source systems through various transformations to its destination systems. It provides visibility into how data is acquired, manipulated, and used within an organization, which is crucial for regulatory compliance, data governance, and impact analysis. Establishing and maintaining data lineage helps ensure data traceability, understanding data dependencies, and facilitating data auditing and troubleshooting.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "28. How would you handle schema evolution in ETL pipelines when source schemas change over time?",
        "answer": "Schema evolution is the process of handling alterations to the source data’s structure or schema over time. Schema inference, schema mapping, and schema evolution policies are some of the strategies that can be used to manage schema evolution in ETL pipelines. Furthermore, to accommodate changes to source schemas without interfering with downstream operations, flexible data models like schema-on-read, data serialization formats like Avro or Parquet, and schema versioning techniques can be used.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "29. Explain the concept of data partitioning in distributed ETL processing.",
        "answer": "Large datasets are divided into smaller, more manageable segments using data partitioning techniques such as range, hash, or key. These partitions are then distributed over several processing nodes for parallel processing. In distributed ETL processing frameworks like Apache Spark or Hadoop, this aids in enhancing processing performance, resource utilization, and scalability. Optimizing ETL performance requires selecting a suitable partitioning method based on processing requirements and data properties.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "30. How would you handle slowly changing dimensions (SCDs) in ETL pipelines?",
        "answer": "Dimensions in a data warehouse that vary gradually over time, like product categories or client addresses, are referred to as slowly changing dimensions (SCDs). Identifying and recording changes to dimension data over time using methods such as Type 1 (overwrite), Type 2 (historical tracking), or Type 3 (partial historical tracking), dimension handling is the process of handling SCDs in ETL pipelines. Accurate and historical dimension data is maintained in the data warehouse by implementing suitable SCD approaches based on business requirements and data characteristics.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "31. Discuss the concept of data masking in ETL processes and its importance for data security and privacy.",
        "answer": "Data masking protects sensitive information while preserving its use for specific uses, such as analytics or testing, by substituting fictitious or anonymized values for sensitive or confidential data. To adhere to data privacy laws, stop unwanted access to private information, and reduce the chance of data breaches, data masking is crucial in ETL operations. Depending on the necessary degree of security and privacy, several techniques, like tokenization, encryption, pseudonymization, and anonymization, can be applied for data masking.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "32. How would you optimize ETL performance for handling large volumes of data?",
        "answer": "Several strategies, including parallel processing, data partitioning for parallelism, optimizing data transfer and transformation operations, making use of distributed processing frameworks, and putting caching and indexing techniques into practice, are involved in optimizing ETL performance for handling large volumes of data. Large dataset ETL performance can also be increased by maximizing hardware resources, fine-tuning database setups, and leveraging in-memory processing technologies.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "33. Discuss the role of metadata management in ETL processes and its significance.",
        "answer": "The process of managing and preserving metadata, which offers details about the composition, context, and organization of data assets, is known as metadata management. Metadata management is essential to ETL procedures because it helps with data discovery and governance, records transformation rules, manages dependencies, and documents data lineage. Developing strong metadata management procedures contributes to increased data quality, regulatory compliance, and ETL process efficiency and effectiveness.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "34. What are the types of partitions in ETL?",
        "answer": "Types of partitions in ETL are Round-Robin partition and Hash partition.\n\nAdvanced ETL Interview Questions (6 to 12 Years)",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "35. What is Round-Robin partitioning?",
        "answer": "In Round-Robin partitioning, the data is evenly distributed by Informatica among all partitions. It is used when the number of rows in the process in each of the partitions is nearly the same.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "36. What is Hash partitioning?",
        "answer": "In Hash partitioning, the Informatica server would apply a hash function in order to partition keys to group data among the partitions. It is used to ensure the processing of a group of rows with the same partitioning key in the same partition.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "37. What is mapping in ETL?",
        "answer": "Mapping refers to the flow of data from the source to the destination.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "38. What is a session in ETL?",
        "answer": "A session is a set of instructions that describe the data movement from the source to the destination.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "39. What is meant by Worklet in ETL?",
        "answer": "Worklet is a set of tasks in ETL. It can be any set of tasks in the program.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "40. What is Workflow in ETL?",
        "answer": "Workflow is a set of instructions that specify the way of executing the tasks to the Informatica.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "41. What is meant by Worklet in ETL?",
        "answer": "Worklet is a set of tasks in ETL. It can be any set of tasks in the program.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "42. What is the use of Mapplet in ETL?",
        "answer": "Mapplet in ETL is used for the purpose of creation as well as the configuration of a group of transformations.\nAlso, prepare yourself for the Informatica Interview Questions and Answers!",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "43. What is meant by operational data store?",
        "answer": "The operational data store (ODS) is the repository that exists between the staging area and the data warehouse. The data stored in ODS has low granularity.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "44. How does the operational data store work?",
        "answer": "Aggregated data is loaded into the enterprise data warehouse (EDW) after it is populated in the operational data store (ODS). Basically, ODS is a semi-data warehouse (DWH) that allows analysts to analyze business data. The data persistence period in ODS is usually in the range of 30–45 days and not more.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "45. What does the ODS in ETL generate?",
        "answer": "Operational data stores function similarly to the extract, transform, and load (ETL) process. In the ETL process, data is extracted from specific sources, transformed, and loaded to its designated destination. Conversely, in the ODS process, raw data from production systems is imported and stored in its original state, without undergoing any transformations. The purpose of this approach is to present the data as-is to business intelligence (BI) applications for analysis and operational decision-making.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "46. When are the tables in ETL analyzed?",
        "answer": "To find a cost-efficient plan for data retrieval, we analyze tables in ETL.\nThe use of the ANALYZE statement allows the validation and computing of statistics for either the index, table, or cluster.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "47. How are the tables analyzed in ETL?",
        "answer": "Statistics generated by the ANALYZE statement is reused by a cost-based optimizer in order to calculate the most efficient plan for data retrieval. The ANALYZE statement can support the validation of structures of objects, as well as space management, in the system. Operations include COMPUTER, ESTIMATE, and DELETE.\nExample for Oracle 7:\nselect OWNER,\nsum(decode(nvl(NUM_ROWS,9999), 9999,0,1)) analyzed,\nsum(decode(nvl(NUM_ROWS,9999), 9999,1,0)) not_analyzed,\ncount(TABLE_NAME) total\nfrom dba_tables\nwhere OWNER not in ('SYS', 'SYSTEM')\ngroup by OWNER\nThis program executes a SQL query against the database’s `dba_tables` view to retrieve information about tables. The purpose is to analyze the data in the tables and determine the number of analyzed and not analyzed tables for each owner.\nHere’s what the program does step by step:\nIt selects the following columns from the `dba_tables` view: `OWNER` (representing the owner of the table), `analyzed` (representing the sum of tables that are analyzed), `not_analyzed` (representing the sum of tables that are not analyzed), and `total` (representing the count of all tables for the owner).\nThe `decode` function is used to conditionally sum the tables based on the `NUM_ROWS` column. If the `NUM_ROWS` value is null or equal to 9999, it indicates that the table is not analyzed. In that case, the `decode` function increments the `not_analyzed` count by 1, and `analyzed` count by 0. Otherwise, if `NUM_ROWS` is not null and not equal to 9999, it indicates an analyzed table, so the `decode` function increments the `analyzed` count by 1 and `not_analyzed` count by 0.\nThe `where` clause filters out tables owned by the ‘SYS‘ and ‘SYSTEM‘ users. It ensures that only tables owned by other users are considered for analysis.\nFinally, the results are grouped by the `OWNER` column, aggregating the counts for each owner.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "48. How can the mapping be fine-tuned in ETL?",
        "answer": "Steps for fine-tuning the mapping involves using the condition for filter in the source qualifying the data without the use of filter, utilizing persistence as well as cache store in lookup t/r, using the aggregations t/r in sorted i/p group by different ports, using operators in expressions instead of functions, and increasing the cache size and commit interval.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "49. What are the differences between connected and unconnected lookups in ETL?",
        "answer": "Connected Lookup Transformation Unconnected Lookup Transformation\nThe upstream transformations in the pipeline directly provide input values to Connected Lookup. Another transformation provides input values to the Unconnected Lookup through its lookup expression.\nThe multiple return ports allow for retrieving multiple columns from the same row. It has just a single return port and it returns one column from every row.\nThe lookup cache holds both the lookup condition column and the output ports of the lookup source columns. The lookup cache includes the lookup condition and the lookup/return port, which comprise all the lookup/output ports.\nUsers can define values and it supports them. User-defined values are not supported.\nMultiple values are sent to downstream transformations. One transformation receives a single output value.\nThis blog will help you get a better understanding of ETL Reporting Tools!",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "50. Can you define cubes and OLAP cubes?",
        "answer": "The cube plays a crucial role in data processing, serving as a fundamental component. Essentially, cubes function as data processing units within the Data Warehouse, encompassing dimensions and fact tables. They facilitate clients by offering a multidimensional perspective on data, along with capabilities for querying and analysis.\nOn the other hand,, Online Analytical Processing (OLAP) is software designed to enable the analysis of data from multiple databases simultaneously. To serve reporting purposes, an OLAP cube can be employed to store data in a multidimensional format. By utilizing cubes, the creation and viewing of reports are simplified, thereby enhancing and streamlining the reporting process. It is the responsibility of end users to manage and maintain these cubes, requiring them to manually update the data contained within.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "51. Describe the facts and their sources.",
        "answer": "An integral component of data warehousing pertains to the concept of a fact table. Essentially, a fact table serves as a representation of the measurements, metrics, or factual information pertaining to a business process. Within these fact tables, the facts themselves are stored and connected to multiple dimension tables through the use of foreign keys. Facts typically consist of detailed or aggregated measurements related to a business process that can be calculated and grouped together to address specific business inquiries. Data schemas such as the star schema or snowflake schema comprise a central fact table surrounded by several dimension tables. Examples of facts include figures such as sales, cost, profit and loss, among others.\nFact tables contain two types of columns: foreign keys and measure columns. Foreign keys are used to store references to dimensions, while measure columns contain numerical facts. Additional attributes may be incorporated based on the specific requirements and needs of the business.\nVarious types of facts exist, including the following three fundamental categories:\nAdditive: Fully additive facts offer the greatest flexibility and utility, as they can be summed across any dimension associated with the fact table.\nSemi-additive: Semi-additive facts can be summed across certain dimensions associated with the fact table, but not all.\nNon-additive: Non-additive facts, found within the Fact table, cannot be summed across any dimension. An example of a non-additive fact is a ratio.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "52. How do you approach performance tuning in ETL processes?",
        "answer": "Optimizing different aspects of ETL procedures, including data extraction, transformation logic, data loading, and resource utilization, is known as performance tuning. Describe your strategy for performance tuning, taking into account methods like caching, parallel processing, indexing, partitioning, query optimization, and utilizing in-memory computing. Give instances of particular performance optimization techniques you have used in projects, along with an explanation of how they have affected ETL performance.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "53. How do you ensure data quality and integrity in ETL pipelines?",
        "answer": "Integrity and quality of data are essential components of ETL procedures. Talk about the methods you use to make sure the data is of high quality, such as data cleansing, deduplication, data validation, data profiling, and the application of business rules or validation checks. Describe how you manage outliers, inconsistencies, and abnormalities in your data, as well as how you track and report on data quality indicators through the ETL pipeline to ensure data integrity.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "54. How do you handle error handling and recovery in complex ETL workflows?",
        "answer": "Error handling and recovery are critical for ensuring data integrity and reliability in ETL processes. Discuss your approach to error handling, including techniques such as retry mechanisms, error logging and monitoring, checkpointing, and recovery strategies. Explain how you handle transient errors, data validation errors, and system failures, and how you design fault-tolerant ETL workflows to recover from errors and resume processing without data loss or corruption.",
        "reference": "intellipaat.com",
        "role": "etl"
    },
    {
        "question": "55. How do you approach ETL testing and validation in complex data environments?",
        "answer": "Ensuring the correctness, consistency, and completeness of data in ETL procedures requires both ETL testing and validation. Talk about the methods you use for ETL testing, such as end-to-end, unit, integration, and regression testing. How do you automate testing and validation in complex data environments? Describe how you create test cases, data mocks, and validation scripts, and how you use technologies like PySpark, Apache Beam, and SQL-based testing frameworks.",
        "reference": "intellipaat.com",
        "role": "etl"
    }
]