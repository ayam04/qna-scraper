[
    {
        "question": "Check out this video on ETL Testing Tutorial:",
        "answer": "Basic Interview Questions",
        "reference": "intellipaat.com",
        "role": "etl-testing"
    },
    {
        "question": "1. Compare ETL Testing with Manual Testing.",
        "answer": "Criteria ETL Testing Manual testing\nBasic procedure Writing scripts for automating the testing process A method of observing and testing\nRequirements No need for additional technical knowledge other than the understanding of the software Needs technical knowledge of SQL and Shell scripting\nEfficiency Fast and systematic, and provides top results Needs time and effort, and is prone to errors\nAlso, Check out our blog on Manual Testing vs Automation Testing.",
        "reference": "intellipaat.com",
        "role": "etl-testing"
    },
    {
        "question": "2. What is ETL?",
        "answer": "ETL refers to the Extracting, Transforming, and Loading of data from any outside system to the required place. These are the basic three steps in the data integration process.\n‘Extracting’ means locating data and removing it from the source file; ‘Transforming’ is the process of transporting it to the required target file, and in the ‘Loading’ stage the file is loaded to the target system in the specified format.\nGet 100% Hike!\nMaster Most in Demand Skills Now !\nBy providing your contact details, you agree to our Terms of Use & Privacy Policy",
        "reference": "intellipaat.com",
        "role": "etl-testing"
    },
    {
        "question": "3. Why ETL Testing is required?",
        "answer": "To keep an eye on data that is being transferred from one system to another\nTo keep track of the efficiency and speed of the process\nTo achieve fast and the best results\nLearn more about the ETL Testing process through this Data Warehouse Tutorial!",
        "reference": "intellipaat.com",
        "role": "etl-testing"
    },
    {
        "question": "4. What are the responsibilities of an ETL Tester?",
        "answer": "An ETL Tester:\nRequires in-depth knowledge of the ETL tools and processes\nNeeds to write SQL queries for various scenarios during the testing phase\nShould be able to carry out different types of tests and keep a check on the other functionalities of the process\nNeeds to carry out quality checks on a regular basis",
        "reference": "intellipaat.com",
        "role": "etl-testing"
    },
    {
        "question": "5. What are the various tools used in ETL?",
        "answer": "Cognos Decision Stream\nOracle Warehouse Builder\nBusiness Objects XI\nSAS Business Warehouse\nSAS Enterprise ETL Server\nCrack API testing interviews with the help of our guide on API Testing Interview Questions and Answers.\n\nIntermediate Interview Questions",
        "reference": "intellipaat.com",
        "role": "etl-testing"
    },
    {
        "question": "6. Define ETL Processing.",
        "answer": "ETL Testing Process:\nAlthough there are many ETL tools, there is a simple testing process commonly used in ETL Testing. It is as important as the implementation of the ETL tool into your business. Having a well-defined ETL Testing strategy can make the testing process much easier. Hence, this process needs to be completed before you start the data integration with the selected ETL tool.\nIn this ETL Testing process, a group of experts comprising the programming and developing team will start writing SQL statements. The development team may customize them according to the requirements.\nETL Testing process has the following stages:\nAnalyzing requirements: Understanding the business structure and their particular requirements.\nValidation and test estimation: Estimating the time and expertise required to carry on with the procedure.\nTest planning and designing the testing environment: Based on the inputs from the estimation, an ETL environment is planned and worked out.\nTest data preparation and execution: Data for the test is prepared and executed as per the requirements.\nSummary report:  Upon the completion of the test run, a brief summary report is prepared for improvising and concluding.\nTo learn more in detail, enroll in Intellipaat’s ETL Testing Course!",
        "reference": "intellipaat.com",
        "role": "etl-testing"
    },
    {
        "question": "7. What do ETL Testing operations include?",
        "answer": "ETL Testing includes:\nVerifying whether the data is transformed accurately according to business requirements\nVerifying that the projected data is loaded into the data warehouse without any truncation or data loss\nMaking sure that the ETL application reports any invalid data and replaces with default values\nMaking sure that the data loads within the expected time frame to improve scalability and performance\nCheck out the ETL Developer Interview Questions to prepare for your next interview.",
        "reference": "intellipaat.com",
        "role": "etl-testing"
    },
    {
        "question": "8. List a few ETL bugs.",
        "answer": "Calculation Bug\nUser Interface Bug\nSource Bug\nLoad Condition Bug\nECP-related Bug\nIn addition to the above ETL Testing questions, there may be other vital questions where you might be asked to mention the ETL tools that you have used earlier. Also, you might be asked about any debugging issues you have faced in your earlier real-time experience.\nInterested in learning ETL in detail? Check out our ETL Tutorial for Beginners!",
        "reference": "intellipaat.com",
        "role": "etl-testing"
    },
    {
        "question": "9. What is Fact? What are the types of Facts?",
        "answer": "Fact is a central component of a multi-dimensional model that contains the measures to be analyzed. Facts are related to dimensions.\nTypes of facts are:\nAdditive Facts\nSemi-additive Facts\nNon-additive Facts\nIf you have any queries, visit our ETL Testing Community and clarify all your doubts today!",
        "reference": "intellipaat.com",
        "role": "etl-testing"
    },
    {
        "question": "10. What are Cubes and OLAP Cubes?",
        "answer": "Cubes are data processing units comprised of fact tables and dimensions from the data warehouse. They provide a multi-dimensional analysis.\nOLAP stands for ‘Online Analytics Processing,’ and OLAP Cubes store voluminous data in a multi-dimensional form for reporting purposes. They consist of facts called ‘measures’ categorized by dimensions.\nPreparing for a Job Interview! Check out our blog Selenium Interview Questions now.",
        "reference": "intellipaat.com",
        "role": "etl-testing"
    },
    {
        "question": "1. Explain the process of ETL testing.",
        "answer": "ETL testing is made easier when a testing strategy is well defined. The ETL testing process goes through different phases, as illustrated below:   Analyze Business Requirements: To perform ETL Testing effectively, it is crucial to understand and capture the business requirements through the use of data models, business flow diagrams, reports, etc.\nIdentifying and Validating Data Source: To proceed, it is necessary to identify the source data and perform preliminary checks such as schema checks, table counts, and table validations. The purpose of this is to make sure the ETL process matches the business model specification.\nDesign Test Cases and Preparing Test Data: Step three includes designing ETL mapping scenarios, developing SQL scripts, and defining transformation rules. Lastly, verifying the documents against business needs to make sure they cater to those needs. As soon as all the test cases have been checked and approved, the pre-execution check is performed. All three steps of our ETL processes - namely extracting, transforming, and loading - are covered by test cases.\nTest Execution with Bug Reporting and Closure: This process continues until the exit criteria (business requirements) have been met. In the previous step, if any defects were found, they were sent to the developer for fixing, after which retesting was performed. Moreover, regression testing is performed in order to prevent the introduction of new bugs during the fix of an earlier bug.\nSummary Report and Result Analysis: At this step, a test report is prepared, which lists the test cases and their status (passed or failed). As a result of this report, stakeholders or decision-makers will be able to properly maintain the delivery threshold by understanding the bug and the result of the testing process.\nTest Closure: Once everything is completed, the reports are closed. Analyze Business Requirements: To perform ETL Testing effectively, it is crucial to understand and capture the business requirements through the use of data models, business flow diagrams, reports, etc. Analyze Business Requirements: Identifying and Validating Data Source: To proceed, it is necessary to identify the source data and perform preliminary checks such as schema checks, table counts, and table validations. The purpose of this is to make sure the ETL process matches the business model specification. Identifying and Validating Data Source: Design Test Cases and Preparing Test Data: Step three includes designing ETL mapping scenarios, developing SQL scripts, and defining transformation rules. Lastly, verifying the documents against business needs to make sure they cater to those needs. As soon as all the test cases have been checked and approved, the pre-execution check is performed. All three steps of our ETL processes - namely extracting, transforming, and loading - are covered by test cases. Design Test Cases and Preparing Test Data: Test Execution with Bug Reporting and Closure: This process continues until the exit criteria (business requirements) have been met. In the previous step, if any defects were found, they were sent to the developer for fixing, after which retesting was performed. Moreover, regression testing is performed in order to prevent the introduction of new bugs during the fix of an earlier bug. Test Execution with Bug Reporting and Closure: Summary Report and Result Analysis: At this step, a test report is prepared, which lists the test cases and their status (passed or failed). As a result of this report, stakeholders or decision-makers will be able to properly maintain the delivery threshold by understanding the bug and the result of the testing process. Summary Report and Result Analysis: Test Closure: Once everything is completed, the reports are closed. Test Closure:",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "2. Name some tools that are used in ETL.",
        "answer": "The use of ETL testing tools increases IT productivity and facilitates the process of extracting insights from big data. With the tool, you no longer have to use labor-intensive, costly traditional programming methods to extract and process data. Technology evolved over time, so did solutions. Nowadays, various ways can be used for ETL testing depending on the source data and the environment. There are several ETL vendors that focus on ETL exclusively, such as Informatica. Software vendors like IBM, Oracle, and Microsoft provide other tools as well.  Open source ETL tools have also recently emerged that are free to use. The following are some ETL software tools to consider: Enterprise Software ETL Enterprise Software ETL Informatica PowerCenter\nIBM InfoSphere DataStage\nOracle Data Integrator (ODI)\nMicrosoft SQL Server Integration Services (SSIS)\nSAP Data Services\nSAS Data Manager, etc. Informatica PowerCenter IBM InfoSphere DataStage Oracle Data Integrator (ODI) Microsoft SQL Server Integration Services (SSIS) SAP Data Services SAS Data Manager, etc. Open Source ETL Open Source ETL Talend Open Studio\nPentaho Data Integration (PDI)\nHadoop, etc. Talend Open Studio Pentaho Data Integration (PDI) Hadoop, etc.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "3. What are the roles and responsibilities of an ETL tester?",
        "answer": "Since ETL testing is so important, ETL testers are in great demand. ETL testers validate data sources, extract data, apply transformation logic, and load data into target tables. The following are key responsibilities of an ETL tester: In-depth knowledge of ETL tools and processes.\nPerforms thorough testing of the ETL software.\nCheck the data warehouse test component.\nPerform the backend data-driven test.\nDesign and execute test cases, test plans, test harnesses, etc.\nIdentifies problems and suggests the best solutions.\nReview and approve the requirements and design specifications.\nWriting SQL queries for testing scenarios.\nVarious types of tests should be carried out, including primary keys, defaults, and checks of other ETL-related functionality.\nConducts regular quality checks. In-depth knowledge of ETL tools and processes. Performs thorough testing of the ETL software. Check the data warehouse test component. Perform the backend data-driven test. Design and execute test cases, test plans, test harnesses, etc. Identifies problems and suggests the best solutions. Review and approve the requirements and design specifications. Writing SQL queries for testing scenarios. Various types of tests should be carried out, including primary keys, defaults, and checks of other ETL-related functionality. Conducts regular quality checks.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "4. What is the importance of ETL testing?",
        "answer": "Following are some of the notable benefits that are highlighted while endorsing ETL Testing: Ensure data is transformed efficiently and quickly from one system to another.\nData quality issues during ETL processes, such as duplicate data or data loss, can also be identified and prevented by ETL testing.\nAssures that the ETL process itself is running smoothly and is not hampered.\nEnsures that all data implemented is in line with client requirements and provides accurate output.\nEnsures that bulk data is moved to the new destination completely and securely. Ensure data is transformed efficiently and quickly from one system to another. Data quality issues during ETL processes, such as duplicate data or data loss, can also be identified and prevented by ETL testing. Assures that the ETL process itself is running smoothly and is not hampered. Ensures that all data implemented is in line with client requirements and provides accurate output. Ensures that bulk data is moved to the new destination completely and securely.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "5. What are different types of ETL testing?",
        "answer": "Before you begin the testing process, you need to define the right ETL Testing technique. It is important to ensure that the ETL test is performed using the right technique and that all stakeholders agree to it. Testing team members should be familiar with this technique and the steps involved in testing. Below are some types of testing techniques that can be used: Production Validation Testing: Also known as \"production reconciliation\" or \"table balancing,\" it involves validating data in production systems and comparing it against the source data.\nSource to Target Count Testing: This ensures that the number of records loaded into the target is consistent with what is expected.\nSource to Target Data Testing: This entails ensuring no data is lost and truncated when loading data into the warehouse, and that the data values are accurate after transformation.\nMetadata Testing: The process of determining whether the source and target systems have the same schema, data types, lengths, indexes, constraints, etc.\nPerformance Testing: Verifying that data loads into the data warehouse within predetermined timelines to ensure speed and scalability.\nData Transformation Testing: This ensures that data transformations are completed according to various business rules and requirements.\nData Quality Testing: This testing involves checking numbers, dates, nulls, precision, etc. Testing includes both Syntax Tests to report invalid characters, incorrect upper/lower case order, etc., and Reference Tests to check if the data is properly formatted.\nData Integration Testing: In this test, testers ensure the data from various sources have been properly incorporated into the target system, as well as verifying the threshold values.\nReport Testing: The test examines the data in a summary report, verifying the layout and functionality, and making calculations for subsequent analysis. Production Validation Testing: Also known as \"production reconciliation\" or \"table balancing,\" it involves validating data in production systems and comparing it against the source data. Production Validation Testing: Source to Target Count Testing: This ensures that the number of records loaded into the target is consistent with what is expected. Source to Target Count Testing: Source to Target Data Testing: This entails ensuring no data is lost and truncated when loading data into the warehouse, and that the data values are accurate after transformation. Source to Target Data Testing: Metadata Testing: The process of determining whether the source and target systems have the same schema, data types, lengths, indexes, constraints, etc. Metadata Testing: Performance Testing: Verifying that data loads into the data warehouse within predetermined timelines to ensure speed and scalability. Performance Testing: Data Transformation Testing: This ensures that data transformations are completed according to various business rules and requirements. Data Transformation Testing: Data Quality Testing: This testing involves checking numbers, dates, nulls, precision, etc. Testing includes both Syntax Tests to report invalid characters, incorrect upper/lower case order, etc., and Reference Tests to check if the data is properly formatted. Data Quality Testing: Data Integration Testing: In this test, testers ensure the data from various sources have been properly incorporated into the target system, as well as verifying the threshold values. Data Integration Testing: Report Testing: The test examines the data in a summary report, verifying the layout and functionality, and making calculations for subsequent analysis. Report Testing:",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "6. Explain the data cleaning process.",
        "answer": "There is always the possibility of duplicate or mislabeled data when combining multiple data sources. Incorrect data leads to unreliable outcomes and algorithms, even when they appear to be correct. Therefore, consolidation of multiple data representations as well as elimination of duplicate data become essential in order to ensure accurate and consistent data. Here comes the importance of the data cleaning process. Data cleaning can also be referred to as data scrubbing or data cleansing. This refers to the process of removing incomplete, duplicate, corrupt, or incorrect data from a dataset. As the need to integrate multiple data sources becomes more apparent, for example in data warehouses or federated database systems, the significance of data cleaning increases greatly. Because the specific steps in a data cleaning process will vary depending on the dataset, developing a template for your process will ensure that you do it correctly and consistently.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "7. What do you mean by ETL Pipeline?",
        "answer": "As the name suggests, ETL pipelines are the mechanisms to perform ETL processes. This involves a series of processes or activities required for transferring data from one or more sources into the data warehouse for analysis, reporting and data synchronization. It is important to move, consolidate, and alter source data from multiple systems to match the parameters and capabilities of the destination database in order to provide valuable insights.   Among its benefits are: They reduce errors, bottlenecks, and latency, ensuring the smooth flow of information between systems.\nWith ETL pipelines, businesses are able to achieve competitive advantage.\nThe ETL pipeline can centralize and standardize data, allowing analysts and decision-makers to easily access and use it.\nIt facilitates data migrations from legacy systems to new repositories. They reduce errors, bottlenecks, and latency, ensuring the smooth flow of information between systems. With ETL pipelines, businesses are able to achieve competitive advantage. The ETL pipeline can centralize and standardize data, allowing analysts and decision-makers to easily access and use it. It facilitates data migrations from legacy systems to new repositories.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "8. What is BI (Business Intelligence)?",
        "answer": "Business Intelligence (BI) involves acquiring, cleaning, analyzing, integrating, and sharing data as a means of identifying actionable insights and enhancing business growth. An effective BI test verifies staging data, ETL process, BI reports, and ensures the implementation is reliable. In simple words, BI is a technique used to gather raw business data and transform it into useful insight for a business. By performing BI Testing, insights from the BI process are verified for accuracy and credibility.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "9. Write the difference between ETL testing and database testing.",
        "answer": "Data validation is involved in both ETL testing and database testing, however, the two are different. The ETL testing procedure normally involves analyzing data stored in a warehouse system. On the other hand, the database testing procedure is commonly used to analyze data stored in transactional systems. The following are the distinct differences between ETL testing and Database testing. database testing database testing ETL Testing  Database Testing \nThe ETL process is used to test data extraction, transformation, and loading for BI reporting purposes.   Data is validated and integrated by performing database testing. \nData movement is being checked to determine if it is going as expected This test is primarily designed to verify that data follows the rules or standards defined in the Data Model.  \nIt verifies whether the counts and data in the source and target match or not.   It ensures that foreign key relationships are maintained and no orphan records are present, as well as that a column in the table has valid values. \nThis technique is applied to OLAP systems.   This technique is applied to OLTP systems. \nThe approach utilizes denormalized data with fewer joins, more indexes, and more aggregates.   The approach utilizes normalized data with joins. \nSome of the most common ETL tools are QuerySurge, Informatica, Cognos, etc.  Some of the most common database testing tools are Selenium, QTP, etc. ETL Testing  Database Testing \nThe ETL process is used to test data extraction, transformation, and loading for BI reporting purposes.   Data is validated and integrated by performing database testing. \nData movement is being checked to determine if it is going as expected This test is primarily designed to verify that data follows the rules or standards defined in the Data Model.  \nIt verifies whether the counts and data in the source and target match or not.   It ensures that foreign key relationships are maintained and no orphan records are present, as well as that a column in the table has valid values. \nThis technique is applied to OLAP systems.   This technique is applied to OLTP systems. \nThe approach utilizes denormalized data with fewer joins, more indexes, and more aggregates.   The approach utilizes normalized data with joins. \nSome of the most common ETL tools are QuerySurge, Informatica, Cognos, etc.  Some of the most common database testing tools are Selenium, QTP, etc. ETL Testing  Database Testing ETL Testing  Database Testing ETL Testing Database Testing The ETL process is used to test data extraction, transformation, and loading for BI reporting purposes.   Data is validated and integrated by performing database testing. \nData movement is being checked to determine if it is going as expected This test is primarily designed to verify that data follows the rules or standards defined in the Data Model.  \nIt verifies whether the counts and data in the source and target match or not.   It ensures that foreign key relationships are maintained and no orphan records are present, as well as that a column in the table has valid values. \nThis technique is applied to OLAP systems.   This technique is applied to OLTP systems. \nThe approach utilizes denormalized data with fewer joins, more indexes, and more aggregates.   The approach utilizes normalized data with joins. \nSome of the most common ETL tools are QuerySurge, Informatica, Cognos, etc.  Some of the most common database testing tools are Selenium, QTP, etc. The ETL process is used to test data extraction, transformation, and loading for BI reporting purposes.   Data is validated and integrated by performing database testing. The ETL process is used to test data extraction, transformation, and loading for BI reporting purposes. Data is validated and integrated by performing database testing. Data movement is being checked to determine if it is going as expected This test is primarily designed to verify that data follows the rules or standards defined in the Data Model. Data movement is being checked to determine if it is going as expected This test is primarily designed to verify that data follows the rules or standards defined in the Data Model. It verifies whether the counts and data in the source and target match or not.   It ensures that foreign key relationships are maintained and no orphan records are present, as well as that a column in the table has valid values. It verifies whether the counts and data in the source and target match or not. It ensures that foreign key relationships are maintained and no orphan records are present, as well as that a column in the table has valid values. This technique is applied to OLAP systems.   This technique is applied to OLTP systems. This technique is applied to OLAP systems. This technique is applied to OLTP systems. The approach utilizes denormalized data with fewer joins, more indexes, and more aggregates.   The approach utilizes normalized data with joins. The approach utilizes denormalized data with fewer joins, more indexes, and more aggregates. The approach utilizes normalized data with joins. Some of the most common ETL tools are QuerySurge, Informatica, Cognos, etc.  Some of the most common database testing tools are Selenium, QTP, etc. Some of the most common ETL tools are QuerySurge, Informatica, Cognos, etc. Some of the most common database testing tools are Selenium, QTP, etc.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "10. What is data source view?",
        "answer": "Several analysis services databases rely on relational schemas, and the Data source view is responsible for defining such schemas (logical model of the schema). Additionally, it can be easily used to create cubes and dimensions, thus enabling users to set their dimensions in an intuitive way. A multidimensional model is incomplete without a DSV. In this way, you are given complete control over the data structures in your project and are able to work independently from the underlying data sources (e.g., changing column names or concatenating columns without directly changing the original data source). Every model must have a DSV, no matter when or how it's created. Using the Data Source View Wizard to create a DSV Using the Data Source View Wizard to create a DSV You must run the Data Source View Wizard from Solution Explorer within SQL Server Data Tools to create the DSV. In solution explorer, Right Click Data source view folder -> Click New Data Source View.\nChoose one of the available data source objects, or add a new one.\nClick Advanced on the same page to specifically select schemas, apply a filter, or exclude information about table relationships.\nFilter Available Objects (If we use a string as a selection criterion, it is possible to prune the list of the available objects).\nA Name Matching page appears if there are no table relationships defined for the relational data source, and you can choose the appropriate method for matching names by clicking on it. In solution explorer, Right Click Data source view folder -> Click New Data Source View. Choose one of the available data source objects, or add a new one. Click Advanced on the same page to specifically select schemas, apply a filter, or exclude information about table relationships. Filter Available Objects (If we use a string as a selection criterion, it is possible to prune the list of the available objects). A Name Matching page appears if there are no table relationships defined for the relational data source, and you can choose the appropriate method for matching names by clicking on it.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "11. Write about the difference between power mart and power center.",
        "answer": "Power Mart  Power Center\nIt only processes small amounts of data and is considered good if the processing requirements are low.   It is considered good when the amount of data to be processed is high, as it processes bulk data in a short period of time.  \nERP sources are not supported.  ERP sources such as SAP, PeopleSoft, etc. are supported.  \nCurrently, it only supports local repositories.  Local and global repositories are supported. \nThere are no specifications for turning a local repository into a global repository.   It is capable of converting local repositories into global ones.  \nSession partitions are not supported.  To improve the performance of ETL transactions, it supports session partitioning. Power Mart  Power Center\nIt only processes small amounts of data and is considered good if the processing requirements are low.   It is considered good when the amount of data to be processed is high, as it processes bulk data in a short period of time.  \nERP sources are not supported.  ERP sources such as SAP, PeopleSoft, etc. are supported.  \nCurrently, it only supports local repositories.  Local and global repositories are supported. \nThere are no specifications for turning a local repository into a global repository.   It is capable of converting local repositories into global ones.  \nSession partitions are not supported.  To improve the performance of ETL transactions, it supports session partitioning. Power Mart  Power Center Power Mart  Power Center Power Mart Power Center It only processes small amounts of data and is considered good if the processing requirements are low.   It is considered good when the amount of data to be processed is high, as it processes bulk data in a short period of time.  \nERP sources are not supported.  ERP sources such as SAP, PeopleSoft, etc. are supported.  \nCurrently, it only supports local repositories.  Local and global repositories are supported. \nThere are no specifications for turning a local repository into a global repository.   It is capable of converting local repositories into global ones.  \nSession partitions are not supported.  To improve the performance of ETL transactions, it supports session partitioning. It only processes small amounts of data and is considered good if the processing requirements are low.   It is considered good when the amount of data to be processed is high, as it processes bulk data in a short period of time. It only processes small amounts of data and is considered good if the processing requirements are low. It is considered good when the amount of data to be processed is high, as it processes bulk data in a short period of time. ERP sources are not supported.  ERP sources such as SAP, PeopleSoft, etc. are supported. ERP sources are not supported. ERP sources such as SAP, PeopleSoft, etc. are supported. Currently, it only supports local repositories.  Local and global repositories are supported. Currently, it only supports local repositories. Local and global repositories are supported. There are no specifications for turning a local repository into a global repository.   It is capable of converting local repositories into global ones. There are no specifications for turning a local repository into a global repository. It is capable of converting local repositories into global ones. Session partitions are not supported.  To improve the performance of ETL transactions, it supports session partitioning. Session partitions are not supported. To improve the performance of ETL transactions, it supports session partitioning.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "12. State difference between ETL and OLAP (Online Analytical Processing) tools.",
        "answer": "ETL tools: The data is extracted, transformed, and loaded into the data warehouse or data mart using ETL tools. Several transformations are necessary before data is loaded into the target table in order to implement business logic.  Example: Data stage, Informatica, etc. \nOLAP (Online Analytical Processing) tools: OLAP tools are designed to create reports from data warehouses and data marts for business analysis. It loads data from the target tables into the OLAP repository and performs the required modifications to create a report. Example: Business Objects, Cognos etc. ETL tools: The data is extracted, transformed, and loaded into the data warehouse or data mart using ETL tools. Several transformations are necessary before data is loaded into the target table in order to implement business logic.  Example: Data stage, Informatica, etc. ETL tools: Example OLAP (Online Analytical Processing) tools: OLAP tools are designed to create reports from data warehouses and data marts for business analysis. It loads data from the target tables into the OLAP repository and performs the required modifications to create a report. Example: Business Objects, Cognos etc. OLAP (Online Analytical Processing) tools: Example",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "13. What do you mean by data purging?",
        "answer": "When data needs to be deleted from the data warehouse, it can be a very tedious task to delete data in bulk. The term data purging refers to methods of permanently erasing and removing data from a data warehouse. Data purging, often contrasted with deletion, involves many different techniques and strategies. When you delete data, you are removing it on a temporary basis, but when you purge data, you are permanently removing the data and freeing up memory or storage space. In general, the data that is deleted is usually junk data such as null values or extra spaces in the row. Using this approach, users can delete multiple files at once and maintain both efficiency and speed.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "14. Explain how a data warehouse differs from data mining.",
        "answer": "Both data mining and data warehousing are powerful data analysis and storage techniques. Data warehousing: To generate meaningful business insights, it involves compiling and organizing data from various sources into a common database. In a data warehouse, data are cleaned, integrated and consolidated to support management decision-making processes. Object-oriented, integrated, time-varying, and nonvolatile data can be stored within a Data warehouse. Data warehousing: To generate meaningful business insights, it involves compiling and organizing data from various sources into a common database. In a data warehouse, data are cleaned, integrated and consolidated to support management decision-making processes. Object-oriented, integrated, time-varying, and nonvolatile data can be stored within a Data warehouse. Data warehousing:   Data mining: Also referred to as KDD (Knowledge Discover in Database), it involves searching for and identifying hidden, relevant, and potentially valuable patterns in large data sets. An important goal of data mining is to discover previously unknown relationships among the data. Through data mining, insights can be extracted that can be used for things such as marketing, fraud detection, and scientific discoveries. Data mining: Also referred to as KDD (Knowledge Discover in Database), it involves searching for and identifying hidden, relevant, and potentially valuable patterns in large data sets. An important goal of data mining is to discover previously unknown relationships among the data. Through data mining, insights can be extracted that can be used for things such as marketing, fraud detection, and scientific discoveries. Data mining:   Difference between Data Warehouse and Data Mining - Difference between Data Warehouse and Data Mining Data Warehousing Data Mining\nIt involves gathering all relevant data for analytics in one place.  Data is extracted from large datasets using this method. \nData extraction and storage assist in facilitating easier reporting.  It identifies patterns by using pattern recognition techniques. \nEngineers are solely responsible for data warehousing, and data is periodically stored.  Data mining is carried out by business users in conjunction with engineers, and data is analyzed regularly. \nIn addition to making data mining easier and more convenient, it helps sort and upload important data to databases.   Analyzing information and data is made easier. \nIt is possible to accumulate a large amount of irrelevant and unnecessary data. Loss and erasure of data can also be problematic.  Not doing it correctly can create data breaches and hacking since data mining isn't always 100% accurate. \nData mining cannot take place without this process, since it compiles and organizes data into a common database.  Because the process requires compiled data, it always takes place after data warehousing.  \nData warehouses simplify every type of business data.  Comparatively, data mining techniques are inexpensive. Data Warehousing Data Mining\nIt involves gathering all relevant data for analytics in one place.  Data is extracted from large datasets using this method. \nData extraction and storage assist in facilitating easier reporting.  It identifies patterns by using pattern recognition techniques. \nEngineers are solely responsible for data warehousing, and data is periodically stored.  Data mining is carried out by business users in conjunction with engineers, and data is analyzed regularly. \nIn addition to making data mining easier and more convenient, it helps sort and upload important data to databases.   Analyzing information and data is made easier. \nIt is possible to accumulate a large amount of irrelevant and unnecessary data. Loss and erasure of data can also be problematic.  Not doing it correctly can create data breaches and hacking since data mining isn't always 100% accurate. \nData mining cannot take place without this process, since it compiles and organizes data into a common database.  Because the process requires compiled data, it always takes place after data warehousing.  \nData warehouses simplify every type of business data.  Comparatively, data mining techniques are inexpensive. Data Warehousing Data Mining Data Warehousing Data Mining Data Warehousing Data Mining It involves gathering all relevant data for analytics in one place.  Data is extracted from large datasets using this method. \nData extraction and storage assist in facilitating easier reporting.  It identifies patterns by using pattern recognition techniques. \nEngineers are solely responsible for data warehousing, and data is periodically stored.  Data mining is carried out by business users in conjunction with engineers, and data is analyzed regularly. \nIn addition to making data mining easier and more convenient, it helps sort and upload important data to databases.   Analyzing information and data is made easier. \nIt is possible to accumulate a large amount of irrelevant and unnecessary data. Loss and erasure of data can also be problematic.  Not doing it correctly can create data breaches and hacking since data mining isn't always 100% accurate. \nData mining cannot take place without this process, since it compiles and organizes data into a common database.  Because the process requires compiled data, it always takes place after data warehousing.  \nData warehouses simplify every type of business data.  Comparatively, data mining techniques are inexpensive. It involves gathering all relevant data for analytics in one place.  Data is extracted from large datasets using this method. It involves gathering all relevant data for analytics in one place. Data is extracted from large datasets using this method. Data extraction and storage assist in facilitating easier reporting.  It identifies patterns by using pattern recognition techniques. Data extraction and storage assist in facilitating easier reporting. It identifies patterns by using pattern recognition techniques. Engineers are solely responsible for data warehousing, and data is periodically stored.  Data mining is carried out by business users in conjunction with engineers, and data is analyzed regularly. Engineers are solely responsible for data warehousing, and data is periodically stored. Data mining is carried out by business users in conjunction with engineers, and data is analyzed regularly. In addition to making data mining easier and more convenient, it helps sort and upload important data to databases.   Analyzing information and data is made easier. In addition to making data mining easier and more convenient, it helps sort and upload important data to databases. Analyzing information and data is made easier. It is possible to accumulate a large amount of irrelevant and unnecessary data. Loss and erasure of data can also be problematic.  Not doing it correctly can create data breaches and hacking since data mining isn't always 100% accurate. It is possible to accumulate a large amount of irrelevant and unnecessary data. Loss and erasure of data can also be problematic. Not doing it correctly can create data breaches and hacking since data mining isn't always 100% accurate. Data mining cannot take place without this process, since it compiles and organizes data into a common database.  Because the process requires compiled data, it always takes place after data warehousing. Data mining cannot take place without this process, since it compiles and organizes data into a common database. Because the process requires compiled data, it always takes place after data warehousing. Data warehouses simplify every type of business data.  Comparatively, data mining techniques are inexpensive. Data warehouses simplify every type of business data. Comparatively, data mining techniques are inexpensive.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "15. Explain data mart.",
        "answer": "An enterprise data warehouse can be divided into subsets, also called data marts, which are focused on a particular business unit or department. Data marts allow selected groups of users to easily access specific data without having to search through an entire data warehouse. Some companies, for example, may have a data mart aligned with purchasing, sales, or inventories as shown below:   In contrast to data warehouses, each data mart has a unique set of end users, and building a data mart takes less time and costs less, so it is more suitable for small businesses. There is no duplicate (or unused) data in a data mart, and the data is updated on a regular basis.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "16. Explain the three-layer architecture of an ETL cycle.",
        "answer": "Typically, ETL tool-based data warehouses use staging areas, data integration layers, and access layers to accomplish their work. In general, the architecture has three layers as shown below:   Staging Layer: In a staging layer, or source layer, data is stored that is extracted from multiple data sources.\nData Integration Layer: The integration layer plays the role of transforming data from the staging layer to the database layer.\nAccess Layer: Also called a dimension layer, it allows users to retrieve data for analytical reporting and information retrieval. Staging Layer: In a staging layer, or source layer, data is stored that is extracted from multiple data sources. Staging Layer: Data Integration Layer: The integration layer plays the role of transforming data from the staging layer to the database layer. Data Integration Layer: Access Layer: Also called a dimension layer, it allows users to retrieve data for analytical reporting and information retrieval. Access Layer:",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "17. What are the different challenges of ETL testing?",
        "answer": "In spite of the importance of ETL testing, companies may face some challenges when trying to implement it in their applications. The volume of data involved or the heterogeneous nature of the data makes ETL testing challenging.   Some of these challenges are listed below: Changing customer requirements result in re-running test cases.\nChanging customer requirements may necessitate a tester creating/modifying new mapping documents and SQL scripts, resulting in a long and tedious process.\nUncertainty about business requirements or employees who are not aware of them.\nDuring migration, data loss may occur, making it difficult for source-to-destination reconciliation to take place.\nAn incomplete or corrupt data source.\nReconciliation between data sources and targets may be impacted by incorporating real-time data.\nThere may be memory issues in the system due to the large volume of historical data.\nTesting with inappropriate tools or in an unstable environment. Changing customer requirements result in re-running test cases. Changing customer requirements may necessitate a tester creating/modifying new mapping documents and SQL scripts, resulting in a long and tedious process. Uncertainty about business requirements or employees who are not aware of them. During migration, data loss may occur, making it difficult for source-to-destination reconciliation to take place. An incomplete or corrupt data source. Reconciliation between data sources and targets may be impacted by incorporating real-time data. There may be memory issues in the system due to the large volume of historical data. Testing with inappropriate tools or in an unstable environment.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "1. State difference between ETL testing and manual testing.",
        "answer": "ETL Testing  Manual Testing \nThe test is an automated process, which means that no special technical knowledge is needed aside from understanding the software.  It requires technical expertise in SQL and Shell scripting since it is a manual process.\nIt is extremely fast and systematic, and it delivers excellent results.  In addition to being time-consuming, it is highly prone to errors.\nDatabases and their counts are central to ETL testing.  Manual testing focuses on the program's functionality.\nMetadata is included and can easily be altered.   It lacks metadata, and changes require more effort.  \nIt is concerned with error handling, log summary, and load progress, which eases the developer's and maintainer's workload.   From a maintenance perspective, it requires maximum effort.  \nIt is very good at handling historical data.  As data increases, processing time decreases. ETL Testing  Manual Testing \nThe test is an automated process, which means that no special technical knowledge is needed aside from understanding the software.  It requires technical expertise in SQL and Shell scripting since it is a manual process.\nIt is extremely fast and systematic, and it delivers excellent results.  In addition to being time-consuming, it is highly prone to errors.\nDatabases and their counts are central to ETL testing.  Manual testing focuses on the program's functionality.\nMetadata is included and can easily be altered.   It lacks metadata, and changes require more effort.  \nIt is concerned with error handling, log summary, and load progress, which eases the developer's and maintainer's workload.   From a maintenance perspective, it requires maximum effort.  \nIt is very good at handling historical data.  As data increases, processing time decreases. ETL Testing  Manual Testing ETL Testing  Manual Testing ETL Testing Manual Testing The test is an automated process, which means that no special technical knowledge is needed aside from understanding the software.  It requires technical expertise in SQL and Shell scripting since it is a manual process.\nIt is extremely fast and systematic, and it delivers excellent results.  In addition to being time-consuming, it is highly prone to errors.\nDatabases and their counts are central to ETL testing.  Manual testing focuses on the program's functionality.\nMetadata is included and can easily be altered.   It lacks metadata, and changes require more effort.  \nIt is concerned with error handling, log summary, and load progress, which eases the developer's and maintainer's workload.   From a maintenance perspective, it requires maximum effort.  \nIt is very good at handling historical data.  As data increases, processing time decreases. The test is an automated process, which means that no special technical knowledge is needed aside from understanding the software.  It requires technical expertise in SQL and Shell scripting since it is a manual process. The test is an automated process, which means that no special technical knowledge is needed aside from understanding the software. It requires technical expertise in SQL and Shell scripting since it is a manual process. It is extremely fast and systematic, and it delivers excellent results.  In addition to being time-consuming, it is highly prone to errors. It is extremely fast and systematic, and it delivers excellent results. In addition to being time-consuming, it is highly prone to errors. Databases and their counts are central to ETL testing.  Manual testing focuses on the program's functionality. Databases and their counts are central to ETL testing. Manual testing focuses on the program's functionality. Metadata is included and can easily be altered.   It lacks metadata, and changes require more effort. Metadata is included and can easily be altered. It lacks metadata, and changes require more effort. It is concerned with error handling, log summary, and load progress, which eases the developer's and maintainer's workload.   From a maintenance perspective, it requires maximum effort. It is concerned with error handling, log summary, and load progress, which eases the developer's and maintainer's workload. From a maintenance perspective, it requires maximum effort. It is very good at handling historical data.  As data increases, processing time decreases. It is very good at handling historical data. As data increases, processing time decreases.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "2. Mention some of the ETL bugs.",
        "answer": "Following are a few common ETL bugs:   User Interface Bug: GUI bugs include issues with color selection, font style, navigation, spelling check, etc.\nInput/Output Bug: This type of bug causes the application to take invalid values in place of valid ones.\nBoundary Value Analysis Bug: Bugs in this section check for both the minimum and maximum values.\nCalculation bugs: These bugs are usually mathematical errors causing incorrect results.\nLoad Condition Bugs: A bug like this does not allow multiple users. The user-accepted data is not allowed.\nRace Condition Bugs: This type of bug interferes with your system’s ability to function properly and causes it to crash or hang.\nECP (Equivalence Class Partitioning) Bug: A bug of this type results in invalid types.\nVersion Control Bugs: Regression Testing is where these kinds of bugs normally occur and does not provide version details.\nHardware Bugs: This type of bug prevents the device from responding to an application as expected.\nHelp Source Bugs: The help documentation will be incorrect due to this bug. User Interface Bug: GUI bugs include issues with color selection, font style, navigation, spelling check, etc. User Interface Bug: Input/Output Bug: This type of bug causes the application to take invalid values in place of valid ones. Input/Output Bug: Boundary Value Analysis Bug: Bugs in this section check for both the minimum and maximum values. Boundary Value Analysis Bug: Calculation bugs: These bugs are usually mathematical errors causing incorrect results. Calculation bugs: Load Condition Bugs: A bug like this does not allow multiple users. The user-accepted data is not allowed. Load Condition Bugs: Race Condition Bugs: This type of bug interferes with your system’s ability to function properly and causes it to crash or hang. Race Condition Bugs: ECP (Equivalence Class Partitioning) Bug: A bug of this type results in invalid types. ECP (Equivalence Class Partitioning) Bug: Version Control Bugs: Regression Testing is where these kinds of bugs normally occur and does not provide version details. Version Control Bugs: Hardware Bugs: This type of bug prevents the device from responding to an application as expected. Hardware Bugs: Help Source Bugs: The help documentation will be incorrect due to this bug. Help Source Bugs:",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "3. Can you define cubes and OLAP cubes?",
        "answer": "The cube is one of the things on which data processing relies heavily. In their simplest form, cubes are just data processing units that contain dimensions and fact tables from the Data warehouse. It provides clients with a multidimensional view of data, querying, and analysis capabilities. cube On the other hand, Online Analytical Processing (OLAP) is software that allows you to analyze data from several databases at the same time. For reporting purposes, an OLAP cube can be used to store data in the multidimensional form. With the cubes, creating and viewing reports becomes easier, as well as smoothing and improving the reporting process. The end users are responsible for managing and maintaining these cubes, who have to manually update their data. Online Analytical Processing  ",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "4. Explain what is fact and write its type.",
        "answer": "An important aspect of data warehousing is the fact table. A fact table basically represents the measurements, metrics, or facts of a business process. In fact tables, facts are stored, and they are linked to a number of dimension tables via foreign keys. Facts are usually details and/or aggregated measurements of a business process which can be calculated and grouped to address the business question. Data schemas like the star schema or snowflake schema consist of a central fact table surrounded by several dimension tables. The measures or numbers like sales, cost, profit and loss, etc., are some examples of facts. Fact tables have two types of columns, foreign keys and measures columns. Foreign keys store foreign keys to dimensions, while measures contain numeric facts. Other attributes can be added, depending on the business need and necessity. Types of Facts Types of Facts Facts can be divided into three basic types, as follows:   Additive: Facts that are fully additive are the most flexible and useful. We can sum up additive facts across any dimension associated with the fact table.\nSemi-additive: We can sum up semi-additive facts across some dimensions associated with the fact table, but not all.\nNon-Additive: The Fact table contains non-additive facts, which cannot be summed up for any dimension. The ratio is an example of a non-additive fact. Additive: Facts that are fully additive are the most flexible and useful. We can sum up additive facts across any dimension associated with the fact table. Additive: Semi-additive: We can sum up semi-additive facts across some dimensions associated with the fact table, but not all. Semi-additive: Non-Additive: The Fact table contains non-additive facts, which cannot be summed up for any dimension. The ratio is an example of a non-additive fact. Non-Additive:",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "5. Define Grain of Fact.",
        "answer": "Accordingly, grain fact refers to the level of storing fact information. Alternatively, it is known as Fact Granularity.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "6. What do you mean by ODS (Operational data store)?",
        "answer": "Between the staging area and the Data Warehouse, ODS serves as a repository for data. Upon inserting the data into ODS, ODS will load all the data into the EDW (Enterprise data warehouse). The benefits of ODS mainly pertain to business operations, as it presents current, clean data from multiple sources in one place. Unlike other databases, an ODS database is read-only, and customers cannot update it.  ",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "7. What do you mean by staging area and write its main purpose?",
        "answer": "During the extract, transform, and load (ETL) process, a staging area or landing zone is used as an intermediate storage area. It serves as a temporary storage area between data sources and data warehouses. Staging areas are primarily used to extract data quickly from their respective data sources, therefore minimizing the impact of those sources. Using the staging area, data is combined from multiple data sources, transformed, validated, and cleaned after data has been loaded.  ",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "8. Explain the Snowflake schema.",
        "answer": "Adding additional dimension tables to a Star Schema makes it a Snowflake Schema. In the Snowflake schema model, multiple hierarchies of dimension tables surround a central fact table.  Alternatively, a dimension table is called a snowflake if its low-cardinality attribute has been segmented into separate normalized tables. These normalized tables are then joined with referential constraints (foreign key constraints) to the original dimensions table. Snowflake schema complexity increases linearly with the level of hierarchy in the dimension tables.   Advantages Advantages Data integrity is reduced because of structured data.\nData are highly structured, so it requires little disk space.\nUpdating or maintaining Snowflaking tables is easy. Data integrity is reduced because of structured data. Data are highly structured, so it requires little disk space. Updating or maintaining Snowflaking tables is easy. Disadvantages Disadvantages Snowflake reduces the space consumed by dimension tables, but the space saved is usually insignificant compared with the entire data warehouse.\nDue to the number of tables added, you may need complex joins to perform a query, which will reduce query performance. Snowflake reduces the space consumed by dimension tables, but the space saved is usually insignificant compared with the entire data warehouse. Due to the number of tables added, you may need complex joins to perform a query, which will reduce query performance.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "9. Explain what you mean by Bus Schema.",
        "answer": "An important part of ETL is dimension identification, and this is largely done by the Bus Schema. A BUS schema is actually comprised of a suite of verified dimensions and uniform definitions and can be used for handling dimension identification across all businesses. To put it another way, the bus schema identifies the common dimensions and facts across all the data marts of an organization just like identifying conforming dimensions (dimensions with the same information/meaning when being referred to different fact tables). Using the Bus schema, information is given in a standard format with precise dimensions in ETL.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "10. What do you mean by schema objects?",
        "answer": "Generally, a schema comprises a set of database objects, such as tables, views, indexes, clusters, database links, and synonyms, etc. This is a logical description or structure of the database. Schema objects can be arranged in various ways in schema models designed for data warehousing.  Star and snowflake schemas are two examples of data warehouse schema models.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "11. What is the benefit of using a Data reader destination adapter?",
        "answer": "ADO Recordset holds a collection of records (records and columns) from a database table. The Data Reader Destination Adapter is very useful when it comes to populating them in a simple manner. Using the ADO.NET DataReader interface, it exposes the data in a data flow for other applications to consume it.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "12. What do you mean by factless table?",
        "answer": "Factless tables do not contain any facts or measures. It contains only dimensional keys and deals with event occurrences at the informational level but not at the calculational level. As the name implies, factless fact tables capture relationships between dimensions but lack any numerical or textual data. Factual fact tables can be categorized into two categories: one that describes events, and the other one that describes conditions. Both may have a significant impact on your dimensional modeling.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "13. Explain SCD (Slowly Change Dimension).",
        "answer": "SCD (Slowly Changing Dimensions) basically keep and manage both current and historical data in a data warehouse over time. Rather than changing regularly on a time-based schedule, SCD changes slowly over time. SCD is considered one of the most critical aspects of ETL.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "1. Explain partitioning in ETL and write its type.",
        "answer": "Essentially, partitioning is the process of dividing up a data storage area for improved performance. It can be used to organize your work. Having all your data in one place without organization makes it more difficult for digital tools to find and analyze the data. It is easier and faster to locate and analyze data when your data warehouse is partitioned. The following reasons make partitioning important: Facilitate easy data management and enhance performance.\nEnsures that all of the system's requirements are balanced.\nBackups/recoveries made easier.\nSimplifies management and optimizes hardware performance. Facilitate easy data management and enhance performance. Ensures that all of the system's requirements are balanced. Backups/recoveries made easier. Simplifies management and optimizes hardware performance. Types of Partitioning - Types of Partitioning Round-robin Partitioning: This is a method in which data is evenly spread among all partitions. Therefore, each partition has approximately the same number of rows. Unlike hash partitioning, the partitioning columns do not need to be specified. New rows are assigned to partitions in round-robin style.\nHash Partitioning: With hash partitioning, rows are evenly distributed across partitions based on a partition key. Using a hash function, the server creates partition keys to group data. Round-robin Partitioning: This is a method in which data is evenly spread among all partitions. Therefore, each partition has approximately the same number of rows. Unlike hash partitioning, the partitioning columns do not need to be specified. New rows are assigned to partitions in round-robin style. Round-robin Partitioning: Hash Partitioning: With hash partitioning, rows are evenly distributed across partitions based on a partition key. Using a hash function, the server creates partition keys to group data. Hash Partitioning:",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "2. Write different ways of updating a table when SSIS (SQL Server Integration Services) is being used.",
        "answer": "In order to update a table in SSIS, the following steps can be taken: Use the SQL command.\nFor storing stage data, use staging tables.\nKeep data in a cache that occupies a limited amount of space and needs to be refreshed frequently.\nScripts can be used for scheduling tasks.\nWhen updating MSSQL, use the full database name. Use the SQL command. For storing stage data, use staging tables. Keep data in a cache that occupies a limited amount of space and needs to be refreshed frequently. Scripts can be used for scheduling tasks. When updating MSSQL, use the full database name.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "3. Write some ETL test cases.",
        "answer": "Among the most common ETL test cases are: Mapping Doc Validation: Determines whether the Mapping Doc contains ETL information.\nData Quality: In this case, every aspect of the data is tested, including number Check, Null Check, Precision Check, etc.\nCorrectness Issues: Tests for missing, incorrect, non-unique, and null data.\nConstraint Validation: Make sure that the constraints are properly defined for each table. Mapping Doc Validation: Determines whether the Mapping Doc contains ETL information. Mapping Doc Validation: Data Quality: In this case, every aspect of the data is tested, including number Check, Null Check, Precision Check, etc. Data Quality: Correctness Issues: Tests for missing, incorrect, non-unique, and null data. Correctness Issues: Constraint Validation: Make sure that the constraints are properly defined for each table. Constraint Validation:",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "4. Explain ETL mapping sheets.",
        "answer": "Typically, ETL mapping sheets include full information about a source and a destination table, including every column as well as their lookup in reference tables. As part of the ETL testing process, ETL testers may need to write big queries with multiple joins to validate data at any point in the testing process. Data verification queries are significantly easier to write using ETL mapping sheets.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "5. How ETL testing is used in third party data management?",
        "answer": "Different kinds of vendors develop different kinds of applications for big companies. Consequently, no single vendor manages everything. Consider a Telecommunication project in which billing is handled by one company and CRM by another. For instance, if a CRM requires data from the company that is managing the billing, now that company will be able to receive the data feed from another company. In this case, we will use the ETL process to load data from the feed.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "6. Explain how ETL is used in data migration projects.",
        "answer": "Data migration projects commonly use ETL tools. As an example, if the organization managed the data in Oracle 10g earlier and now they want to move to SQL Server cloud database, the data will need to be migrated from Source to Target. ETL tools can be very helpful for carrying out this type of migration. The user will have to spend a lot of time writing ETL code. The ETL tools are therefore very useful since they make coding simpler than P-SQL or T-SQL. Hence, ETL is a very useful process for data migration projects.",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "7. What are the conditions under which you use dynamic cache and static cache in connected and unconnected transformations?",
        "answer": "In order to update the master table and slowly changing dimensions (SCD) type 1, it is necessary to use the dynamic cache.\nIn the case of flat files, a static cache is used. In order to update the master table and slowly changing dimensions (SCD) type 1, it is necessary to use the dynamic cache. In the case of flat files, a static cache is used. Conclusion With abundant job opportunities and lucrative salary options, ETL testing has become a popular trend. ETL Testing has an extensive market share and is one of the cornerstones of data warehousing and business analytics. To make this process more organized and simpler, many software vendors have introduced ETL testing tools. Most employers who seek ETL testers look for candidates with specific technical skills and experience that meet their needs. No worries, this platform is a great resource for both beginners and professionals. In this article, we have covered 35+ ETL testing interview questions ranging from freshers to experienced level questions typically asked during interviews. Preparation is key before you go for your job interview. Recommended Resources: Recommended Resources: SQL SQL Python Python Java Java Informatica Informatica",
        "reference": "interviewbit.com",
        "role": "etl-testing"
    },
    {
        "question": "",
        "answer": "ETL stands for Extraction, Transformation, and Loading. It is an essential concept in Data Warehousing systems. There are three basics steps in Data Integration Process. Extraction stands for extracting the data from different data sources such as transactional systems or applications. Transformation stands to apply the conversion rules on data so that it becomes suitable for analytical reporting. Loading process involves, to move the data into the target system, i.e., Data Warehouse.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "2) Explain the concept of Extraction, Transformation, and Loading?",
        "answer": "Extraction\nExtracted the data from an external source and move it to the data Warehouse pre-processor database.\nTransformation\nTransform data task allows point to point generating, modifying, and transforming the data.\nLoading\nIn this task, the data is added to the database table in a warehouse.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "3) What is the three-layer architecture of an ETL cycle?",
        "answer": "The three layers in the ETL are:\nADVERTISEMENT\nADVERTISEMENT\nStaging Layer: Staging layer is used to store the data which is extracted from the different data source systems.\nData Integration Layer: Integration layer transforms the data from the staging layer and moves the data to a database. In the database, the data is arranged into hierarchical groups, which is often called dimension, and into facts and aggregation facts. The combination of facts and dimension table in a data warehouse system is called a schema.\nAccess Layer: Access layer is used by the end-users to retrieve the data for analytical reporting.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "4) What is BI?",
        "answer": "Business Intelligence is the process for collecting raw business data and transforming it into a meaningful vision that is more useful for business.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "5) What are the differences between ETL and BI tools?",
        "answer": "ETL TOOLS BI TOOLS\nThe ETL tools are used to extract the data from different data sources, transform the data, and load it into a data warehouse system. BI tools are used to generate interactive and ad-hoc reports for end-users, data visualization for monthly, quarterly, and annual board meetings.\nMost commonly ETL tools are Informatica, SAP BO data service, Microsoft SSIS, Oracle Data Integrator (ODI) Clover ETL Open Source, etc. Most commonly BI tools are SAP Lumira, IBM Cognos, Microsoft BI platform, Tableau, Oracle Business Intelligence Enterprise Edition, etc.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "6) What are the ETL tools available in the market?",
        "answer": "The popular ETL tools available in the market are:\nIBM- Websphere DataStage\nInformatica- Power Center\nSAP- Business objects data service BODS\nSAS - Data Integration Studio\nOracle- Warehouse Builder\nOpen source Clover ETL.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "7) When we need the staging area in the ETL process?",
        "answer": "Staging area is a central area which is available between the data sources and data warehouse/data marts systems. It is a place where data is stored temporarily in the process of data integration. In the staging, area data is cleansed and checked for any duplication. The staging area is designed to provide many benefits, but the primary goal is to use the staging area. It is used to increase efficiency, ensure the data integrity, and support the data quality operations.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "8) What is the difference between the data warehouse and data mining?",
        "answer": "Data warehousing is a broad concept as compared to data mining. Data Mining involves extracting the hidden information from the data and interpreting it for future forecasting. In contrast, data warehousing includes operations such as analytical reporting to generate detailed reports and ad-hoc reports, information processing to generate interactive dashboards and charts.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "9) What are the differences between data warehousing and data mining?",
        "answer": "OLTP OLAP\nOLTP stands for Online Transactional Processing. OLAP stands for Online Analytical Processing.\nOLTP is a relational database, and it is used to manage the day to day transaction. OLAP is a multidimensional system, and it is also called a data warehouse.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "10) What is a dimension table and how it is different from the fact table?",
        "answer": "Here, we are taking an example to describe how the dimension table is distinguishing from the fact table.\nSuppose a company sells its products to its customer. Every sale is a fact which occurs within the company, and the fact table is used to record these facts. Each fact table stores the primary key that joins fact table with the dimension table and measures/ facts.\nExample: Fact Units\nCust_ID Prod_ID Time_ID No. of units sold\n101 24 1 25\n102 25 2 15\n103 26 3 30\nA dimension table which store attributes or dimensions describe the objects in a fact table. It is a set of companion tables to a fact table.\nCust_ID Cust_Name Gender\n101 Sana F\n102 Jass M",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "11) What is a Data Mart?",
        "answer": "Data Mart is a simple form of Data Warehouse, and it is focused on a single functional area. It gets the only from few sources.\nFor example: In an organization, data marts may exist for marketing, finance, human resource, and other individual departments which stores the data related to their specific functions.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "12) What is the difference between Manual Testing and ETL Testing?",
        "answer": "The difference between Manual testing and ETL testing is:\nManual testing focuses on the functionality of the program while the ETL testing is related to database and its count.\nETL is the automated testing process where we do not need any technical knowledge. ETL testing is extremely faster, systematic, and assurance of the result required by the business.\nManual testing is a time-consuming process where we need the technical knowledge to write the test cases and scripts. It is slow, highly prone to errors, and also need efforts.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "13) What is ETL Testing?",
        "answer": "ETL stands for Extraction, Transform, and Loading the information. ETL testing is done to ensure that the data is loaded from different source to destination after the accurately business transformation. It involves data verification at multiple stages that are being used between the source and the destination.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "14) What is the responsibility of ETL tester?",
        "answer": "The responsibility of ETL Tester is divided into three major categories:\nStage Tables\nBusiness Logic Transformation\nTarget table loading from the staging table, once we apply the transformation.\n\nResponsibilities of ETL tester are:\nETL tester tests the ETL software thoroughly.\nThe tester will check the test component of the ETL Data Warehouse.\nThe tester will execute the data-driven test in the backend.\nThe tester creates the design and executes the test cases, test plans or test harness, etc.\nTester identifies the problems and will suggest the best solution also.\nTester approves the requirements and design specification.\nTester transfers the data from flat files.\nThey write the SQL queries for the different test scenario.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "15) What is the need for ETL Testing?",
        "answer": "In today's time, we are migrating the lots of system from old technology to new technology. At the time of migration activities, we also need to migrate the data as well from old DBMS to latest DBMS. So there is a lot of need to test whether the data is correct from the target side.\nHere, are some important points where the need for ETL testing is arising:\nETL testing used to keep an eye on the data which is being transferred from one system to another.\nThe need for ETL testing is to keep a track on the efficiency and speed of the process.\nThe need for ETL testing is arising to be familiar with the ETL process before we implement it into our business and production.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "",
        "answer": "Before ETL tools user writes the extended code for data transformation to data loading.\nETL makes life more comfortable, and one tool manages all the scenarios of transformation and loading of the data.\nHere is the following example where we are using the ETL:\nExample: Data Warehousing\nETL is used in data warehousing concept. Here, we need to fetch the data from multiple different systems and loads it in the data warehouse database. ETL concept is used here to extract the data from the source, transform the data, and load it into the target system.\n\nExample: Data Migration\nData migrations are a difficult task if we are using PLSQL. If we want to migrate the data using a simple way, we will use different ETL tools.\nExample: Mergers and Acquisitions\nIn today's time, lots of companies are merging into different MNCs. To move the data from one company to another, the need for ETL concept arises.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "17) How we use ETL in third party management?",
        "answer": "The big organization always gives different application development to different kind of vendors. A single vendor cannot manage everything. Here we are taking an example of a telecommunication project where billing is handled by one company, and another company manages CRM. If CRM company needs the data from the company, who is managing the billing, now the company will receive the data feed from other company. To load the data from the ETL process is used.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "18) How we use ETL in Data Warehousing?",
        "answer": "Most commonly, the ETL used in Data Warehousing. User fetches the historical data as well as current data for developing the data warehouse. Data in the data warehouse is the combination of historical data as well as transactional data. Data Source of data warehouse might be different. We need to fetch the data from multiple different systems and load it into a single target system, which is also called a data warehouse.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "19) What is the difference between ETL Testing and Database Testing?",
        "answer": "The differences between the ETL testing and Database testing are:\nETL Testing Database Testing\nIn ETL testing, the goal is the reporting of business intelligence In DB testing, the goal is to integrate the data.\nThe flow of business environment is based on the data used earlier Database Testing applies to business flow systems only.\nThe tools Informatica, Query Surge, Cognos can be used. In DB testing, the QTP and Selenium tools are used.\nIn ETL testing, Dimensional model is used. In DB testing, relational model is used.\nIn ETL testing, Analytics are processed. In DB testing, Transactions are processed.\nDenormalized data is used in ETL testing. .Normalized data is used.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "20) What are the characteristics of Data Warehouse?",
        "answer": "Data Warehouse is a database which is different from the operational database and stores the historical data.\nData Warehouse Database contains the analytical as well as transactional data.\nData Warehouse is used for data analysis and reporting purpose.\nData Warehouse helps the higher management to take strategic and tactical decisions using historical or current data.\nData Warehouse helps the business user to the current trend to run the business.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "21) What are the types of Data Warehouse systems?",
        "answer": "Online Analytical Processing (OLAP)\nPredictive Analysis\nOnline Transactional Processing\nData Mart",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "22) What are the steps followed in ETL testing process?",
        "answer": "The different steps followed in ETL testing process are:\nStep 1. Requirement Analyzing\nIn this step, we understand the business structure and the requirement.\nStep 2. Validation and Test Estimation\nAn estimation of time and expertise is required in this step.\nStep 3. Test Planning and designing the testing environment\nThis step is based on the validation and test estimation. In this step, the environment of ETL is planned according to the input which is used in the test estimation and worked according to that.\nStep 4. Test Data Preparation and Execution\nAs per the test, data is prepared and executed as per the requirement.\nStep 5. Summary Report\nOn the completion of the test run, a summary report is prepared for concluding and improvising.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "",
        "answer": "ETL tools are generally used in Data Migration Project. If any organization is managing the data in Oracle 10g previously, now the organization wants to use SQL server cloud database, then there is a need to move the data from source to target. For this kind of movement, ETL tools are very useful. If we want to write the code for ETL, it is a very time-consuming process. To make this simple, we use ETL tool, which makes the coding simple PL SQL or T- SQL code. So the ETL process is useful in Data Migration Projects.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "24) What are the steps followed to choose the ETL process?",
        "answer": "It is a very difficult task to choose the ETL tools. To select the correct ETL tool, we need to consider a lot of factors according to the project. To choose the ETL tool for a specific project is a very strategic move, even we need it for a small project.\nHere are some points which will help us to choose the ETL tool.\nData Connectivity\nTo choose the ETL tool, we will focus on how the ETL tool should communicate with any source of data no matter where the data comes. Data connectivity is very critical.\nPerformance\nTo move and change the data requires some serious processing power. So here, we need to check the performance factor.\nTransformation Flexibility\nMerging, Matching, and Changing the data is very critical. ETL tools should provide all these Mergings, Matching and Changing operations and many transformation packages. It allows the modification to the data in the transformation phase with simple drag and drop.\nData Quality\nWe can take advantage of the data only when the data is clean and consistent.\nFlexible data action option\nWhen the ETL is ready, we need to check that ETL will work on previous data as well as new coming data.\nCommitted ETL vendor\nWe are working with the organization data while we are doing the ETL process. So here we have to choose the vendor who is aware of the industry and whose support will be beneficial.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "25) What are the ETL bugs?",
        "answer": "Here are the following ETL bugs:\nSource Bugs\nLoad Condition Bugs\nCalculation Bugs\nECP related Bugs\nUser-Interface Bugs",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "26) What is Operation Data Source?",
        "answer": "ODS stands for Operational Data Source.\nODS works between the staging area and the Data Warehouse. The data is ODS will be at the level of granularity.\nWhen the data is inserted in ODS, all the data will be loaded in the EDW through ODS.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "27) What is the data extraction phase in ETL?",
        "answer": "Data Extraction is nothing, but it is extracting the data from multiple different sources using ETL tools.\nHere are two types of data extraction.\nFull Extraction: All extracted data from an operational system or source system load to the staging area.\nPartial Extraction: Sometimes, we get the notification from the source system to update the specific data. It is called Delta Load.\nSource System Performance: The extraction strategies of data should not affect the performance of the source system.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "28) What are the ETL Tools?",
        "answer": "The popular tools are:\n1. Enterprise ETL tools\nInformatica\nTalend\nIBM Datastage\nAbnitio\nMS SQL Server Integration service\nClover ETL\n2. Open Source ETL tools\nPentaho\nKettle",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "29) What is partitioning in ETL?",
        "answer": "Transactions are always needed to be divided for better performance. The same processes are known as Partitioning. It merely makes sure that the server can directly access the sources through multiple connections.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "30) What is ETL Pipeline?",
        "answer": "ETL Pipeline refers to a set of processes to extract the data from one system, transform it, and load it into some database or data warehouse. ETL pipelines are built for data warehousing applications, which includes both enterprise data warehouse as well as subject-specific data marts. ETL pipelines are also used for data migration solutions. Data warehouse/ business intelligence engineers build ETL pipelines.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "31) What is the Data Pipeline?",
        "answer": "Data Pipeline refers to any set of processes elements that move data from one system to another. Data Pipeline can be built for any kind of application which uses data to bring the value. It can be used for integrating the data across the applications, build the data-driven web products and carrying out the data mining activities. Data engineers build the data pipeline.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "32) What is the staging place in the ETL Testing?",
        "answer": "Staging place is the temporary storage area that is used during the data integration process. In this place, data is analyzed carefully for redundancy and duplication.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "",
        "answer": "ETL mapping sheet contains all the necessary information from the source file and stores the details in rows and column. Mapping sheets help in writing the SQL queries to speed up the testing process.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "34) What is the transformation in ETL Testing?",
        "answer": "Transformation is defined as the archive objects to generate, modify, or pass the data. Transformation can be Active or passive. Transformation is beneficial in many ways.\nIt helps in getting values very quickly.\nThe transformation can update the slowly changing dimension table.\nIt checks or verifies whether the record exists or not inside the table.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "35) What is the use of dynamic cache and static cache in transformation?",
        "answer": "Dynamic cache is used to update the dimension or master table slowly. The static cache is used in flat files.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "36) What is a mapping, Session, Worklet, and Mapplet?",
        "answer": "Mapping: Mapping represents workflow from source to target.\nWorkflow: Workflow is a set of instructions which tells the Informatica server how to execute the tasks.\nMapplet: Mapplet configures or creates a set of transformation.\nWorklet: It is an object that represents a set of tasks.\nSession: Session is a set of instructions that describe how and when to move the data from sources to target.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "37) What is full load and incremental or refresh load?",
        "answer": "Full Load: Full load completely erase the content of one or more tables and reload with fresh data.\nIncremental Load: In this, we apply the ongoing changes to one or more table, which is based on a predefined schedule.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "38) What are joiner and lookup?",
        "answer": "The joiner is used to join two or more tables to retrieve the data from tables.\nLookup is used to check and compare the source table and the target table.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "39) What is data purging?",
        "answer": "Data Purging is a term that is commonly used to describe the methods which remove and permanently erase the data from a storage space. In other words, it can be defined as deleting the data from the data warehouse is known as data purging. Usually, we have to clean up the junk data like rows which have null values or spaces. Data Purging is the process of cleaning the junk values.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    },
    {
        "question": "40) What is the difference between ETL tools and OLAP tools?",
        "answer": "ETL Tools is meant for extraction the data from the legacy system and load it into the specified database with some process of cleansing data.\nFor example: Informatica, data stage etc.\nOLAP Tools: It is used for reporting purpose in OLAP data available in the multidirectional model. We can write a simple query to extract the data from the database.\nExample: Business object, Cognos, etc.",
        "reference": "javatpoint.com",
        "role": "etl-testing"
    }
]